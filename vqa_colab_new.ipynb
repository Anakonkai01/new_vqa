{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "e5c79e5d",
      "metadata": {
        "id": "e5c79e5d"
      },
      "source": [
        "# Visual Question Answering ‚Äî End-to-End Pipeline\n",
        "\n",
        "**B√†i to√°n:** Cho ·∫£nh + c√¢u h·ªèi ‚Üí sinh c√¢u tr·∫£ l·ªùi b·∫±ng LSTM-Decoder.\n",
        "\n",
        "**4 ki·∫øn tr√∫c:**\n",
        "\n",
        "| Model | CNN Encoder | Attention |\n",
        "|-------|-------------|----------|\n",
        "| A | Scratch CNN | No |\n",
        "| B | Pretrained ResNet101 | No |\n",
        "| C | Scratch CNN | Bahdanau |\n",
        "| D | Pretrained ResNet101 | Bahdanau |\n",
        "\n",
        "**Pipeline:**\n",
        "1. Clone repo + c√†i ƒë·∫∑t dependencies\n",
        "2. T·∫£i d·ªØ li·ªáu VQA 2.0 t·ª´ Kaggle\n",
        "3. Build vocab (questions + answers)\n",
        "4. Train 4 models (A, B, C, D)\n",
        "5. Plot training curves\n",
        "6. Evaluate t·ª´ng model (VQA Accuracy, Exact Match, BLEU-1/2/3/4, METEOR)\n",
        "7. So s√°nh 4 models side-by-side\n",
        "8. Inference tr√™n sample\n",
        "9. Attention Visualization (Model C, D)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6f5b83d0",
      "metadata": {
        "id": "6f5b83d0"
      },
      "source": [
        "---\n",
        "## Step 0 ‚Äî Environment Setup\n",
        "\n",
        "- Ki·ªÉm tra GPU\n",
        "- Clone repository t·ª´ GitHub\n",
        "- C√†i ƒë·∫∑t dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "20779874",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "20779874",
        "outputId": "4b44a303-3e1a-461a-c08a-f806e9d6faf3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "PyTorch: 2.10.0+cu128\n",
            "CUDA available: True\n",
            "GPU: NVIDIA RTX PRO 6000 Blackwell Server Edition\n",
            "Memory: 102.0 GB\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "print(f\"PyTorch: {torch.__version__}\")\n",
        "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "48ce8e6d",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "48ce8e6d",
        "outputId": "723b3d8a-af86-46d9-f40d-a87ba102e064"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cloning into 'new_vqa'...\n",
            "remote: Enumerating objects: 210, done.\u001b[K\n",
            "remote: Counting objects: 100% (210/210), done.\u001b[K\n",
            "remote: Compressing objects: 100% (130/130), done.\u001b[K\n",
            "remote: Total 210 (delta 128), reused 155 (delta 77), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (210/210), 136.08 KiB | 1.13 MiB/s, done.\n",
            "Resolving deltas: 100% (128/128), done.\n",
            "/content/new_vqa\n"
          ]
        }
      ],
      "source": [
        "# Clone repository\n",
        "!git clone https://github.com/Anakonkai01/new_vqa.git\n",
        "%cd new_vqa\n",
        "\n",
        "# Checkout branch (thay ƒë·ªïi n·∫øu c·∫ßn)\n",
        "# !git checkout experiment/new"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "192b27f7",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "192b27f7",
        "outputId": "e79c20a5-d2c5-4127-ad3c-87f80b69fa02"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "remote: Enumerating objects: 9, done.\u001b[K\n",
            "remote: Counting objects:  11% (1/9)\u001b[K\rremote: Counting objects:  22% (2/9)\u001b[K\rremote: Counting objects:  33% (3/9)\u001b[K\rremote: Counting objects:  44% (4/9)\u001b[K\rremote: Counting objects:  55% (5/9)\u001b[K\rremote: Counting objects:  66% (6/9)\u001b[K\rremote: Counting objects:  77% (7/9)\u001b[K\rremote: Counting objects:  88% (8/9)\u001b[K\rremote: Counting objects: 100% (9/9)\u001b[K\rremote: Counting objects: 100% (9/9), done.\u001b[K\n",
            "remote: Compressing objects: 100% (1/1)\u001b[K\rremote: Compressing objects: 100% (1/1), done.\u001b[K\n",
            "remote: Total 5 (delta 4), reused 5 (delta 4), pack-reused 0 (from 0)\u001b[K\n",
            "Unpacking objects:  20% (1/5)\rUnpacking objects:  40% (2/5)\rUnpacking objects:  60% (3/5)\rUnpacking objects:  80% (4/5)\rUnpacking objects: 100% (5/5)\rUnpacking objects: 100% (5/5), 969 bytes | 969.00 KiB/s, done.\n",
            "From https://github.com/Anakonkai01/new_vqa\n",
            "   bbe1378..3e70c39  main       -> origin/main\n",
            "Updating bbe1378..3e70c39\n",
            "Fast-forward\n",
            " src/compare.py | 11 \u001b[32m+++++++++\u001b[m\u001b[31m--\u001b[m\n",
            " src/train.py   |  9 \u001b[32m+++++++++\u001b[m\n",
            " 2 files changed, 18 insertions(+), 2 deletions(-)\n"
          ]
        }
      ],
      "source": [
        "!git pull"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ffbf78fd",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ffbf78fd",
        "outputId": "57b9797a-6fbf-4332-fdc2-cdef82b25b72"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# C√†i ƒë·∫∑t dependencies\n",
        "!pip install -q nltk tqdm matplotlib Pillow\n",
        "\n",
        "import nltk\n",
        "nltk.download('wordnet', quiet=True)\n",
        "nltk.download('omw-1.4', quiet=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "655fe7d4",
      "metadata": {
        "id": "655fe7d4"
      },
      "source": [
        "---\n",
        "## Google Drive ‚Äî Mount & Setup\n",
        "\n",
        "Mount Google Drive ƒë·ªÉ l∆∞u tr·ªØ:\n",
        "- **Checkpoints** (model weights) ‚Äî quan tr·ªçng nh·∫•t, m·∫•t nhi·ªÅu gi·ªù ƒë·ªÉ train l·∫°i\n",
        "- **Vocab files** ‚Äî nh·ªè nh∆∞ng c·∫ßn thi·∫øt ƒë·ªÉ resume\n",
        "- **Output files** ‚Äî training curves, attention maps, analysis plots\n",
        "\n",
        "> Khi runtime Colab b·ªã disconnect, m·ªçi d·ªØ li·ªáu local s·∫Ω **m·∫•t**. Drive gi√∫p b·∫°n resume training t·ª´ checkpoint ƒë√£ l∆∞u."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fd1d5dc9",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fd1d5dc9",
        "outputId": "9d452a78-ec0b-451a-a6e8-318bb89c59e1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n",
            "Drive project dir: /content/drive/MyDrive/VQA_Project\n",
            "  checkpoints/  ‚Äî model weights (resume, best, milestones)\n",
            "  vocab/         ‚Äî vocab_questions.json, vocab_answers.json\n",
            "  outputs/       ‚Äî training curves, attention maps, analysis plots\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Th∆∞ m·ª•c l∆∞u tr·ªØ tr√™n Drive\n",
        "DRIVE_DIR = '/content/drive/MyDrive/VQA_Project'\n",
        "\n",
        "import os\n",
        "os.makedirs(f'{DRIVE_DIR}/checkpoints', exist_ok=True)\n",
        "os.makedirs(f'{DRIVE_DIR}/vocab', exist_ok=True)\n",
        "os.makedirs(f'{DRIVE_DIR}/outputs', exist_ok=True)\n",
        "\n",
        "print(f\"Drive project dir: {DRIVE_DIR}\")\n",
        "print(f\"  checkpoints/  ‚Äî model weights (resume, best, milestones)\")\n",
        "print(f\"  vocab/         ‚Äî vocab_questions.json, vocab_answers.json\")\n",
        "print(f\"  outputs/       ‚Äî training curves, attention maps, analysis plots\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fbb18152",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fbb18152",
        "outputId": "15e4834d-a393-4874-8305-633deb7558a4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Helper functions defined: sync_to_drive(), restore_from_drive()\n"
          ]
        }
      ],
      "source": [
        "import shutil, os, glob\n",
        "\n",
        "DRIVE_DIR = '/content/drive/MyDrive/VQA_Project'\n",
        "\n",
        "def sync_to_drive(src_pattern, drive_subdir, label=\"\"):\n",
        "    \"\"\"Copy files matching src_pattern to Drive subfolder.\"\"\"\n",
        "    dst_dir = f'{DRIVE_DIR}/{drive_subdir}'\n",
        "    os.makedirs(dst_dir, exist_ok=True)\n",
        "    files = glob.glob(src_pattern)\n",
        "    if not files:\n",
        "        print(f\"  [SKIP] No files matching: {src_pattern}\")\n",
        "        return\n",
        "    for f in files:\n",
        "        dst = os.path.join(dst_dir, os.path.basename(f))\n",
        "        shutil.copy2(f, dst)\n",
        "    names = [os.path.basename(f) for f in files]\n",
        "    total_mb = sum(os.path.getsize(f) for f in files) / 1e6\n",
        "    print(f\"  ‚úì {label or drive_subdir}: {len(files)} files ({total_mb:.1f} MB) ‚Üí Drive/{drive_subdir}/\")\n",
        "\n",
        "def restore_from_drive(drive_subdir, local_dir, label=\"\"):\n",
        "    \"\"\"Restore files from Drive subfolder to local directory.\"\"\"\n",
        "    src_dir = f'{DRIVE_DIR}/{drive_subdir}'\n",
        "    if not os.path.exists(src_dir):\n",
        "        print(f\"  [SKIP] Drive/{drive_subdir}/ not found\")\n",
        "        return 0\n",
        "    os.makedirs(local_dir, exist_ok=True)\n",
        "    files = [f for f in os.listdir(src_dir) if os.path.isfile(os.path.join(src_dir, f))]\n",
        "    for f in files:\n",
        "        shutil.copy2(os.path.join(src_dir, f), os.path.join(local_dir, f))\n",
        "    total_mb = sum(os.path.getsize(os.path.join(local_dir, f)) for f in files) / 1e6\n",
        "    print(f\"  ‚úì {label or drive_subdir}: restored {len(files)} files ({total_mb:.1f} MB)\")\n",
        "    return len(files)\n",
        "\n",
        "def download_to_local(model_letter):\n",
        "    \"\"\"T·∫£i checkpoint c·ªßa 1 model v·ªÅ m√°y local qua browser (google.colab.files.download).\n",
        "    D√πng khi Drive c√≥ th·ªÉ h·∫øt dung l∆∞·ª£ng ‚Äî t·∫£i th·∫≥ng v·ªÅ m√°y t√≠nh c√° nh√¢n.\n",
        "    \"\"\"\n",
        "    from google.colab import files as colab_files\n",
        "    m = model_letter.lower()\n",
        "    patterns = [\n",
        "        f'checkpoints/model_{m}_best.pth',\n",
        "        f'checkpoints/model_{m}_epoch*.pth',\n",
        "        f'checkpoints/history_model_{m}.json',\n",
        "    ]\n",
        "    downloaded = 0\n",
        "    for pat in patterns:\n",
        "        for f in sorted(glob.glob(pat)):\n",
        "            try:\n",
        "                sz = os.path.getsize(f) / 1e6\n",
        "                print(f\"  ‚¨á Downloading {os.path.basename(f)} ({sz:.1f} MB)...\")\n",
        "                colab_files.download(f)\n",
        "                downloaded += 1\n",
        "            except Exception as e:\n",
        "                print(f\"  [WARN] Failed to download {f}: {e}\")\n",
        "    if downloaded > 0:\n",
        "        print(f\"  üì• Model {model_letter.upper()}: {downloaded} files downloaded to local machine\")\n",
        "    else:\n",
        "        print(f\"  [SKIP] No checkpoints found for Model {model_letter.upper()}\")\n",
        "    return downloaded\n",
        "\n",
        "def auto_sync_model(model_letter):\n",
        "    \"\"\"T·ª± ƒë·ªông sync checkpoints l√™n Drive + t·∫£i v·ªÅ m√°y local.\n",
        "    G·ªçi NGAY SAU m·ªói cell training ƒë·ªÉ ƒë·∫£m b·∫£o kh√¥ng m·∫•t checkpoint.\n",
        "    Chi·∫øn l∆∞·ª£c 2 l·ªõp b·∫£o v·ªá:\n",
        "      1. Copy l√™n Google Drive (nhanh, nh∆∞ng c√≥ th·ªÉ h·∫øt dung l∆∞·ª£ng)\n",
        "      2. T·∫£i v·ªÅ m√°y local qua browser (ch·∫≠m h∆°n, nh∆∞ng an to√†n nh·∫•t)\n",
        "    \"\"\"\n",
        "    m = model_letter.lower()\n",
        "    patterns = [\n",
        "        f'checkpoints/model_{m}_resume.pth',\n",
        "        f'checkpoints/model_{m}_best.pth',\n",
        "        f'checkpoints/model_{m}_epoch*.pth',\n",
        "        f'checkpoints/history_model_{m}.json',\n",
        "    ]\n",
        "\n",
        "    # === L·ªõp 1: Sync l√™n Drive ===\n",
        "    drive_total = 0\n",
        "    os.makedirs(f'{DRIVE_DIR}/checkpoints', exist_ok=True)\n",
        "    for pat in patterns:\n",
        "        for f in glob.glob(pat):\n",
        "            dst = os.path.join(f'{DRIVE_DIR}/checkpoints', os.path.basename(f))\n",
        "            try:\n",
        "                shutil.copy2(f, dst)\n",
        "                drive_total += 1\n",
        "            except OSError as e:\n",
        "                print(f\"  ‚ö†Ô∏è Drive sync failed for {os.path.basename(f)}: {e}\")\n",
        "                print(f\"     (Drive c√≥ th·ªÉ ƒë√£ h·∫øt dung l∆∞·ª£ng)\")\n",
        "    if drive_total > 0:\n",
        "        print(f\"  üíæ Drive sync Model {model_letter.upper()}: {drive_total} files ‚Üí Drive/checkpoints/\")\n",
        "\n",
        "    # === L·ªõp 2: T·∫£i v·ªÅ m√°y local ===\n",
        "    download_to_local(model_letter)\n",
        "\n",
        "def auto_sync_all():\n",
        "    \"\"\"Sync t·∫•t c·∫£ checkpoints + history c·ªßa 4 models l√™n Drive + t·∫£i v·ªÅ m√°y local.\"\"\"\n",
        "    print(\"=== Auto-syncing ALL checkpoints ===\")\n",
        "    for m in ['A', 'B', 'C', 'D']:\n",
        "        auto_sync_model(m)\n",
        "    print(\"‚úì Done!\")\n",
        "\n",
        "def download_all_to_local():\n",
        "    \"\"\"T·∫£i T·∫§T C·∫¢ checkpoints c·ªßa 4 models v·ªÅ m√°y local (kh√¥ng qua Drive).\"\"\"\n",
        "    print(\"=== Downloading ALL checkpoints to local machine ===\")\n",
        "    total = 0\n",
        "    for m in ['A', 'B', 'C', 'D']:\n",
        "        total += download_to_local(m)\n",
        "    print(f\"\\n‚úì T·ªïng: {total} files downloaded\")\n",
        "\n",
        "print(\"Helper functions defined:\")\n",
        "print(\"  sync_to_drive()       ‚Äî copy l√™n Drive\")\n",
        "print(\"  restore_from_drive()  ‚Äî restore t·ª´ Drive v·ªÅ Colab\")\n",
        "print(\"  auto_sync_model('A')  ‚Äî sync Drive + t·∫£i v·ªÅ m√°y local (1 model)\")\n",
        "print(\"  auto_sync_all()       ‚Äî sync Drive + t·∫£i v·ªÅ m√°y local (4 models)\")\n",
        "print(\"  download_to_local('A') ‚Äî ch·ªâ t·∫£i v·ªÅ m√°y local (1 model)\")\n",
        "print(\"  download_all_to_local() ‚Äî ch·ªâ t·∫£i v·ªÅ m√°y local (4 models)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "03dbb8fb",
      "metadata": {
        "id": "03dbb8fb"
      },
      "source": [
        "### ‚ö†Ô∏è Restore t·ª´ Drive (ch·∫°y khi runtime restart)\n",
        "\n",
        "N·∫øu Colab **b·ªã disconnect/kill** gi·ªØa ch·ª´ng, ch·∫°y l·∫°i c√°c cell theo th·ª© t·ª±:\n",
        "1. **Mount Drive** (cell tr√™n)\n",
        "2. **Helper functions** (cell tr√™n ‚Äî ƒë·ªãnh nghƒ©a `sync_to_drive`, `restore_from_drive`, `auto_sync_model`)\n",
        "3. **Cell restore d∆∞·ªõi ƒë√¢y** ‚Äî kh√¥i ph·ª•c checkpoints + vocab t·ª´ Drive v·ªÅ local\n",
        "\n",
        "> **C∆° ch·∫ø b·∫£o v·ªá:** M·ªói cell training ƒë·ªÅu t·ª± ƒë·ªông g·ªçi `auto_sync_model()` ƒë·ªÉ l∆∞u checkpoint l√™n Drive **ngay sau khi train xong**. N·∫øu Colab b·ªã kill gi·ªØa Phase (v√≠ d·ª•: train xong Model B nh∆∞ng ch∆∞a train Model C), ch·ªâ c·∫ßn restore ‚Üí ch·∫°y ti·∫øp t·ª´ Model C.\n",
        "\n",
        "> Sau khi restore, **b·ªè qua** c√°c cell training ƒë√£ ho√†n th√†nh v√† ch·∫°y ti·∫øp t·ª´ model/phase ti·∫øp theo."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "56b5bc25",
      "metadata": {
        "id": "56b5bc25"
      },
      "outputs": [],
      "source": [
        "# ‚ö†Ô∏è RESTORE ‚Äî Ch·ªâ ch·∫°y khi runtime restart v√† c·∫ßn resume training\n",
        "# N·∫øu ƒë√¢y l√† l·∫ßn ch·∫°y ƒë·∫ßu ti√™n, B·ªé QUA cell n√†y\n",
        "\n",
        "print(\"=== Restoring from Google Drive ===\")\n",
        "n1 = restore_from_drive('checkpoints', 'checkpoints', 'Checkpoints')\n",
        "n2 = restore_from_drive('vocab', 'data/processed', 'Vocab')\n",
        "n3 = restore_from_drive('outputs', 'checkpoints', 'Outputs (curves, plots)')\n",
        "\n",
        "if n1 + n2 + n3 == 0:\n",
        "    print(\"\\n  Kh√¥ng c√≥ g√¨ ƒë·ªÉ restore ‚Äî c√≥ th·ªÉ ƒë√¢y l√† l·∫ßn ch·∫°y ƒë·∫ßu ti√™n.\")\n",
        "else:\n",
        "    print(f\"\\n  ‚úì Restore ho√†n t·∫•t! T·ªïng: {n1+n2+n3} files\")\n",
        "    print(\"  ‚Üí Ti·∫øp t·ª•c training t·ª´ phase ti·∫øp theo (d√πng --resume)\")\n",
        "    # Hi·ªán checkpoints ƒë√£ restore\n",
        "    if os.path.exists('checkpoints'):\n",
        "        print(\"\\n  Checkpoints hi·ªán c√≥:\")\n",
        "        for f in sorted(os.listdir('checkpoints')):\n",
        "            if f.endswith('.pth') or f.endswith('.json'):\n",
        "                sz = os.path.getsize(f'checkpoints/{f}') / 1e6\n",
        "                print(f\"    {f:45s} {sz:8.1f} MB\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "04358e10",
      "metadata": {},
      "outputs": [],
      "source": [
        "# üìã Ki·ªÉm tra tr·∫°ng th√°i Drive ‚Äî xem ƒë√£ c√≥ nh·ªØng checkpoint n√†o\n",
        "import os\n",
        "\n",
        "drive_ckpt = f'{DRIVE_DIR}/checkpoints'\n",
        "if os.path.exists(drive_ckpt):\n",
        "    print(\"=== Checkpoints tr√™n Google Drive ===\\n\")\n",
        "    files = sorted(os.listdir(drive_ckpt))\n",
        "    \n",
        "    # Ph√¢n lo·∫°i theo model\n",
        "    for model in ['A', 'B', 'C', 'D']:\n",
        "        m = model.lower()\n",
        "        model_files = [f for f in files if f'model_{m}' in f or f'history_model_{m}' in f]\n",
        "        if model_files:\n",
        "            print(f\"  Model {model}:\")\n",
        "            for f in model_files:\n",
        "                sz = os.path.getsize(os.path.join(drive_ckpt, f)) / 1e6\n",
        "                tag = \"\"\n",
        "                if 'epoch10' in f: tag = \" ‚Üê Phase 1 milestone\"\n",
        "                elif 'epoch15' in f: tag = \" ‚Üê Phase 2 milestone\"\n",
        "                elif 'epoch20' in f: tag = \" ‚Üê Phase 3 milestone\"\n",
        "                elif 'resume' in f: tag = \" ‚Üê resume point\"\n",
        "                elif 'best' in f: tag = \" ‚Üê best val loss\"\n",
        "                print(f\"    {f:45s} {sz:7.1f} MB{tag}\")\n",
        "            print()\n",
        "    \n",
        "    # X√°c ƒë·ªãnh phase hi·ªán t·∫°i cho m·ªói model\n",
        "    print(\"=== Tr·∫°ng th√°i training ===\\n\")\n",
        "    for model in ['A', 'B', 'C', 'D']:\n",
        "        m = model.lower()\n",
        "        has_e10 = any('epoch10' in f and f'model_{m}' in f for f in files)\n",
        "        has_e15 = any('epoch15' in f and f'model_{m}' in f for f in files)\n",
        "        has_e20 = any('epoch20' in f and f'model_{m}' in f for f in files)\n",
        "        has_resume = any('resume' in f and f'model_{m}' in f for f in files)\n",
        "        \n",
        "        if has_e20:\n",
        "            status = \"‚úÖ Phase 3 DONE (epoch 20)\"\n",
        "        elif has_e15:\n",
        "            status = \"‚è≥ Phase 2 done ‚Üí C·∫ßn ch·∫°y Phase 3\"\n",
        "        elif has_e10:\n",
        "            status = \"‚è≥ Phase 1 done ‚Üí C·∫ßn ch·∫°y Phase 2\"\n",
        "        elif has_resume:\n",
        "            status = \"‚è≥ ƒêang train (c√≥ resume checkpoint)\"\n",
        "        else:\n",
        "            status = \"‚ùå Ch∆∞a b·∫Øt ƒë·∫ßu\"\n",
        "        print(f\"  Model {model}: {status}\")\n",
        "    \n",
        "    print(\"\\n‚Üí D·ª±a v√†o tr·∫°ng th√°i tr√™n, ch·∫°y ti·∫øp t·ª´ phase/model t∆∞∆°ng ·ª©ng.\")\n",
        "else:\n",
        "    print(\"Ch∆∞a c√≥ checkpoints tr√™n Drive ‚Äî ƒë√¢y l√† l·∫ßn ch·∫°y ƒë·∫ßu ti√™n.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b48ad651",
      "metadata": {
        "id": "b48ad651"
      },
      "source": [
        "---\n",
        "## Step 1 ‚Äî Download VQA 2.0 Data t·ª´ Kaggle\n",
        "\n",
        "T·∫£i 3 datasets:\n",
        "- **vqa-20-images**: COCO train2014 images\n",
        "- **vqa-2-0-val2014**: COCO val2014 images\n",
        "- **vqa2-0-data-json**: VQA 2.0 question + annotation JSON files\n",
        "\n",
        "> **Note:** C·∫ßn c·∫•u h√¨nh Kaggle API key tr∆∞·ªõc (upload `kaggle.json` ho·∫∑c set bi·∫øn m√¥i tr∆∞·ªùng)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1bdcf6ff",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 73
        },
        "id": "1bdcf6ff",
        "outputId": "2fa31772-2869-4811-b042-4bf4068a991e"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-6971d2ef-12ae-49d7-a1b2-2a27cd20e0e5\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-6971d2ef-12ae-49d7-a1b2-2a27cd20e0e5\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saving kaggle.json to kaggle.json\n"
          ]
        }
      ],
      "source": [
        "# N·∫øu ch∆∞a c√≥ kaggle.json, upload n√≥:\n",
        "from google.colab import files\n",
        "files.upload()  # upload kaggle.json\n",
        "!mkdir -p ~/.kaggle && mv kaggle.json ~/.kaggle/ && chmod 600 ~/.kaggle/kaggle.json\n",
        "\n",
        "!pip install -q kaggle"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c22c71bd",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c22c71bd",
        "outputId": "5a8952e6-f409-49cf-a061-2731bc3daca5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dataset URL: https://www.kaggle.com/datasets/bishoyabdelmassieh/vqa-20-images\n",
            "License(s): unknown\n",
            "Downloading vqa-20-images.zip to datasets\n",
            "100% 24.8G/24.8G [01:31<00:00, 239MB/s]\n",
            "100% 24.8G/24.8G [01:31<00:00, 290MB/s]\n",
            "Dataset URL: https://www.kaggle.com/datasets/hongnhnnguyntrn/vqa-2-0-val2014\n",
            "License(s): unknown\n",
            "Downloading vqa-2-0-val2014.zip to datasets\n",
            "100% 6.15G/6.15G [00:16<00:00, 409MB/s]\n",
            "100% 6.15G/6.15G [00:16<00:00, 393MB/s]\n",
            "Dataset URL: https://www.kaggle.com/datasets/hongnhnnguyntrn/vqa2-0-data-json\n",
            "License(s): unknown\n",
            "Downloading vqa2-0-data-json.zip to datasets\n",
            "  0% 0.00/50.4M [00:00<?, ?B/s]\n",
            "100% 50.4M/50.4M [00:00<00:00, 4.23GB/s]\n"
          ]
        }
      ],
      "source": [
        "# T·∫£i d·ªØ li·ªáu t·ª´ Kaggle\n",
        "!kaggle datasets download -d bishoyabdelmassieh/vqa-20-images -p datasets --unzip\n",
        "!kaggle datasets download -d hongnhnnguyntrn/vqa-2-0-val2014 -p datasets --unzip\n",
        "!kaggle datasets download -d hongnhnnguyntrn/vqa2-0-data-json -p datasets --unzip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3ca3f08c",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3ca3f08c",
        "outputId": "d57ebe63-5ab9-45c9-8be8-85995e80fc0a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloaded files:\n",
            "datasets/\n",
            "  test2015/\n",
            "    test2015/\n",
            "  vqa_data_json/\n",
            "    v2_Questions_Val_mscoco/\n",
            "    v2_Questions_Train_mscoco/\n",
            "    v2_Questions_Test_mscoco/\n",
            "    v2_Annotations_Train_mscoco/\n",
            "    v2_Annotations_Val_mscoco/\n",
            "  train2014/\n",
            "    train2014/\n",
            "  val2014/\n",
            "    COCO_val2014_000000383518.jpg\n",
            "    COCO_val2014_000000302405.jpg\n",
            "    COCO_val2014_000000091654.jpg\n",
            "    COCO_val2014_000000295857.jpg\n",
            "    COCO_val2014_000000311964.jpg\n",
            "    ... (40504 files total)\n"
          ]
        }
      ],
      "source": [
        "# Ki·ªÉm tra c·∫•u tr√∫c dataset ƒë√£ t·∫£i\n",
        "import os\n",
        "print(\"Downloaded files:\")\n",
        "for root, dirs, files in os.walk('datasets'):\n",
        "    level = root.replace('datasets', '').count(os.sep)\n",
        "    indent = ' ' * 2 * level\n",
        "    print(f\"{indent}{os.path.basename(root)}/\")\n",
        "    if level < 2:  # ch·ªâ hi·ªán 2 levels ƒë·∫ßu\n",
        "        subindent = ' ' * 2 * (level + 1)\n",
        "        for f in files[:5]:\n",
        "            print(f\"{subindent}{f}\")\n",
        "        if len(files) > 5:\n",
        "            print(f\"{subindent}... ({len(files)} files total)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a5b09220",
      "metadata": {
        "id": "a5b09220"
      },
      "source": [
        "### S·∫Øp x·∫øp d·ªØ li·ªáu v√†o ƒë√∫ng c·∫•u tr√∫c th∆∞ m·ª•c project\n",
        "\n",
        "Project y√™u c·∫ßu c·∫•u tr√∫c:\n",
        "```\n",
        "data/raw/images/train2014/   ‚Üê COCO train images\n",
        "data/raw/images/val2014/     ‚Üê COCO val images  \n",
        "data/raw/vqa_json/           ‚Üê VQA 2.0 JSON files\n",
        "data/processed/              ‚Üê vocab files (s·∫Ω ƒë∆∞·ª£c t·∫°o ·ªü step sau)\n",
        "```\n",
        "\n",
        "> **Quan tr·ªçng:** Cell d∆∞·ªõi s·∫Ω t·∫°o symlinks/move d·ªØ li·ªáu v√†o ƒë√∫ng v·ªã tr√≠. H√£y ki·ªÉm tra output c·ªßa cell tr√™n ƒë·ªÉ x√°c nh·∫≠n ƒë∆∞·ªùng d·∫´n ch√≠nh x√°c, n·∫øu c·∫•u tr√∫c Kaggle kh√°c th√¨ s·ª≠a l·∫°i cell d∆∞·ªõi."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cbcf1ee5",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cbcf1ee5",
        "outputId": "466bff1b-0cb2-4123-fb91-a2d81662a7fb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Linked train2014: datasets/train2014/train2014 -> data/raw/images/train2014\n",
            "Linked val2014: datasets/val2014 -> data/raw/images/val2014\n",
            "  Copied: datasets/vqa_data_json/v2_Questions_Train_mscoco/v2_OpenEnded_mscoco_train2014_questions.json -> data/raw/vqa_json/v2_OpenEnded_mscoco_train2014_questions.json\n",
            "  Copied: datasets/vqa_data_json/v2_Questions_Val_mscoco/v2_OpenEnded_mscoco_val2014_questions.json -> data/raw/vqa_json/v2_OpenEnded_mscoco_val2014_questions.json\n",
            "  Copied: datasets/vqa_data_json/v2_Annotations_Train_mscoco/v2_mscoco_train2014_annotations.json -> data/raw/vqa_json/v2_mscoco_train2014_annotations.json\n",
            "  Copied: datasets/vqa_data_json/v2_Annotations_Val_mscoco/v2_mscoco_val2014_annotations.json -> data/raw/vqa_json/v2_mscoco_val2014_annotations.json\n",
            "\n",
            "--- Verification ---\n",
            "  data/raw/images/train2014: 82,783 files\n",
            "  data/raw/images/val2014: 40,504 files\n",
            "  data/raw/vqa_json/v2_OpenEnded_mscoco_train2014_questions.json: 42.0 MB\n",
            "  data/raw/vqa_json/v2_OpenEnded_mscoco_val2014_questions.json: 20.2 MB\n",
            "  data/raw/vqa_json/v2_mscoco_train2014_annotations.json: 355.5 MB\n",
            "  data/raw/vqa_json/v2_mscoco_val2014_annotations.json: 171.8 MB\n"
          ]
        }
      ],
      "source": [
        "import os, glob, shutil\n",
        "\n",
        "# T·∫°o th∆∞ m·ª•c ƒë√≠ch\n",
        "os.makedirs('data/raw/images', exist_ok=True)\n",
        "os.makedirs('data/raw/vqa_json', exist_ok=True)\n",
        "os.makedirs('data/processed', exist_ok=True)\n",
        "os.makedirs('checkpoints', exist_ok=True)\n",
        "\n",
        "# ‚îÄ‚îÄ Helper: t√¨m th∆∞ m·ª•c ch·ª©a COCO images ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
        "def find_coco_dir(base, split):\n",
        "    \"\"\"T√¨m th∆∞ m·ª•c ch·ª©a ·∫£nh COCO_<split>_*.jpg trong base.\"\"\"\n",
        "    for root, dirs, files in os.walk(base):\n",
        "        for f in files:\n",
        "            if f.startswith(f'COCO_{split}_') and f.endswith('.jpg'):\n",
        "                return root\n",
        "    return None\n",
        "\n",
        "# ‚îÄ‚îÄ Symlink train2014 images ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
        "train_dir = find_coco_dir('datasets', 'train2014')\n",
        "if train_dir and not os.path.exists('data/raw/images/train2014'):\n",
        "    os.symlink(os.path.abspath(train_dir), 'data/raw/images/train2014')\n",
        "    print(f\"Linked train2014: {train_dir} -> data/raw/images/train2014\")\n",
        "elif os.path.exists('data/raw/images/train2014'):\n",
        "    print(\"train2014 already exists.\")\n",
        "else:\n",
        "    print(\"WARNING: Could not find train2014 images in datasets/\")\n",
        "\n",
        "# ‚îÄ‚îÄ Symlink val2014 images ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
        "val_dir = find_coco_dir('datasets', 'val2014')\n",
        "if val_dir and not os.path.exists('data/raw/images/val2014'):\n",
        "    os.symlink(os.path.abspath(val_dir), 'data/raw/images/val2014')\n",
        "    print(f\"Linked val2014: {val_dir} -> data/raw/images/val2014\")\n",
        "elif os.path.exists('data/raw/images/val2014'):\n",
        "    print(\"val2014 already exists.\")\n",
        "else:\n",
        "    print(\"WARNING: Could not find val2014 images in datasets/\")\n",
        "\n",
        "# ‚îÄ‚îÄ Copy VQA JSON files ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
        "json_patterns = [\n",
        "    'v2_OpenEnded_mscoco_train2014_questions.json',\n",
        "    'v2_OpenEnded_mscoco_val2014_questions.json',\n",
        "    'v2_mscoco_train2014_annotations.json',\n",
        "    'v2_mscoco_val2014_annotations.json',\n",
        "]\n",
        "for jname in json_patterns:\n",
        "    dst = f'data/raw/vqa_json/{jname}'\n",
        "    if os.path.exists(dst):\n",
        "        print(f\"  Already exists: {dst}\")\n",
        "        continue\n",
        "    # T√¨m file trong datasets/\n",
        "    matches = glob.glob(f'datasets/**/{jname}', recursive=True)\n",
        "    if matches:\n",
        "        shutil.copy2(matches[0], dst)\n",
        "        print(f\"  Copied: {matches[0]} -> {dst}\")\n",
        "    else:\n",
        "        print(f\"  WARNING: {jname} not found in datasets/\")\n",
        "\n",
        "# ‚îÄ‚îÄ Verify ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
        "print(\"\\n--- Verification ---\")\n",
        "for p in ['data/raw/images/train2014', 'data/raw/images/val2014']:\n",
        "    if os.path.exists(p):\n",
        "        n = len(os.listdir(p))\n",
        "        print(f\"  {p}: {n:,} files\")\n",
        "    else:\n",
        "        print(f\"  MISSING: {p}\")\n",
        "for p in json_patterns:\n",
        "    full = f'data/raw/vqa_json/{p}'\n",
        "    sz = os.path.getsize(full) / 1e6 if os.path.exists(full) else 0\n",
        "    print(f\"  {full}: {sz:.1f} MB\" if sz > 0 else f\"  MISSING: {full}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "442f1d33",
      "metadata": {
        "id": "442f1d33"
      },
      "source": [
        "---\n",
        "## Step 2 ‚Äî Build Vocabulary\n",
        "\n",
        "X√¢y d·ª±ng:\n",
        "- **Question vocabulary**: c√°c t·ª´ xu·∫•t hi·ªán >= 3 l·∫ßn trong training questions\n",
        "- **Answer vocabulary**: c√°c c√¢u tr·∫£ l·ªùi xu·∫•t hi·ªán >= 5 l·∫ßn\n",
        "\n",
        "Output:\n",
        "- `data/processed/vocab_questions.json`\n",
        "- `data/processed/vocab_answers.json`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e3145d1e",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e3145d1e",
        "outputId": "755274b3-a57f-4856-c979-5b2f301b86f8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "1. Reading question file: data/raw/vqa_json/v2_OpenEnded_mscoco_train2014_questions.json\n",
            "Building question vocabulary...\n",
            " -> Tokenizing...\n",
            " -> Filtering (threshold=3)...\n",
            " -> Done. Vocab size: 8174\n",
            "Done! Saved to: data/processed/vocab_questions.json\n",
            "\n",
            "2. Reading annotation file: data/raw/vqa_json/v2_mscoco_train2014_annotations.json\n",
            "Building answer vocabulary...\n",
            " -> Tokenizing...\n",
            " -> Filtering (threshold=5)...\n",
            " -> Done. Vocab size: 3706\n",
            "Done! Saved to: data/processed/vocab_answers.json\n"
          ]
        }
      ],
      "source": [
        "!python src/scripts/1_build_vocab.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "59448812",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "59448812",
        "outputId": "37e93974-ca18-4618-b202-a36890e8c402"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Question vocab size: 8174\n",
            "Answer vocab size  : 3706\n",
            "\n",
            "Sample question words: ['<pad>', '<start>', '<end>', '<unk>', 'what', 'is', 'this', 'photo', 'taken', 'looking', 'through', 'position', 'man', 'playing', 'color']\n",
            "Sample answer words  : ['<pad>', '<start>', '<end>', '<unk>', 'net', 'pitcher', 'orange', 'yes', 'white', 'skiing', 'red', 'frisbee', 'and', 'purple', 'brushing']\n"
          ]
        }
      ],
      "source": [
        "# Ki·ªÉm tra vocab ƒë√£ t·∫°o\n",
        "import json\n",
        "\n",
        "with open('data/processed/vocab_questions.json') as f:\n",
        "    vq = json.load(f)\n",
        "with open('data/processed/vocab_answers.json') as f:\n",
        "    va = json.load(f)\n",
        "\n",
        "print(f\"Question vocab size: {len(vq['word2idx'])}\")\n",
        "print(f\"Answer vocab size  : {len(va['word2idx'])}\")\n",
        "print(f\"\\nSample question words: {list(vq['word2idx'].keys())[:15]}\")\n",
        "print(f\"Sample answer words  : {list(va['word2idx'].keys())[:15]}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "da31e69f",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "da31e69f",
        "outputId": "a0e7e946-2f56-4e0b-bfe8-abd12bc7e7b7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=== Syncing vocab to Drive ===\n",
            "  ‚úì Vocab files: 2 files (0.4 MB) ‚Üí Drive/vocab/\n"
          ]
        }
      ],
      "source": [
        "# üíæ L∆∞u vocab l√™n Drive (nh·ªè, nh∆∞ng quan tr·ªçng ƒë·ªÉ resume)\n",
        "print(\"=== Syncing vocab to Drive ===\")\n",
        "sync_to_drive('data/processed/vocab_*.json', 'vocab', 'Vocab files')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "aa32bb7f",
      "metadata": {
        "id": "aa32bb7f"
      },
      "source": [
        "---\n",
        "## ƒê√°nh gi√° d·ª±a v√†o ƒë·ªô ƒëo n√†o? T·∫°i sao?\n",
        "\n",
        "B√†i to√°n VQA v·ªõi output d·∫°ng **generative** (LSTM-Decoder sinh c√¢u tr·∫£ l·ªùi token-by-token) c·∫ßn nhi·ªÅu g√≥c ƒë√°nh gi√° kh√°c nhau. Ch√∫ng t√¥i s·ª≠ d·ª•ng **7 metrics** sau:\n",
        "\n",
        "### 1. VQA Accuracy (Metric ch√≠nh)\n",
        "$$\\text{VQA Acc}(a) = \\min\\left(\\frac{\\text{s·ªë annotators tr·∫£ l·ªùi gi·ªëng prediction}}{3},\\; 1.0\\right)$$\n",
        "\n",
        "- ƒê√¢y l√† **official metric** c·ªßa VQA Challenge (Antol et al., 2015).\n",
        "- M·ªói c√¢u h·ªèi c√≥ **10 annotators** tr·∫£ l·ªùi ‚Üí n·∫øu ‚â•3 ng∆∞·ªùi ƒë·ªìng √Ω v·ªõi prediction ‚Üí ƒëi·ªÉm t·ªëi ƒëa.\n",
        "- **T·∫°i sao ch·ªçn**: Metric n√†y ph·∫£n √°nh th·ª±c t·∫ø r·∫±ng nhi·ªÅu c√¢u h·ªèi c√≥ nhi·ªÅu ƒë√°p √°n h·ª£p l·ªá (v√≠ d·ª•: \"red\" v√† \"dark red\" ƒë·ªÅu ƒë√∫ng).\n",
        "\n",
        "### 2. Exact Match\n",
        "- So kh·ªõp ch√≠nh x√°c gi·ªØa prediction v√† ground truth (majority answer).\n",
        "- **T·∫°i sao ch·ªçn**: Metric ƒë∆°n gi·∫£n nh·∫•t, d·ªÖ hi·ªÉu, nh∆∞ng **qu√° nghi√™m** ‚Äî kh√¥ng cho ph√©p c√°c bi·∫øn th·ªÉ h·ª£p l·ªá.\n",
        "\n",
        "### 3. BLEU-1, BLEU-2, BLEU-3, BLEU-4 (Papineni et al., 2002)\n",
        "$$\\text{BLEU-N} = \\text{BP} \\times \\exp\\left(\\sum_{n=1}^{N} w_n \\log p_n\\right)$$\n",
        "\n",
        "- ƒêo **n-gram precision** gi·ªØa predicted answer v√† ground truth.\n",
        "- BLEU-1: unigram (t·ª´ ƒë∆°n), BLEU-4: 4-gram (c·ª•m 4 t·ª´).\n",
        "- **T·∫°i sao ch·ªçn**: Metric chu·∫©n cho **text generation** (machine translation, image captioning). BLEU-4 ƒë·∫∑c bi·ªát quan tr·ªçng v√¨ ƒëo kh·∫£ nƒÉng sinh c·ª•m t·ª´ ƒë√∫ng, kh√¥ng ch·ªâ t·ª´ ƒë∆°n.\n",
        "\n",
        "### 4. METEOR (Banerjee & Lavie, 2005)\n",
        "- X√©t **synonyms + stemming + alignment** gi·ªØa prediction v√† ground truth.\n",
        "- **T·∫°i sao ch·ªçn**: B√π ƒë·∫Øp nh∆∞·ª£c ƒëi·ªÉm c·ªßa BLEU ‚Äî BLEU ch·ªâ so kh·ªõp exact n-gram, c√≤n METEOR hi·ªÉu r·∫±ng \"car\" v√† \"automobile\" l√† c√πng nghƒ©a. T∆∞∆°ng quan v·ªõi ƒë√°nh gi√° con ng∆∞·ªùi t·ªët h∆°n BLEU.\n",
        "\n",
        "### T·ªïng k·∫øt l·ª±a ch·ªçn metrics\n",
        "\n",
        "| Metric | ƒê·∫∑c ƒëi·ªÉm | Vai tr√≤ |\n",
        "|--------|----------|---------|\n",
        "| **VQA Accuracy** | Multi-annotator, official | Metric **ch√≠nh** ƒë·ªÉ x·∫øp h·∫°ng |\n",
        "| **Exact Match** | Strict matching | Baseline ƒë∆°n gi·∫£n |\n",
        "| **BLEU-1‚Üí4** | N-gram precision | ƒê√°nh gi√° ch·∫•t l∆∞·ª£ng text generation |\n",
        "| **METEOR** | Synonym-aware | B·ªï sung cho BLEU, x√©t ng·ªØ nghƒ©a |\n",
        "\n",
        "> **VQA Accuracy** l√† metric quy·∫øt ƒë·ªãnh khi so s√°nh c√°c model, c√°c metric c√≤n l·∫°i cung c·∫•p g√≥c nh√¨n b·ªï sung v·ªÅ ch·∫•t l∆∞·ª£ng sinh c√¢u tr·∫£ l·ªùi."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "48d13083",
      "metadata": {
        "id": "48d13083"
      },
      "source": [
        "---\n",
        "## Step 3 ‚Äî Training Strategy\n",
        "\n",
        "### T·∫°i sao c·∫ßn chia th√†nh nhi·ªÅu Phase?\n",
        "\n",
        "Training m·ªôt VQA model hi·ªáu qu·∫£ **kh√¥ng n√™n l√†m t·∫•t c·∫£ c√πng l√∫c**. C√≥ 3 k·ªπ thu·∫≠t c·∫ßn √°p d·ª•ng **tu·∫ßn t·ª±**, m·ªói k·ªπ thu·∫≠t ch·ªâ hi·ªáu qu·∫£ khi k·ªπ thu·∫≠t tr∆∞·ªõc ƒë√£ ho√†n th√†nh:\n",
        "\n",
        "| Phase | K·ªπ thu·∫≠t | √Åp d·ª•ng cho | L√Ω do ph·∫£i l√†m tu·∫ßn t·ª± |\n",
        "|-------|---------|------------|----------------------|\n",
        "| **1 ‚Äî Baseline** | Teacher Forcing, ResNet frozen | C·∫£ 4 models | Decoder + Q-Encoder c·∫ßn h·ªçc c√°ch s·ª≠ d·ª•ng features tr∆∞·ªõc |\n",
        "| **2 ‚Äî Fine-tune** | Unfreeze ResNet (B,D) / Continue training (A,C) | C·∫£ 4 models | ResNet ch·ªâ n√™n adapt khi decoder ·ªïn ƒë·ªãnh; A/C train th√™m ƒë·ªÉ c√¥ng b·∫±ng |\n",
        "| **3 ‚Äî Scheduled Sampling** | D·∫ßn thay GT b·∫±ng model prediction | C·∫£ 4 models | Model ph·∫£i predict t∆∞∆°ng ƒë·ªëi ƒë√∫ng tr∆∞·ªõc, n·∫øu kh√¥ng SS s·∫Ω feed garbage |\n",
        "\n",
        "> **Nguy√™n t·∫Øc c√¥ng b·∫±ng:** M·ªói phase √°p d·ª•ng cho **t·∫•t c·∫£ 4 models** v·ªõi c√πng s·ªë epochs **v√† c√πng batch size (`bs=256`)**. Evaluate + Compare sau **m·ªói phase** ƒë·ªÉ th·∫•y progression. ƒê√¢y l√† controlled experiment ‚Äî thay ƒë·ªïi duy nh·∫•t gi·ªØa c√°c models l√† **ki·∫øn tr√∫c** (CNNEncoder + c√≥/kh√¥ng Attention).\n",
        "\n",
        "### V√¨ sao KH√îNG unfreeze ResNet ngay t·ª´ ƒë·∫ßu?\n",
        "\n",
        "> ResNet101 pretrained ƒë√£ h·ªçc features r·∫•t t·ªët t·ª´ ImageNet. N·∫øu unfreeze ngay v·ªõi `lr=1e-3`, **gradient t·ª´ random decoder** s·∫Ω l√† noise, ph√° h·ªßy pretrained weights (catastrophic forgetting) tr∆∞·ªõc khi decoder k·ªãp h·ªçc. **Chu·∫©n practice** (Show Attend & Tell, Bottom-Up Top-Down): freeze tr∆∞·ªõc ‚Üí unfreeze sau.\n",
        "\n",
        "### V√¨ sao KH√îNG d√πng Scheduled Sampling ngay t·ª´ ƒë·∫ßu?\n",
        "\n",
        "> ·ªû epoch ƒë·∫ßu, model predict g·∫ßn nh∆∞ random. Scheduled Sampling s·∫Ω feed **garbage tokens** l√†m input ‚Üí training ch·∫≠m 2-3√ó, loss kh√≥ gi·∫£m, gradient noisy. SS ch·ªâ c√≥ √Ω nghƒ©a khi model ƒë√£ ƒë·∫°t prediction t∆∞∆°ng ƒë·ªëi ƒë√∫ng ‚Üí \"h·ªçc c√°ch recover t·ª´ l·ªói nh·ªè\" thay v√¨ \"b·ªã ƒë·∫ßu ƒë·ªôc b·ªüi noise\".\n",
        "\n",
        "### Tham s·ªë\n",
        "\n",
        "| Parameter | Value | Ghi ch√∫ |\n",
        "|-----------|-------|---------|\n",
        "| `embed_size` | 512 | Chu·∫©n cho VQA |\n",
        "| `hidden_size` | 1024 | Chu·∫©n cho VQA |\n",
        "| `num_layers` | 2 | ƒê·ªß cho LSTM decoder |\n",
        "| `batch_size` | 256 | Th·ªëng nh·∫•t cho c·∫£ 3 phases, 4 models ‚Äî 102GB VRAM cho ph√©p |\n",
        "| AMP | BFloat16 | T·ª± detect Blackwell Ampere+ ‚Üí BF16, ~2√ó faster |\n",
        "| TF32 | Auto-enabled | Near-FP32 accuracy cho matmul + conv |\n",
        "| `cudnn.benchmark` | True | Auto-tune conv algorithms |\n",
        "| `grad_clip` | 5.0 | Stabilize training |\n",
        "| `num_workers` | 8 | T·∫≠n d·ª•ng bandwidth |\n",
        "| Scheduler | ReduceLROnPlateau | factor=0.5, patience=2 |\n",
        "\n",
        "### Ch·ªëng Overfitting ‚Äî Regularization Strategy\n",
        "\n",
        "| K·ªπ thu·∫≠t | Gi√° tr·ªã | T√°c d·ª•ng |\n",
        "|----------|---------|----------|\n",
        "| **Weight Decay** (L2) | `1e-5` | Penalize large weights ‚Üí ngƒÉn model memorize training data |\n",
        "| **Embedding Dropout** | `0.5` | Dropout tr√™n embedding layer (c·∫£ LSTMDecoder v√† LSTMDecoderWithAttention) |\n",
        "| **LSTM Dropout** | `0.5` | Dropout gi·ªØa LSTM layers (khi `num_layers > 1`) |\n",
        "| **Early Stopping** | `patience=3` | D·ª´ng training n·∫øu val loss kh√¥ng c·∫£i thi·ªán sau 3 epochs li√™n ti·∫øp |\n",
        "| **ReduceLROnPlateau** | `patience=2` | Gi·∫£m LR √ó 0.5 khi val loss plateau |\n",
        "\n",
        "> **T·∫°i sao c·∫ßn regularization?** V·ªõi ~443K training samples nh∆∞ng model c√≥ h√†ng tri·ªáu parameters (ƒë·∫∑c bi·ªát khi unfreeze ResNet ~41M params ·ªü Phase 2), model r·∫•t d·ªÖ overfit ‚Äî train loss gi·∫£m nh∆∞ng val loss tƒÉng. Regularization ƒëi·ªÅu h√≤a gi·ªØa **model capacity** v√† **generalization**.\n",
        "\n",
        "### Batch Size ‚Äî Controlled Experiment\n",
        "\n",
        "> **Nguy√™n t·∫Øc:** T·∫•t c·∫£ 4 models d√πng **c√πng `batch_size`** ƒë·ªÉ ƒë·∫£m b·∫£o so s√°nh c√¥ng b·∫±ng khoa h·ªçc. Batch size kh√°c nhau d·∫´n ƒë·∫øn:\n",
        "> - **S·ªë gradient updates/epoch kh√°c nhau** (inversely proportional)\n",
        "> - **Implicit regularization kh√°c nhau** (smaller batch ‚Üí more noise ‚Üí more regularization)\n",
        "> - **Effective learning rate kh√°c nhau** (theo linear scaling rule)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3ff255f1",
      "metadata": {
        "id": "3ff255f1"
      },
      "source": [
        "### Phase 1 ‚Äî Baseline Training (Teacher Forcing, ResNet Frozen)\n",
        "\n",
        "Train 4 ki·∫øn tr√∫c v·ªõi **pure teacher forcing** v√† ResNet **frozen** (Model B, D).\n",
        "\n",
        "**M·ª•c ti√™u:** Decoder + Question Encoder h·ªôi t·ª• tr∆∞·ªõc, h·ªçc c√°ch s·ª≠ d·ª•ng image features.\n",
        "\n",
        "| Model | Encoder | Attention | batch_size | ∆Ø·ªõc t√≠nh th·ªùi gian/epoch |\n",
        "|-------|---------|-----------|------------|------------------------|\n",
        "| A | Scratch CNN (5 conv blocks) | No | 256 | ~15 min |\n",
        "| B | ResNet101 (frozen) | No | 256 | ~10 min |\n",
        "| C | Scratch CNN Spatial (49 regions) | Bahdanau | 256 | ~20 min |\n",
        "| D | ResNet101 Spatial (frozen) | Bahdanau | 256 | ~15 min |"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8y0WnFGgfR15",
      "metadata": {
        "id": "8y0WnFGgfR15"
      },
      "source": [
        "### RUN 1 TIMES ONLY, OPTIMIZE FOR I/O"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "yccWwpvSeclG",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yccWwpvSeclG",
        "outputId": "0b64d008-2ea3-4ae6-debc-3192c6cd4f43"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Copying train2014 to RAM...\n",
            "Copying val2014 to RAM...\n",
            "Done!\n"
          ]
        }
      ],
      "source": [
        "# Copy ·∫£nh v√†o RAM disk ƒë·ªÉ lo·∫°i b·ªè disk I/O bottleneck\n",
        "import shutil, os\n",
        "\n",
        "print(\"Copying train2014 to RAM...\")\n",
        "shutil.copytree(\n",
        "    'data/raw/images/train2014',\n",
        "    '/dev/shm/train2014',\n",
        "    dirs_exist_ok=True\n",
        ")\n",
        "\n",
        "print(\"Copying val2014 to RAM...\")\n",
        "shutil.copytree(\n",
        "    'data/raw/images/val2014',\n",
        "    '/dev/shm/val2014',\n",
        "    dirs_exist_ok=True\n",
        ")\n",
        "\n",
        "print(\"Done!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "WgGazb8cedlH",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WgGazb8cedlH",
        "outputId": "b84c2de6-8f3b-4dd9-80a7-418bed2ac736"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Symlinks updated to RAM disk\n"
          ]
        }
      ],
      "source": [
        "# Tr·ªè l·∫°i data path sang RAM\n",
        "import os\n",
        "os.remove('data/raw/images/train2014')\n",
        "os.remove('data/raw/images/val2014')\n",
        "os.symlink('/dev/shm/train2014', 'data/raw/images/train2014')\n",
        "os.symlink('/dev/shm/val2014',   'data/raw/images/val2014')\n",
        "print(\"Symlinks updated to RAM disk\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0622b048",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0622b048",
        "outputId": "3cee021b-f74b-4598-c75f-6e24388dff7c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train: 443757 | Val: 214354\n",
            "Weight decay     : 1.0e-05\n",
            "/content/new_vqa/src/train.py:267: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler = GradScaler(enabled=(use_amp and not use_bf16))\n",
            "AMP: BFloat16 (Ampere+ detected ‚Äî no GradScaler needed)\n",
            "Model: A | Device: cuda\n",
            "Early stopping   : patience=3\n",
            "  0% 0/10 [00:00<?, ?it/s]/content/new_vqa/src/train.py:333: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with autocast(enabled=use_amp, dtype=amp_dtype):\n",
            "/content/new_vqa/src/train.py:370: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with autocast(enabled=use_amp, dtype=amp_dtype):\n",
            "Epoch 1/10 | Train: 2.6029 | Val: 2.4877 | LR: 1.00e-03\n",
            "  -> New best val loss: 2.4877. Saved best checkpoint.\n",
            " 10% 1/10 [04:33<41:02, 273.65s/it]Epoch 2/10 | Train: 2.4653 | Val: 2.4207 | LR: 1.00e-03\n",
            "  -> New best val loss: 2.4207. Saved best checkpoint.\n",
            " 20% 2/10 [08:55<35:34, 266.86s/it]Epoch 3/10 | Train: 2.3928 | Val: 2.3617 | LR: 1.00e-03\n",
            "  -> New best val loss: 2.3617. Saved best checkpoint.\n",
            " 30% 3/10 [13:18<30:54, 264.97s/it]Epoch 4/10 | Train: 2.3352 | Val: 2.3300 | LR: 1.00e-03\n",
            "  -> New best val loss: 2.3300. Saved best checkpoint.\n",
            " 40% 4/10 [17:42<26:26, 264.48s/it]"
          ]
        }
      ],
      "source": [
        "# Phase 1 ‚Äî Train Model A: Scratch CNN, No Attention\n",
        "!python src/train.py --model A --epochs 10 --lr 1e-3 --batch_size 256 --num_workers 12 \\\n",
        "    --weight_decay 1e-5 --early_stopping 3\n",
        "auto_sync_model('A')  # üíæ L∆∞u ngay l√™n Drive sau khi train xong"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "462a35d6",
      "metadata": {
        "id": "462a35d6"
      },
      "outputs": [],
      "source": [
        "# Phase 1 ‚Äî Train Model B: ResNet101 (pretrained, frozen), No Attention\n",
        "!python src/train.py --model B --epochs 10 --lr 1e-3 --batch_size 256 --num_workers 12 \\\n",
        "    --weight_decay 1e-5 --early_stopping 3\n",
        "auto_sync_model('B')  # üíæ L∆∞u ngay l√™n Drive sau khi train xong"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bf4e8460",
      "metadata": {
        "id": "bf4e8460"
      },
      "outputs": [],
      "source": [
        "# Phase 1 ‚Äî Train Model C: Scratch CNN Spatial, Bahdanau Attention\n",
        "!python src/train.py --model C --epochs 10 --lr 1e-3 --batch_size 256 --num_workers 12 \\\n",
        "    --weight_decay 1e-5 --early_stopping 3\n",
        "auto_sync_model('C')  # üíæ L∆∞u ngay l√™n Drive sau khi train xong"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d6a2d160",
      "metadata": {
        "id": "d6a2d160"
      },
      "outputs": [],
      "source": [
        "# Phase 1 ‚Äî Train Model D: ResNet101 Spatial (pretrained, frozen), Bahdanau Attention\n",
        "!python src/train.py --model D --epochs 10 --lr 1e-3 --batch_size 256 --num_workers 12 \\\n",
        "    --weight_decay 1e-5 --early_stopping 3\n",
        "auto_sync_model('D')  # üíæ L∆∞u ngay l√™n Drive sau khi train xong"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "04679148",
      "metadata": {
        "id": "04679148"
      },
      "outputs": [],
      "source": [
        "# Ki·ªÉm tra checkpoints Phase 1\n",
        "import os\n",
        "print(\"Saved checkpoints after Phase 1:\")\n",
        "for f in sorted(os.listdir('checkpoints')):\n",
        "    sz = os.path.getsize(f'checkpoints/{f}') / 1e6\n",
        "    print(f\"  {f:45s} {sz:8.1f} MB\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f24af4c3",
      "metadata": {
        "id": "f24af4c3"
      },
      "outputs": [],
      "source": [
        "# Sync Phase 1 checkpoints ‚Üí Drive\n",
        "print(\"=== Syncing Phase 1 checkpoints to Drive ===\")\n",
        "sync_to_drive('checkpoints/model_*_resume.pth', 'checkpoints', 'Resume checkpoints')\n",
        "sync_to_drive('checkpoints/model_*_best.pth', 'checkpoints', 'Best checkpoints')\n",
        "sync_to_drive('checkpoints/model_*_epoch10.pth', 'checkpoints', 'Epoch 10 (milestone)')\n",
        "sync_to_drive('checkpoints/model_*_history.json', 'checkpoints', 'Training history')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "61c73f4d",
      "metadata": {
        "id": "61c73f4d"
      },
      "source": [
        "#### Evaluate & Compare ‚Äî Sau Phase 1 (Baseline)\n",
        "\n",
        "So s√°nh c√¥ng b·∫±ng l·∫ßn 1: T·∫•t c·∫£ 4 models c√πng ƒëi·ªÅu ki·ªán (10 epochs, teacher forcing, ResNet frozen).\n",
        "\n",
        "ƒê√¢y l√† **controlled experiment** ‚Äî ch·ªâ kh√°c nhau v·ªÅ ki·∫øn tr√∫c (scratch vs pretrained, no attn vs attn)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "58ee1465",
      "metadata": {
        "id": "58ee1465"
      },
      "outputs": [],
      "source": [
        "# So s√°nh 4 models sau Phase 1 (epoch 10)\n",
        "!python src/compare.py --models A,B,C,D --epoch 10"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "09d503f5",
      "metadata": {
        "id": "09d503f5"
      },
      "source": [
        "#### Ph√¢n t√≠ch k·∫øt qu·∫£ Phase 1"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d4680ea0",
      "metadata": {
        "id": "d4680ea0"
      },
      "source": [
        "### Phase 2 ‚Äî Fine-tune / Continue Training (5 epochs, t·∫•t c·∫£ 4 models)\n",
        "\n",
        "Sau Phase 1, decoder + question encoder ƒë√£ h·ªôi t·ª•. Phase 2 √°p d·ª•ng cho **c·∫£ 4 models** ƒë·ªÉ ƒë·∫£m b·∫£o so s√°nh c√¥ng b·∫±ng:\n",
        "\n",
        "| Model | K·ªπ thu·∫≠t Phase 2 | L√Ω do |\n",
        "|-------|-----------------|-------|\n",
        "| **A** | Continue training (lr gi·∫£m) | Scratch CNN ƒë√£ train end-to-end, ti·∫øp t·ª•c t·ªëi ∆∞u |\n",
        "| **B** | **Unfreeze layer3+4** + differential LR | Adapt pretrained features cho VQA domain |\n",
        "| **C** | Continue training (lr gi·∫£m) | Scratch CNN ƒë√£ train end-to-end, ti·∫øp t·ª•c t·ªëi ∆∞u |\n",
        "| **D** | **Unfreeze layer3+4** + differential LR | Adapt pretrained features cho VQA domain |\n",
        "\n",
        "**Differential Learning Rate (Model B, D):**\n",
        "- Backbone (layer3+4): `lr √ó 0.1 = 5e-5` ‚Äî thay ƒë·ªïi ch·∫≠m, gi·ªØ pretrained knowledge\n",
        "- Head (decoder + Q-Encoder): `lr = 5e-4` ‚Äî adapt nhanh h∆°n\n",
        "\n",
        "**Model A, C:** C≈©ng gi·∫£m LR xu·ªëng `5e-4` v√† train th√™m 5 epochs ~ c√πng t·ªïng epochs v·ªõi B, D.\n",
        "\n",
        "| Model | batch_size | LR (head) | LR (backbone) | Epochs |\n",
        "|-------|-----------|-----------|---------------|--------|\n",
        "| A | 256 | 5e-4 | ‚Äî | 5 |\n",
        "| B | 256 | 5e-4 | 5e-5 | 5 |\n",
        "| C | 256 | 5e-4 | ‚Äî | 5 |\n",
        "| D | 256 | 5e-4 | 5e-5 | 5 |"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b94268b6",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b94268b6",
        "outputId": "83242d6b-6bc3-452b-9024-564e5416c96b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "^C\n"
          ]
        }
      ],
      "source": [
        "# Phase 2 ‚Äî Continue training Model A (resume, lower LR)\n",
        "!python src/train.py --model A --epochs 5 --lr 5e-4 --batch_size 256 --num_workers 12 \\\n",
        "    --resume checkpoints/model_a_resume.pth \\\n",
        "    --weight_decay 1e-5 --early_stopping 3\n",
        "auto_sync_model('A')  # üíæ L∆∞u ngay l√™n Drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bb009614",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bb009614",
        "outputId": "052e40a6-95f7-4790-e003-c68081ca436e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"/content/new_vqa/src/train.py\", line 1, in <module>\n",
            "    import torch\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/__init__.py\", line 2204, in <module>\n",
            "    from torch import _VF as _VF, functional as functional  # usort: skip\n",
            "    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/functional.py\", line 8, in <module>\n",
            "    import torch.nn.functional as F\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/__init__.py\", line 8, in <module>\n",
            "    from torch.nn.modules import *  # usort: skip # noqa: F403\n",
            "    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/__init__.py\", line 1, in <module>\n",
            "    from .module import Module  # usort: skip\n",
            "    ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\", line 17, in <module>\n",
            "    from torch.utils._python_dispatch import is_traceable_wrapper_subclass\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/utils/__init__.py\", line 8, in <module>\n",
            "    from torch.utils import (\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/utils/data/__init__.py\", line 1, in <module>\n",
            "    from torch.utils.data.dataloader import (\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py\", line 26, in <module>\n",
            "    import torch.utils.data.graph_settings\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/utils/data/graph_settings.py\", line 8, in <module>\n",
            "    from torch.utils.data.datapipes.iter.sharding import (\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/utils/data/datapipes/__init__.py\", line 1, in <module>\n",
            "    from torch.utils.data.datapipes import dataframe as dataframe, iter as iter, map as map\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/utils/data/datapipes/iter/__init__.py\", line 9, in <module>\n",
            "    from torch.utils.data.datapipes.iter.combining import (\n",
            "  File \"<frozen importlib._bootstrap>\", line 1360, in _find_and_load\n",
            "  File \"<frozen importlib._bootstrap>\", line 1331, in _find_and_load_unlocked\n",
            "  File \"<frozen importlib._bootstrap>\", line 935, in _load_unlocked\n",
            "  File \"<frozen importlib._bootstrap_external>\", line 995, in exec_module\n",
            "  File \"<frozen importlib._bootstrap_external>\", line 1128, in get_code\n",
            "  File \"<frozen importlib._bootstrap_external>\", line 757, in _compile_bytecode\n",
            "KeyboardInterrupt\n",
            "^C\n"
          ]
        }
      ],
      "source": [
        "# Phase 2 ‚Äî Fine-tune Model B: resume t·ª´ Phase 1 + unfreeze layer3+layer4\n",
        "!python src/train.py --model B --epochs 5 --lr 5e-4 --batch_size 256 --num_workers 12 \\\n",
        "    --resume checkpoints/model_b_resume.pth \\\n",
        "    --finetune_cnn --cnn_lr_factor 0.1 \\\n",
        "    --weight_decay 1e-5 --early_stopping 3\n",
        "auto_sync_model('B')  # üíæ L∆∞u ngay l√™n Drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "79ec032e",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "79ec032e",
        "outputId": "63840908-5632-4dec-ee10-3aaea6ac4a65"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"/content/new_vqa/src/train.py\", line 17, in <module>\n",
            "    from dataset import VQADataset, vqa_collate_fn\n",
            "  File \"/content/new_vqa/src/dataset.py\", line 15, in <module>\n",
            "    from torchvision import transforms\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torchvision/__init__.py\", line 10, in <module>\n",
            "    from torchvision import _meta_registrations, datasets, io, models, ops, transforms, utils  # usort:skip\n",
            "    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torchvision/models/__init__.py\", line 2, in <module>\n",
            "    from .convnext import *\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torchvision/models/convnext.py\", line 9, in <module>\n",
            "    from ..ops.misc import Conv2dNormActivation, Permute\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torchvision/ops/__init__.py\", line 23, in <module>\n",
            "    from .poolers import MultiScaleRoIAlign\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torchvision/ops/poolers.py\", line 10, in <module>\n",
            "    from .roi_align import roi_align\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torchvision/ops/roi_align.py\", line 7, in <module>\n",
            "    from torch._dynamo.utils import is_compile_supported\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/_dynamo/__init__.py\", line 13, in <module>\n",
            "    from . import (\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/_dynamo/aot_compile.py\", line 14, in <module>\n",
            "    from torch._dynamo.convert_frame import GraphRuntimeEnv\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/_dynamo/convert_frame.py\", line 60, in <module>\n",
            "    from torch._dynamo.symbolic_convert import TensorifyState\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/_dynamo/symbolic_convert.py\", line 59, in <module>\n",
            "    from . import (\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/_dynamo/trace_rules.py\", line 59, in <module>\n",
            "    from .variables import (\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/__init__.py\", line 19, in <module>\n",
            "    from .base import VariableTracker\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/base.py\", line 824, in <module>\n",
            "    from . import builder\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/builder.py\", line 93, in <module>\n",
            "    from ..pgo import (\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/_dynamo/pgo.py\", line 40, in <module>\n",
            "    from torch.compiler._cache import (\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/compiler/_cache.py\", line 105, in <module>\n",
            "    @dataclasses.dataclass\n",
            "     ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/dataclasses.py\", line 1275, in dataclass\n",
            "    return wrap(cls)\n",
            "           ^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/dataclasses.py\", line 1265, in wrap\n",
            "    return _process_class(cls, init, repr, eq, order, unsafe_hash,\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/dataclasses.py\", line 1063, in _process_class\n",
            "    _init_fn(all_init_fields,\n",
            "  File \"/usr/lib/python3.12/dataclasses.py\", line 619, in _init_fn\n",
            "    return _create_fn('__init__',\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/dataclasses.py\", line 473, in _create_fn\n",
            "    exec(txt, globals, ns)\n",
            "  File \"<string>\", line 0, in <module>\n",
            "KeyboardInterrupt\n"
          ]
        }
      ],
      "source": [
        "# Phase 2 ‚Äî Continue training Model C (resume, lower LR)\n",
        "!python src/train.py --model C --epochs 5 --lr 5e-4 --batch_size 256 --num_workers 12 \\\n",
        "    --resume checkpoints/model_c_resume.pth \\\n",
        "    --weight_decay 1e-5 --early_stopping 3\n",
        "auto_sync_model('C')  # üíæ L∆∞u ngay l√™n Drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5565d677",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5565d677",
        "outputId": "764dc2fa-2726-4436-b041-2e75711aad2b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"<frozen importlib._bootstrap>\", line 1331, in _find_and_load_unlocked\n",
            "  File \"<frozen importlib._bootstrap>\", line 935, in _load_unlocked\n",
            "  File \"<frozen importlib._bootstrap_external>\", line 999, in exec_module\n",
            "  File \"<frozen importlib._bootstrap>\", line 488, in _call_with_frames_removed\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/_dynamo/exc.py\", line 44, in <module>\n",
            "    from .utils import counters\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/_dynamo/utils.py\", line 2129, in <module>\n",
            "    @dataclasses.dataclass\n",
            "     ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/dataclasses.py\", line 1275, in dataclass\n",
            "    return wrap(cls)\n",
            "           ^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/dataclasses.py\", line 1265, in wrap\n",
            "    return _process_class(cls, init, repr, eq, order, unsafe_hash,\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/dataclasses.py\", line 1063, in _process_class\n",
            "    _init_fn(all_init_fields,\n",
            "  File \"/usr/lib/python3.12/dataclasses.py\", line 619, in _init_fn\n",
            "    return _create_fn('__init__',\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/dataclasses.py\", line 473, in _create_fn\n",
            "    exec(txt, globals, ns)\n",
            "  File \"<string>\", line 0, in <module>\n",
            "KeyboardInterrupt\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/new_vqa/src/train.py\", line 17, in <module>\n",
            "    from dataset import VQADataset, vqa_collate_fn\n",
            "  File \"/content/new_vqa/src/dataset.py\", line 15, in <module>\n",
            "    from torchvision import transforms\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torchvision/__init__.py\", line 10, in <module>\n",
            "    from torchvision import _meta_registrations, datasets, io, models, ops, transforms, utils  # usort:skip\n",
            "    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torchvision/models/__init__.py\", line 2, in <module>\n",
            "    from .convnext import *\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torchvision/models/convnext.py\", line 9, in <module>\n",
            "    from ..ops.misc import Conv2dNormActivation, Permute\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torchvision/ops/__init__.py\", line 23, in <module>\n",
            "    from .poolers import MultiScaleRoIAlign\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torchvision/ops/poolers.py\", line 10, in <module>\n",
            "    from .roi_align import roi_align\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torchvision/ops/roi_align.py\", line 7, in <module>\n",
            "    from torch._dynamo.utils import is_compile_supported\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/_dynamo/__init__.py\", line 13, in <module>\n",
            "    from . import (\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/_dynamo/aot_compile.py\", line 14, in <module>\n",
            "    from torch._dynamo.convert_frame import GraphRuntimeEnv\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/_dynamo/convert_frame.py\", line 60, in <module>\n",
            "    from torch._dynamo.symbolic_convert import TensorifyState\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/_dynamo/symbolic_convert.py\", line 53, in <module>\n",
            "    from torch._dynamo.exc import ObservedException, TensorifyScalarRestartAnalysis\n",
            "  File \"<frozen importlib._bootstrap>\", line 1360, in _find_and_load\n",
            "  File \"<frozen importlib._bootstrap>\", line 1334, in _find_and_load_unlocked\n",
            "KeyboardInterrupt\n"
          ]
        }
      ],
      "source": [
        "# Phase 2 ‚Äî Fine-tune Model D: resume t·ª´ Phase 1 + unfreeze layer3+layer4\n",
        "!python src/train.py --model D --epochs 5 --lr 5e-4 --batch_size 256 --num_workers 12 \\\n",
        "    --resume checkpoints/model_d_resume.pth \\\n",
        "    --finetune_cnn --cnn_lr_factor 0.1 \\\n",
        "    --weight_decay 1e-5 --early_stopping 3\n",
        "auto_sync_model('D')  # üíæ L∆∞u ngay l√™n Drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "32929059",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "32929059",
        "outputId": "0607ba7f-d6c7-4964-80c4-adb6ff6b8ea9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saved checkpoints after Phase 2 (fine-tuning):\n",
            "  history_model_a.json                               0.0 MB\n",
            "  model_a_best.pth                                 186.4 MB\n",
            "  model_a_resume.pth                               559.3 MB\n"
          ]
        }
      ],
      "source": [
        "# Ki·ªÉm tra checkpoints sau Phase 2\n",
        "import os\n",
        "print(\"Saved checkpoints after Phase 2 (fine-tuning):\")\n",
        "for f in sorted(os.listdir('checkpoints')):\n",
        "    sz = os.path.getsize(f'checkpoints/{f}') / 1e6\n",
        "    print(f\"  {f:45s} {sz:8.1f} MB\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a7aac375",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 408
        },
        "id": "a7aac375",
        "outputId": "8b3cd958-a980-40ee-a248-b683ff1d9f55"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=== Syncing Phase 2 checkpoints to Drive ===\n",
            "  ‚úì Resume checkpoints: 1 files (559.3 MB) ‚Üí Drive/checkpoints/\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-436/3536563886.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"=== Syncing Phase 2 checkpoints to Drive ===\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0msync_to_drive\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'checkpoints/model_*_resume.pth'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'checkpoints'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Resume checkpoints'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0msync_to_drive\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'checkpoints/model_*_best.pth'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'checkpoints'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Best checkpoints'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0msync_to_drive\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'checkpoints/model_*_epoch15.pth'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'checkpoints'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Epoch 15 (milestone)'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0msync_to_drive\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'checkpoints/model_*_history.json'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'checkpoints'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Training history'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-436/3883983321.py\u001b[0m in \u001b[0;36msync_to_drive\u001b[0;34m(src_pattern, drive_subdir, label)\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mf\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfiles\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0mdst\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdst_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbasename\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m         \u001b[0mshutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdst\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m     \u001b[0mnames\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbasename\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mf\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfiles\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0mtotal_mb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetsize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mf\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfiles\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;36m1e6\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/shutil.py\u001b[0m in \u001b[0;36mcopy2\u001b[0;34m(src, dst, follow_symlinks)\u001b[0m\n\u001b[1;32m    473\u001b[0m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    474\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 475\u001b[0;31m     \u001b[0mcopyfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdst\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfollow_symlinks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfollow_symlinks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    476\u001b[0m     \u001b[0mcopystat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdst\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfollow_symlinks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfollow_symlinks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    477\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdst\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/shutil.py\u001b[0m in \u001b[0;36mcopyfile\u001b[0;34m(src, dst, follow_symlinks)\u001b[0m\n\u001b[1;32m    271\u001b[0m                     \u001b[0;32melif\u001b[0m \u001b[0m_USE_CP_SENDFILE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    272\u001b[0m                         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 273\u001b[0;31m                             \u001b[0m_fastcopy_sendfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfsrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfdst\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    274\u001b[0m                             \u001b[0;32mreturn\u001b[0m \u001b[0mdst\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    275\u001b[0m                         \u001b[0;32mexcept\u001b[0m \u001b[0m_GiveupOnFastCopy\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/shutil.py\u001b[0m in \u001b[0;36m_fastcopy_sendfile\u001b[0;34m(fsrc, fdst)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m             \u001b[0msent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msendfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutfd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moffset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mblocksize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mOSError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m             \u001b[0;31m# ...in oder to have a more informative exception.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "# üíæ Sync Phase 2 checkpoints ‚Üí Drive\n",
        "print(\"=== Syncing Phase 2 checkpoints to Drive ===\")\n",
        "sync_to_drive('checkpoints/model_*_resume.pth', 'checkpoints', 'Resume checkpoints')\n",
        "sync_to_drive('checkpoints/model_*_best.pth', 'checkpoints', 'Best checkpoints')\n",
        "sync_to_drive('checkpoints/model_*_epoch15.pth', 'checkpoints', 'Epoch 15 (milestone)')\n",
        "sync_to_drive('checkpoints/model_*_history.json', 'checkpoints', 'Training history')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "68617b64",
      "metadata": {
        "id": "68617b64"
      },
      "source": [
        "#### Evaluate & Compare ‚Äî Sau Phase 2 (Fine-tune / Continue)\n",
        "\n",
        "So s√°nh c√¥ng b·∫±ng l·∫ßn 2: T·∫•t c·∫£ 4 models c√πng c√≥ **15 epochs t·ªïng**.\n",
        "\n",
        "- Model B, D: ƒë∆∞·ª£c h∆∞·ªüng l·ª£i t·ª´ unfreeze ResNet ‚Üí pretrained features adapt cho VQA\n",
        "- Model A, C: ti·∫øp t·ª•c t·ªëi ∆∞u v·ªõi scratch CNN\n",
        "\n",
        "So s√°nh n√†y cho th·∫•y **·∫£nh h∆∞·ªüng th·ª±c s·ª± c·ªßa fine-tuning pretrained backbone**."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3bc9019d",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3bc9019d",
        "outputId": "0e82c144-162c-43c1-9885-9bd6b322c014"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"/content/new_vqa/src/compare.py\", line 204, in <module>\n",
            "    main()\n",
            "  File \"/content/new_vqa/src/compare.py\", line 170, in main\n",
            "    val_dataset = VQADataset(\n",
            "                  ^^^^^^^^^^^\n",
            "  File \"/content/new_vqa/src/dataset.py\", line 91, in __init__\n",
            "    annotations = json.load(f)['annotations']\n",
            "                  ^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/json/__init__.py\", line 293, in load\n",
            "    return loads(fp.read(),\n",
            "           ^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/json/__init__.py\", line 346, in loads\n",
            "    return _default_decoder.decode(s)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/json/decoder.py\", line 338, in decode\n",
            "    obj, end = self.raw_decode(s, idx=_w(s, 0).end())\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/json/decoder.py\", line 354, in raw_decode\n",
            "    obj, end = self.scan_once(s, idx)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^\n",
            "KeyboardInterrupt\n"
          ]
        }
      ],
      "source": [
        "# So s√°nh 4 models sau Phase 2 (epoch 15)\n",
        "!python src/compare.py --models A,B,C,D --epoch 15"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8b22b3cc",
      "metadata": {
        "id": "8b22b3cc"
      },
      "source": [
        "#### Ph√¢n t√≠ch k·∫øt qu·∫£ Phase 2 ‚Äî Fine-tune / Continue Training\n",
        "\n",
        "**So s√°nh Phase 2 vs Phase 1 ‚Äî ·∫¢nh h∆∞·ªüng c·ªßa Fine-tuning:**\n",
        "\n",
        "1. **Model B, D (Unfreeze ResNet layer3+4):**\n",
        "   - Pretrained ResNet ƒë∆∞·ª£c train tr√™n ImageNet (object classification) ‚Üí features t·ªët nh∆∞ng **ch∆∞a t·ªëi ∆∞u cho VQA**.\n",
        "   - Unfreeze top layers cho ph√©p ResNet **adapt features cho VQA domain** ‚Äî v√≠ d·ª•: h·ªçc bi·ªÉu di·ªÖn t·ªët h∆°n cho counting, spatial relationships, colors.\n",
        "   - **Differential LR** (backbone: 5e-5, head: 5e-4) ngƒÉn **catastrophic forgetting** ‚Äî gi·ªØ pretrained knowledge ·ªü early layers, ch·ªâ tinh ch·ªânh high-level features.\n",
        "\n",
        "2. **Model A, C (Continue training):**\n",
        "   - Scratch CNN ti·∫øp t·ª•c t·ªëi ∆∞u v·ªõi LR th·∫•p h∆°n (5e-4 vs 1e-3).\n",
        "   - C·∫£i thi·ªán marginal ‚Äî ph·∫ßn l·ªõn learning ƒë√£ x·∫£y ra ·ªü Phase 1.\n",
        "   - ƒê·∫£m b·∫£o **so s√°nh c√¥ng b·∫±ng**: t·ªïng epochs b·∫±ng nhau cho t·∫•t c·∫£ models.\n",
        "\n",
        "3. **K·ª≥ v·ªçng c·∫£i thi·ªán:**\n",
        "   - B, D c·∫£i thi·ªán **ƒë√°ng k·ªÉ** nh·ªù unfreeze CNN ‚Üí features adapt cho VQA.\n",
        "   - A, C c·∫£i thi·ªán **nh·∫π** ‚Äî ch·ªß y·∫øu t·ª´ continued optimization.\n",
        "   - Gap gi·ªØa pretrained vs scratch **m·ªü r·ªông** sau phase n√†y.\n",
        "\n",
        "> **Key insight:** Fine-tuning pretrained backbone l√† k·ªπ thu·∫≠t quan tr·ªçng ‚Äî nh∆∞ng **ch·ªâ hi·ªáu qu·∫£ khi decoder ƒë√£ ·ªïn ƒë·ªãnh** (Phase 1). N·∫øu unfreeze ngay t·ª´ ƒë·∫ßu, gradient noise t·ª´ random decoder s·∫Ω ph√° h·ªßy pretrained weights."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "48264d56",
      "metadata": {
        "id": "48264d56"
      },
      "source": [
        "### Phase 3 ‚Äî Scheduled Sampling (5 epochs, t·∫•t c·∫£ 4 models)\n",
        "\n",
        "√Åp d·ª•ng Scheduled Sampling cho **c·∫£ 4 models** ƒë·ªÉ so s√°nh c√¥ng b·∫±ng.\n",
        "\n",
        "**C∆° ch·∫ø:**\n",
        "- M·ªói decode step, v·ªõi x√°c su·∫•t `Œµ` d√πng GT token, `(1-Œµ)` d√πng model's prediction\n",
        "- `Œµ` gi·∫£m d·∫ßn theo inverse-sigmoid decay: `Œµ(epoch) = k / (k + exp(epoch/k))`\n",
        "- `ss_k=5`: t·ªëc ƒë·ªô decay v·ª´a ph·∫£i\n",
        "\n",
        "**T·∫°i sao ch·ªâ √°p d·ª•ng ·ªü Phase 3?**\n",
        "> Model ƒë√£ predict t∆∞∆°ng ƒë·ªëi ƒë√∫ng sau Phase 1+2 ‚Üí SS gi√∫p \"h·ªçc c√°ch recover t·ª´ l·ªói nh·ªè\" thay v√¨ \"b·ªã ƒë·∫ßu ƒë·ªôc b·ªüi garbage tokens\" nh∆∞ khi √°p d·ª•ng ngay t·ª´ ƒë·∫ßu.\n",
        "\n",
        "| Model | batch_size | LR (head) | LR (backbone) | ss_k | Epochs |\n",
        "|-------|-----------|-----------|---------------|------|--------|\n",
        "| A | 256 | 2e-4 | ‚Äî | 5 | 5 |\n",
        "| B | 256 | 2e-4 | 2e-5 | 5 | 5 |\n",
        "| C | 256 | 2e-4 | ‚Äî | 5 | 5 |\n",
        "| D | 256 | 2e-4 | 2e-5 | 5 | 5 |\n",
        "\n",
        "> T·ªïng m·ªói model: **20 epochs** (10 + 5 + 5). **`batch_size=256` xuy√™n su·ªët c·∫£ 3 phases** ‚Äî controlled experiment ho√†n h·∫£o. So s√°nh sau Phase 3 = so s√°nh cu·ªëi c√πng."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6d29f328",
      "metadata": {
        "id": "6d29f328"
      },
      "outputs": [],
      "source": [
        "# Phase 3 ‚Äî Scheduled Sampling cho Model A\n",
        "!python src/train.py --model A --epochs 5 --lr 2e-4 --batch_size 256 --num_workers 12 \\\n",
        "    --resume checkpoints/model_a_resume.pth \\\n",
        "    --scheduled_sampling --ss_k 5 \\\n",
        "    --weight_decay 1e-5\n",
        "auto_sync_model('A')  # üíæ L∆∞u ngay l√™n Drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "704359ac",
      "metadata": {
        "id": "704359ac"
      },
      "outputs": [],
      "source": [
        "# Phase 3 ‚Äî Scheduled Sampling cho Model B (gi·ªØ unfreeze CNN)\n",
        "!python src/train.py --model B --epochs 5 --lr 2e-4 --batch_size 256 --num_workers 12 \\\n",
        "    --resume checkpoints/model_b_resume.pth \\\n",
        "    --finetune_cnn --cnn_lr_factor 0.1 \\\n",
        "    --scheduled_sampling --ss_k 5 \\\n",
        "    --weight_decay 1e-5\n",
        "auto_sync_model('B')  # üíæ L∆∞u ngay l√™n Drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1975c63b",
      "metadata": {
        "id": "1975c63b"
      },
      "outputs": [],
      "source": [
        "# Phase 3 ‚Äî Scheduled Sampling cho Model C\n",
        "!python src/train.py --model C --epochs 5 --lr 2e-4 --batch_size 256 --num_workers 12 \\\n",
        "    --resume checkpoints/model_c_resume.pth \\\n",
        "    --scheduled_sampling --ss_k 5 \\\n",
        "    --weight_decay 1e-5\n",
        "auto_sync_model('C')  # üíæ L∆∞u ngay l√™n Drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0753006f",
      "metadata": {
        "id": "0753006f"
      },
      "outputs": [],
      "source": [
        "# Phase 3 ‚Äî Scheduled Sampling cho Model D (gi·ªØ unfreeze CNN)\n",
        "!python src/train.py --model D --epochs 5 --lr 2e-4 --batch_size 256 --num_workers 12 \\\n",
        "    --resume checkpoints/model_d_resume.pth \\\n",
        "    --finetune_cnn --cnn_lr_factor 0.1 \\\n",
        "    --scheduled_sampling --ss_k 5 \\\n",
        "    --weight_decay 1e-5\n",
        "auto_sync_model('D')  # üíæ L∆∞u ngay l√™n Drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2ef01b72",
      "metadata": {
        "id": "2ef01b72"
      },
      "outputs": [],
      "source": [
        "# Ki·ªÉm tra checkpoints sau Phase 3\n",
        "import os\n",
        "print(\"Saved checkpoints after Phase 3 (scheduled sampling):\")\n",
        "for f in sorted(os.listdir('checkpoints')):\n",
        "    sz = os.path.getsize(f'checkpoints/{f}') / 1e6\n",
        "    print(f\"  {f:45s} {sz:8.1f} MB\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "439f8e3c",
      "metadata": {
        "id": "439f8e3c"
      },
      "outputs": [],
      "source": [
        "# üíæ Sync Phase 3 checkpoints ‚Üí Drive (FINAL)\n",
        "print(\"=== Syncing Phase 3 checkpoints to Drive (FINAL) ===\")\n",
        "sync_to_drive('checkpoints/model_*_resume.pth', 'checkpoints', 'Resume checkpoints')\n",
        "sync_to_drive('checkpoints/model_*_best.pth', 'checkpoints', 'Best checkpoints')\n",
        "sync_to_drive('checkpoints/model_*_epoch20.pth', 'checkpoints', 'Epoch 20 (milestone)')\n",
        "sync_to_drive('checkpoints/model_*_history.json', 'checkpoints', 'Training history')\n",
        "print(\"\\n‚úì T·∫•t c·∫£ checkpoints cu·ªëi c√πng ƒë√£ l∆∞u an to√†n tr√™n Drive!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1d781f07",
      "metadata": {
        "id": "1d781f07"
      },
      "source": [
        "#### Evaluate & Compare ‚Äî Sau Phase 3 (Scheduled Sampling) ‚Äî Final\n",
        "\n",
        "So s√°nh c√¥ng b·∫±ng l·∫ßn 3 (cu·ªëi c√πng): T·∫•t c·∫£ 4 models c√πng **20 epochs**, c√πng √°p d·ª•ng Scheduled Sampling.\n",
        "\n",
        "ƒê√¢y l√† **k·∫øt qu·∫£ ch√≠nh** ƒë·ªÉ ƒë∆∞a v√†o b√°o c√°o ‚Äî controlled experiment v·ªõi c·∫£ 3 bi·∫øn:\n",
        "1. **Scratch vs Pretrained**: A vs B, C vs D\n",
        "2. **No Attention vs Attention**: A vs C, B vs D\n",
        "3. **Progression**: Phase 1 ‚Üí 2 ‚Üí 3 cho th·∫•y ·∫£nh h∆∞·ªüng c·ªßa fine-tuning v√† scheduled sampling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "51788e9e",
      "metadata": {
        "id": "51788e9e"
      },
      "outputs": [],
      "source": [
        "# So s√°nh cu·ªëi c√πng: 4 models sau Phase 3 (epoch 20)\n",
        "!python src/compare.py --models A,B,C,D --epoch 20"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e7b436ce",
      "metadata": {
        "id": "e7b436ce"
      },
      "source": [
        "#### Ph√¢n t√≠ch k·∫øt qu·∫£ Phase 3 ‚Äî Scheduled Sampling (Final)\n",
        "\n",
        "**So s√°nh Phase 3 vs Phase 2 ‚Äî ·∫¢nh h∆∞·ªüng c·ªßa Scheduled Sampling:**\n",
        "\n",
        "1. **Scheduled Sampling gi·∫£i quy·∫øt Exposure Bias:**\n",
        "   - Training d√πng **teacher forcing** (ground truth input) nh∆∞ng inference d√πng **model's own predictions**.\n",
        "   - S·ª± kh√°c bi·ªát n√†y g·ªçi l√† **exposure bias** ‚Äî model ch∆∞a bao gi·ªù th·∫•y input sai c·ªßa ch√≠nh m√¨nh trong training.\n",
        "   - SS d·∫ßn thay GT b·∫±ng model prediction: $\\epsilon(epoch) = \\frac{k}{k + e^{epoch/k}}$ ‚Üí model h·ªçc **recover t·ª´ l·ªói nh·ªè**.\n",
        "\n",
        "2. **C·∫£i thi·ªán d·ª± ki·∫øn:**\n",
        "   - T·∫•t c·∫£ 4 models ƒë·ªÅu h∆∞·ªüng l·ª£i t·ª´ SS, nh∆∞ng m·ª©c ƒë·ªô kh√°c nhau.\n",
        "   - Model ƒë√£ predict t∆∞∆°ng ƒë·ªëi ƒë√∫ng (B, D) ‚Üí SS gi√∫p polish th√™m.\n",
        "   - Model y·∫øu h∆°n (A) ‚Üí SS c≈©ng gi√∫p, nh∆∞ng n·∫øu prediction qu√° k√©m th√¨ SS c√≥ th·ªÉ kh√¥ng gi√∫p nhi·ªÅu.\n",
        "\n",
        "3. **K·∫øt qu·∫£ t·ªïng h·ª£p ‚Äî Ranking cu·ªëi c√πng:**\n",
        "\n",
        "   | Rank | Model | ƒê·∫∑c ƒëi·ªÉm | L√Ω do |\n",
        "   |------|-------|----------|-------|\n",
        "   | 1 | **D** | Pretrained + Attention | Features t·ªët nh·∫•t + attention focus spatial |\n",
        "   | 2 | **B** | Pretrained + No Attn | Features t·ªët, nh∆∞ng thi·∫øu spatial focus |\n",
        "   | 3 | **C** | Scratch + Attention | Attention gi√∫p, nh∆∞ng features y·∫øu |\n",
        "   | 4 | **A** | Scratch + No Attn | Baseline y·∫øu nh·∫•t |\n",
        "\n",
        "**Ph√¢n t√≠ch 2 tr·ª•c ch√≠nh:**\n",
        "\n",
        "- **Tr·ª•c 1 ‚Äî Pretrained vs Scratch:** Pretrained features **lu√¥n t·ªët h∆°n** v√¨ ResNet101 mang ki·∫øn th·ª©c t·ª´ ImageNet (1.2M ·∫£nh, 1000 classes). Scratch CNN ch·ªâ c√≥ d·ªØ li·ªáu VQA (~443K) v√† ki·∫øn tr√∫c ƒë∆°n gi·∫£n (5 conv blocks vs 101 layers).\n",
        "\n",
        "- **Tr·ª•c 2 ‚Äî Attention vs No Attention:** Attention **gi√∫p ƒë√°ng k·ªÉ** cho c√°c c√¢u h·ªèi c·∫ßn spatial reasoning (v·ªã tr√≠, ƒë·∫øm, m√†u s·∫Øc v·∫≠t c·ª• th·ªÉ). Tuy nhi√™n, attention ch·ªâ hi·ªáu qu·∫£ khi features ƒë·ªß t·ªët ‚Äî ƒë√¢y l√† l√Ω do D > C nh∆∞ng gap D-B c√≥ th·ªÉ kh√°c gap C-A.\n",
        "\n",
        "- **Tr·ª•c 3 ‚Äî Phase progression:** Fine-tuning (Phase 2) + Scheduled Sampling (Phase 3) **t√≠ch l≈©y c·∫£i thi·ªán** cho t·∫•t c·∫£ models, ch·ª©ng minh r·∫±ng training strategy quan tr·ªçng kh√¥ng k√©m ki·∫øn tr√∫c."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e9efbb05",
      "metadata": {
        "id": "e9efbb05"
      },
      "source": [
        "---\n",
        "## Step 4 ‚Äî Plot Training Curves (All Phases)\n",
        "\n",
        "So s√°nh train/val loss c·ªßa 4 models qua to√†n b·ªô 20 epochs (3 phases).\n",
        "\n",
        "Output: `checkpoints/training_curves.png`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6812c136",
      "metadata": {
        "id": "6812c136"
      },
      "outputs": [],
      "source": [
        "!python src/plot_curves.py --models A,B,C,D --output checkpoints/training_curves.png"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c403aedf",
      "metadata": {
        "id": "c403aedf"
      },
      "outputs": [],
      "source": [
        "# Hi·ªÉn th·ªã training curves\n",
        "from IPython.display import Image, display\n",
        "display(Image(filename='checkpoints/training_curves.png'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7f51eac7",
      "metadata": {
        "id": "7f51eac7"
      },
      "outputs": [],
      "source": [
        "# üíæ L∆∞u training curves l√™n Drive\n",
        "print(\"=== Syncing training curves to Drive ===\")\n",
        "sync_to_drive('checkpoints/training_curves.png', 'outputs', 'Training curves')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "534a4aa9",
      "metadata": {
        "id": "534a4aa9"
      },
      "source": [
        "---\n",
        "## Step 5 ‚Äî Evaluate t·ª´ng Model (Best Checkpoint)\n",
        "\n",
        "ƒê√°nh gi√° chi ti·∫øt t·ª´ng model s·ª≠ d·ª•ng **best checkpoint** (lowest val loss qua t·∫•t c·∫£ phases).\n",
        "\n",
        "Metrics:\n",
        "- **VQA Accuracy**: `min(matching_annotations / 3, 1.0)` ‚Äî official VQA metric\n",
        "- **Exact Match**: prediction == ground truth (strict)\n",
        "- **BLEU-1, BLEU-2, BLEU-3, BLEU-4**: n-gram overlap\n",
        "- **METEOR**: synonym-aware matching"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7152d9cb",
      "metadata": {
        "id": "7152d9cb"
      },
      "outputs": [],
      "source": [
        "# Evaluate Model A (best checkpoint)\n",
        "!python src/evaluate.py --model_type A --checkpoint checkpoints/model_a_best.pth"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "572915e0",
      "metadata": {
        "id": "572915e0"
      },
      "outputs": [],
      "source": [
        "# Evaluate Model B (best checkpoint)\n",
        "!python src/evaluate.py --model_type B --checkpoint checkpoints/model_b_best.pth"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7bc2abbb",
      "metadata": {
        "id": "7bc2abbb"
      },
      "outputs": [],
      "source": [
        "# Evaluate Model C (best checkpoint)\n",
        "!python src/evaluate.py --model_type C --checkpoint checkpoints/model_c_best.pth"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6ac4794b",
      "metadata": {
        "id": "6ac4794b"
      },
      "outputs": [],
      "source": [
        "# Evaluate Model D (best checkpoint)\n",
        "!python src/evaluate.py --model_type D --checkpoint checkpoints/model_d_best.pth"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1fe0b481",
      "metadata": {
        "id": "1fe0b481"
      },
      "source": [
        "### (Optional) Evaluate v·ªõi Beam Search\n",
        "\n",
        "Thay v√¨ greedy decode (ch·ªçn token x√°c su·∫•t cao nh·∫•t), beam search gi·ªØ top-k candidates t·∫°i m·ªói b∆∞·ªõc ƒë·ªÉ t√¨m sequence t·ªët h∆°n."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "55761ca1",
      "metadata": {
        "id": "55761ca1"
      },
      "outputs": [],
      "source": [
        "# (Optional) Evaluate v·ªõi beam search width=3\n",
        "# !python src/evaluate.py --model_type D --beam_width 3"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "76cde939",
      "metadata": {
        "id": "76cde939"
      },
      "source": [
        "---\n",
        "## Step 6 ‚Äî So s√°nh t·ªïng h·ª£p 4 Models\n",
        "\n",
        "### 3 b·∫£ng so s√°nh ƒë√£ ch·∫°y ·ªü Step 3:\n",
        "1. **Phase 1** (epoch 10): Baseline ‚Äî controlled experiment, ch·ªâ kh√°c ki·∫øn tr√∫c\n",
        "2. **Phase 2** (epoch 15): + Fine-tune/Continue ‚Äî ·∫£nh h∆∞·ªüng c·ªßa CNN fine-tuning\n",
        "3. **Phase 3** (epoch 20): + Scheduled Sampling ‚Äî ·∫£nh h∆∞·ªüng c·ªßa SS\n",
        "\n",
        "### Ph√¢n t√≠ch ch√≠nh:\n",
        "- **Scratch vs Pretrained** (A vs B, C vs D): Pretrained features c√≥ t·ªët h∆°n?\n",
        "- **No Attention vs Attention** (A vs C, B vs D): Attention c√≥ gi√∫p?\n",
        "- **Phase progression**: Fine-tuning v√† SS c·∫£i thi·ªán bao nhi√™u %?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "68ca3b63",
      "metadata": {
        "id": "68ca3b63"
      },
      "outputs": [],
      "source": [
        "# So s√°nh cu·ªëi c√πng ‚Äî best checkpoint c·ªßa m·ªói model\n",
        "# (D√πng epoch 20 ‚Äî sau t·∫•t c·∫£ phases)\n",
        "!python src/compare.py --models A,B,C,D --epoch 20"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6ec5e2cf",
      "metadata": {
        "id": "6ec5e2cf"
      },
      "source": [
        "#### Ph√¢n t√≠ch t·ªïng h·ª£p ‚Äî So s√°nh 4 Models qua 3 Phases\n",
        "\n",
        "**Progression qua 3 Phases:**\n",
        "\n",
        "M·ªói phase ƒë√≥ng g√≥p m·ªôt y·∫øu t·ªë kh√°c nhau v√†o performance:\n",
        "\n",
        "| Phase | K·ªπ thu·∫≠t √°p d·ª•ng | ·∫¢nh h∆∞·ªüng ch√≠nh |\n",
        "|-------|-----------------|-----------------|\n",
        "| Phase 1 (10 ep) | Teacher Forcing, frozen ResNet | Decoder + Q-Encoder h·ªôi t·ª•, h·ªçc c√°ch s·ª≠ d·ª•ng features |\n",
        "| Phase 2 (+5 ep) | Unfreeze CNN (B,D), lower LR | CNN features adapt cho VQA domain ‚Üí B,D c·∫£i thi·ªán nhi·ªÅu |\n",
        "| Phase 3 (+5 ep) | Scheduled Sampling | Gi·∫£m exposure bias ‚Üí c·∫£i thi·ªán inference quality |\n",
        "\n",
        "**K·∫øt lu·∫≠n ch√≠nh:**\n",
        "\n",
        "1. **Pretrained features quan tr·ªçng nh·∫•t:** Gap l·ªõn nh·∫•t gi·ªØa c√°c models ƒë·∫øn t·ª´ vi·ªác s·ª≠ d·ª•ng pretrained ResNet101 vs scratch CNN. Transfer learning t·ª´ ImageNet cung c·∫•p feature extraction ch·∫•t l∆∞·ª£ng cao m√† scratch CNN kh√¥ng th·ªÉ ƒë·∫°t ƒë∆∞·ª£c v·ªõi l∆∞·ª£ng d·ªØ li·ªáu h·∫°n ch·∫ø.\n",
        "\n",
        "2. **Attention c·∫£i thi·ªán ƒë√°ng k·ªÉ nh∆∞ng ph·ª• thu·ªôc feature quality:** Attention mechanism gi√∫p model focus v√†o v√πng ·∫£nh relevant, nh∆∞ng ch·ªâ th·ª±c s·ª± hi·ªáu qu·∫£ khi features ƒë·ªß t·ªët (D > C m·∫°nh h∆°n C > A).\n",
        "\n",
        "3. **Training strategy t√≠ch l≈©y:** M·ªói phase ƒë√≥ng g√≥p c·∫£i thi·ªán ri√™ng ‚Äî kh√¥ng c√≥ shortcut. Fine-tuning tr∆∞·ªõc khi Scheduled Sampling l√† th·ª© t·ª± ƒë√∫ng.\n",
        "\n",
        "4. **Generative VQA vs Discriminative VQA:** H·ªá th·ªëng sinh answer token-by-token kh√≥ h∆°n nhi·ªÅu so v·ªõi ch·ªçn 1 trong N ƒë√°p √°n c·ªë ƒë·ªãnh, nh∆∞ng linh ho·∫°t h∆°n ‚Äî c√≥ th·ªÉ sinh c√¢u tr·∫£ l·ªùi ch∆∞a th·∫•y trong training."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "69b55d5a",
      "metadata": {
        "id": "69b55d5a"
      },
      "source": [
        "---\n",
        "## Step 7 ‚Äî Single-Sample Inference\n",
        "\n",
        "Ch·∫°y inference tr√™n 1 sample c·ª• th·ªÉ ƒë·ªÉ xem model sinh c√¢u tr·∫£ l·ªùi nh∆∞ th·∫ø n√†o.\n",
        "\n",
        "Script `inference.py` m·∫∑c ƒë·ªãnh ch·∫°y model A tr√™n sample ƒë·∫ßu ti√™n. C√≥ th·ªÉ s·ª≠a tr·ª±c ti·∫øp trong code n·∫øu mu·ªën ƒë·ªïi model/sample."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "95e6c9ed",
      "metadata": {
        "id": "95e6c9ed"
      },
      "outputs": [],
      "source": [
        "!python src/inference.py"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0d396b98",
      "metadata": {
        "id": "0d396b98"
      },
      "source": [
        "---\n",
        "## Step 8 ‚Äî Attention Visualization (Model C, D)\n",
        "\n",
        "Tr·ª±c quan h√≥a c∆° ch·∫ø attention:\n",
        "- V·ªõi m·ªói token ƒë∆∞·ª£c sinh ra, hi·ªÉn th·ªã **heatmap** tr√™n ·∫£nh g·ªëc cho th·∫•y v√πng n√†o model ƒëang \"nh√¨n v√†o\"\n",
        "- Attention weights `alpha` c√≥ shape `(49,)` ‚Üí reshape th√†nh `7√ó7` ‚Üí upsample l√™n `224√ó224`\n",
        "\n",
        "Output: `checkpoints/attn_model_c.png`, `checkpoints/attn_model_d.png`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9934c6e8",
      "metadata": {
        "id": "9934c6e8"
      },
      "outputs": [],
      "source": [
        "# Attention visualization ‚Äî Model C\n",
        "!python src/visualize.py --model_type C --sample_idx 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e56294ff",
      "metadata": {
        "id": "e56294ff"
      },
      "outputs": [],
      "source": [
        "# Attention visualization ‚Äî Model D\n",
        "!python src/visualize.py --model_type D --sample_idx 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2bf774c0",
      "metadata": {
        "id": "2bf774c0"
      },
      "outputs": [],
      "source": [
        "# Hi·ªÉn th·ªã attention maps\n",
        "from IPython.display import Image, display\n",
        "import os\n",
        "\n",
        "for mt in ['c', 'd']:\n",
        "    path = f'checkpoints/attn_model_{mt}.png'\n",
        "    if os.path.exists(path):\n",
        "        print(f\"\\n--- Model {mt.upper()} Attention ---\")\n",
        "        display(Image(filename=path))\n",
        "    else:\n",
        "        print(f\"Not found: {path}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0f9e08b7",
      "metadata": {
        "id": "0f9e08b7"
      },
      "outputs": [],
      "source": [
        "# üíæ L∆∞u attention maps l√™n Drive\n",
        "print(\"=== Syncing attention maps to Drive ===\")\n",
        "sync_to_drive('checkpoints/attn_model_*.png', 'outputs', 'Attention maps')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "de16d614",
      "metadata": {
        "id": "de16d614"
      },
      "source": [
        "---\n",
        "## Step 9 ‚Äî Qualitative Analysis (V√≠ d·ª• D·ª± ƒëo√°n ƒê√∫ng & Sai)\n",
        "\n",
        "Hi·ªÉn th·ªã m·ªôt s·ªë v√≠ d·ª• c·ª• th·ªÉ: ·∫£nh + c√¢u h·ªèi + predicted answer vs ground truth.\n",
        "\n",
        "M·ª•c ƒë√≠ch:\n",
        "- Xem **model d·ª± ƒëo√°n ƒë√∫ng** trong tr∆∞·ªùng h·ª£p n√†o\n",
        "- Xem **model sai** ·ªü ƒë√¢u v√† t·∫°i sao\n",
        "- So s√°nh tr·ª±c quan 4 models tr√™n c√πng m·ªôt c√¢u h·ªèi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4f60c7dc",
      "metadata": {
        "id": "4f60c7dc"
      },
      "outputs": [],
      "source": [
        "import torch, json, os, sys, random\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "from torchvision import transforms\n",
        "import torch.nn.functional as F\n",
        "\n",
        "sys.path.append('src')\n",
        "from vocab import Vocabulary\n",
        "from inference import get_model, greedy_decode, greedy_decode_with_attention\n",
        "from models.vqa_models import hadamard_fusion\n",
        "\n",
        "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# Load vocab\n",
        "vocab_q = Vocabulary(); vocab_q.load('data/processed/vocab_questions.json')\n",
        "vocab_a = Vocabulary(); vocab_a.load('data/processed/vocab_answers.json')\n",
        "\n",
        "# Load val data\n",
        "VAL_IMAGE_DIR = 'data/raw/images/val2014'\n",
        "VAL_Q_JSON    = 'data/raw/vqa_json/v2_OpenEnded_mscoco_val2014_questions.json'\n",
        "VAL_A_JSON    = 'data/raw/vqa_json/v2_mscoco_val2014_annotations.json'\n",
        "\n",
        "with open(VAL_Q_JSON) as f:\n",
        "    val_questions = json.load(f)['questions']\n",
        "with open(VAL_A_JSON) as f:\n",
        "    val_annotations = json.load(f)['annotations']\n",
        "\n",
        "qid2ann = {ann['question_id']: ann for ann in val_annotations}\n",
        "\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "def denorm(t):\n",
        "    mean = torch.tensor([.485,.456,.406]).view(3,1,1)\n",
        "    std  = torch.tensor([.229,.224,.225]).view(3,1,1)\n",
        "    return (t*std+mean).clamp(0,1).permute(1,2,0).numpy()\n",
        "\n",
        "# Load all 4 models (best checkpoint)\n",
        "models_dict = {}\n",
        "for mt in ['A', 'B', 'C', 'D']:\n",
        "    ckpt = f'checkpoints/model_{mt.lower()}_best.pth'\n",
        "    if not os.path.exists(ckpt):\n",
        "        ckpt = f'checkpoints/model_{mt.lower()}_epoch20.pth'\n",
        "    if not os.path.exists(ckpt):\n",
        "        print(f\"  [SKIP] No checkpoint for Model {mt}\")\n",
        "        continue\n",
        "    m = get_model(mt, len(vocab_q), len(vocab_a))\n",
        "    m.load_state_dict(torch.load(ckpt, map_location='cpu'))\n",
        "    m.to(DEVICE).eval()\n",
        "    models_dict[mt] = m\n",
        "    print(f\"  Loaded Model {mt}: {ckpt}\")\n",
        "\n",
        "# Pick random samples\n",
        "random.seed(42)\n",
        "sample_indices = random.sample(range(len(val_questions)), min(6, len(val_questions)))\n",
        "\n",
        "fig, axes = plt.subplots(len(sample_indices), 1, figsize=(14, 5 * len(sample_indices)))\n",
        "if len(sample_indices) == 1:\n",
        "    axes = [axes]\n",
        "\n",
        "for row, idx in enumerate(sample_indices):\n",
        "    q_info = val_questions[idx]\n",
        "    q_text = q_info['question']\n",
        "    q_id   = q_info['question_id']\n",
        "    img_id = q_info['image_id']\n",
        "    gt_ans = qid2ann[q_id]['multiple_choice_answer']\n",
        "\n",
        "    img_path = os.path.join(VAL_IMAGE_DIR, f'COCO_val2014_{img_id:012d}.jpg')\n",
        "    if not os.path.exists(img_path):\n",
        "        continue\n",
        "\n",
        "    img = Image.open(img_path).convert('RGB')\n",
        "    img_t = transform(img)\n",
        "    q_t   = torch.tensor(vocab_q.numericalize(q_text), dtype=torch.long)\n",
        "\n",
        "    # Get predictions from all models\n",
        "    preds = {}\n",
        "    for mt, model in models_dict.items():\n",
        "        with torch.no_grad():\n",
        "            if mt in ('A', 'B'):\n",
        "                preds[mt] = greedy_decode(model, img_t, q_t, vocab_a, device=DEVICE)\n",
        "            else:\n",
        "                preds[mt] = greedy_decode_with_attention(model, img_t, q_t, vocab_a, device=DEVICE)\n",
        "\n",
        "    # Display\n",
        "    axes[row].imshow(denorm(img_t))\n",
        "    axes[row].axis('off')\n",
        "\n",
        "    pred_text = ' | '.join([f'{mt}: \"{p}\"' for mt, p in preds.items()])\n",
        "    match_markers = ' | '.join([\n",
        "        f'{mt}: {\"‚úì\" if p.strip().lower() == gt_ans.strip().lower() else \"‚úó\"}'\n",
        "        for mt, p in preds.items()\n",
        "    ])\n",
        "\n",
        "    axes[row].set_title(\n",
        "        f'Q: {q_text}\\nGT: \"{gt_ans}\" | {pred_text}\\n{match_markers}',\n",
        "        fontsize=9, loc='left', wrap=True\n",
        "    )\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('checkpoints/qualitative_analysis.png', dpi=150, bbox_inches='tight')\n",
        "plt.show()\n",
        "print(\"Saved: checkpoints/qualitative_analysis.png\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cb13c945",
      "metadata": {
        "id": "cb13c945"
      },
      "outputs": [],
      "source": [
        "# üíæ L∆∞u qualitative analysis l√™n Drive\n",
        "print(\"=== Syncing qualitative analysis to Drive ===\")\n",
        "sync_to_drive('checkpoints/qualitative_analysis.png', 'outputs', 'Qualitative analysis')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "519f3929",
      "metadata": {
        "id": "519f3929"
      },
      "source": [
        "#### Nh·∫≠n x√©t Qualitative Analysis\n",
        "\n",
        "T·ª´ c√°c v√≠ d·ª• tr√™n, c√≥ th·ªÉ quan s√°t:\n",
        "\n",
        "1. **C√¢u h·ªèi Yes/No:** T·∫•t c·∫£ models th∆∞·ªùng x·ª≠ l√Ω t·ªët ‚Äî c√¢u tr·∫£ l·ªùi ng·∫Øn (1 token), d·ªÖ sinh.\n",
        "\n",
        "2. **C√¢u h·ªèi ƒë·∫øm (How many?):** Models pretrained (B, D) th∆∞·ªùng ch√≠nh x√°c h∆°n v√¨ ResNet features t·ªët h∆°n cho object recognition. Attention (D) gi√∫p focus v√†o v√πng ch·ª©a objects c·∫ßn ƒë·∫øm.\n",
        "\n",
        "3. **C√¢u h·ªèi v·ªÅ thu·ªôc t√≠nh (What color? What kind?):** Y√™u c·∫ßu model hi·ªÉu fine-grained visual features. Scratch CNN (A, C) th∆∞·ªùng predict c√¢u tr·∫£ l·ªùi ph·ªï bi·∫øn nh·∫•t thay v√¨ c√¢u tr·∫£ l·ªùi ƒë√∫ng cho ·∫£nh c·ª• th·ªÉ.\n",
        "\n",
        "4. **C√¢u h·ªèi spatial (Where? What is on the left?):** Attention models (C, D) c√≥ l·ª£i th·∫ø r√µ r·ªát ‚Äî c√≥ th·ªÉ focus v√†o v√πng spatial c·ª• th·ªÉ trong ·∫£nh.\n",
        "\n",
        "5. **Failure cases ph·ªï bi·∫øn:**\n",
        "   - Predict c√¢u tr·∫£ l·ªùi ph·ªï bi·∫øn nh·∫•t (\"yes\", \"2\", \"white\") b·∫•t k·ªÉ ·∫£nh ‚Äî **language bias**.\n",
        "   - Sinh t·ª´ l·∫∑p ho·∫∑c v√¥ nghƒ©a ‚Äî **decoder degeneration** (th∆∞·ªùng x·∫£y ra ·ªü scratch models).\n",
        "   - C√¢u tr·∫£ l·ªùi g·∫ßn ƒë√∫ng nh∆∞ng kh√¥ng exactly match (v√≠ d·ª• \"dark blue\" vs \"blue\") ‚Äî metric qu√° strict."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0da7f6bc",
      "metadata": {
        "id": "0da7f6bc"
      },
      "source": [
        "---\n",
        "## Step 10 ‚Äî Error Analysis theo Lo·∫°i C√¢u h·ªèi\n",
        "\n",
        "Ph√¢n t√≠ch accuracy theo **lo·∫°i c√¢u h·ªèi** (question type) ƒë·ªÉ hi·ªÉu model m·∫°nh/y·∫øu ·ªü ƒë√¢u.\n",
        "\n",
        "VQA 2.0 annotation cung c·∫•p `answer_type` (3 lo·∫°i ch√≠nh):\n",
        "- **yes/no**: C√¢u h·ªèi ƒë√∫ng/sai\n",
        "- **number**: C√¢u h·ªèi ƒë·∫øm\n",
        "- **other**: C√¢u h·ªèi m·ªü (what, where, who, ...)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4a74ba83",
      "metadata": {
        "id": "4a74ba83"
      },
      "outputs": [],
      "source": [
        "import torch, json, os, sys\n",
        "import matplotlib.pyplot as plt\n",
        "from torch.utils.data import DataLoader\n",
        "import tqdm\n",
        "\n",
        "sys.path.append('src')\n",
        "from vocab import Vocabulary\n",
        "from dataset import VQADataset, vqa_collate_fn\n",
        "from inference import (get_model, batch_greedy_decode, batch_greedy_decode_with_attention)\n",
        "\n",
        "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "vocab_q = Vocabulary(); vocab_q.load('data/processed/vocab_questions.json')\n",
        "vocab_a = Vocabulary(); vocab_a.load('data/processed/vocab_answers.json')\n",
        "\n",
        "# Load annotations with answer_type\n",
        "VAL_A_JSON = 'data/raw/vqa_json/v2_mscoco_val2014_annotations.json'\n",
        "with open(VAL_A_JSON) as f:\n",
        "    raw_anns = json.load(f)['annotations']\n",
        "qid2type = {ann['question_id']: ann['answer_type'] for ann in raw_anns}\n",
        "qid2all  = {ann['question_id']: [a['answer'].lower().strip() for a in ann['answers']] for ann in raw_anns}\n",
        "\n",
        "# Load val dataset\n",
        "val_dataset = VQADataset(\n",
        "    image_dir='data/raw/images/val2014',\n",
        "    question_json_path='data/raw/vqa_json/v2_OpenEnded_mscoco_val2014_questions.json',\n",
        "    annotations_json_path=VAL_A_JSON,\n",
        "    vocab_q=vocab_q, vocab_a=vocab_a, split='val2014',\n",
        "    max_samples=5000  # Limit for speed; remove for full eval\n",
        ")\n",
        "question_ids = [q['question_id'] for q in val_dataset.questions]\n",
        "\n",
        "val_loader = DataLoader(val_dataset, batch_size=64, shuffle=False,\n",
        "                        collate_fn=vqa_collate_fn, num_workers=2)\n",
        "\n",
        "def decode_tensor(a_tensor, vocab_a):\n",
        "    special = {vocab_a.word2idx['<pad>'], vocab_a.word2idx['<start>'], vocab_a.word2idx['<end>']}\n",
        "    return ' '.join([vocab_a.idx2word[int(i)] for i in a_tensor if int(i) not in special])\n",
        "\n",
        "# Evaluate each model by answer_type\n",
        "results_by_type = {}\n",
        "\n",
        "for mt in ['A', 'B', 'C', 'D']:\n",
        "    ckpt = f'checkpoints/model_{mt.lower()}_best.pth'\n",
        "    if not os.path.exists(ckpt):\n",
        "        ckpt = f'checkpoints/model_{mt.lower()}_epoch20.pth'\n",
        "    if not os.path.exists(ckpt):\n",
        "        print(f\"  [SKIP] No checkpoint for Model {mt}\")\n",
        "        continue\n",
        "\n",
        "    model = get_model(mt, len(vocab_q), len(vocab_a))\n",
        "    model.load_state_dict(torch.load(ckpt, map_location='cpu'))\n",
        "    model.to(DEVICE).eval()\n",
        "\n",
        "    decode_fn = batch_greedy_decode_with_attention if mt in ('C','D') else batch_greedy_decode\n",
        "    all_preds = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for imgs, qs, ans in tqdm.tqdm(val_loader, desc=f'Model {mt}', leave=False):\n",
        "            preds = decode_fn(model, imgs, qs, vocab_a, device=DEVICE)\n",
        "            all_preds.extend(preds)\n",
        "\n",
        "    # Compute VQA accuracy per answer_type\n",
        "    type_correct = {'yes/no': 0, 'number': 0, 'other': 0}\n",
        "    type_total   = {'yes/no': 0, 'number': 0, 'other': 0}\n",
        "\n",
        "    for idx, pred_str in enumerate(all_preds):\n",
        "        qid  = question_ids[idx]\n",
        "        atype = qid2type.get(qid, 'other')\n",
        "        pred_clean = pred_str.strip().lower()\n",
        "        all_answers = qid2all.get(qid, [])\n",
        "        match_count = sum(1 for a in all_answers if a == pred_clean)\n",
        "        vqa_acc = min(match_count / 3.0, 1.0)\n",
        "\n",
        "        type_correct[atype] = type_correct.get(atype, 0) + vqa_acc\n",
        "        type_total[atype]   = type_total.get(atype, 0) + 1\n",
        "\n",
        "    results_by_type[mt] = {\n",
        "        t: (type_correct[t] / type_total[t] * 100) if type_total[t] > 0 else 0\n",
        "        for t in ['yes/no', 'number', 'other']\n",
        "    }\n",
        "    print(f\"  Model {mt}: yes/no={results_by_type[mt]['yes/no']:.1f}%  \"\n",
        "          f\"number={results_by_type[mt]['number']:.1f}%  \"\n",
        "          f\"other={results_by_type[mt]['other']:.1f}%\")\n",
        "\n",
        "# Plot grouped bar chart\n",
        "if results_by_type:\n",
        "    fig, ax = plt.subplots(figsize=(10, 5))\n",
        "    q_types = ['yes/no', 'number', 'other']\n",
        "    x       = range(len(q_types))\n",
        "    width   = 0.18\n",
        "    colors  = {'A': '#1f77b4', 'B': '#ff7f0e', 'C': '#2ca02c', 'D': '#d62728'}\n",
        "\n",
        "    for i, (mt, res) in enumerate(sorted(results_by_type.items())):\n",
        "        vals = [res[t] for t in q_types]\n",
        "        bars = ax.bar([xi + i * width for xi in x], vals, width,\n",
        "                      label=f'Model {mt}', color=colors.get(mt, None))\n",
        "        for bar, v in zip(bars, vals):\n",
        "            ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.5,\n",
        "                    f'{v:.1f}', ha='center', va='bottom', fontsize=7)\n",
        "\n",
        "    ax.set_xlabel('Answer Type')\n",
        "    ax.set_ylabel('VQA Accuracy (%)')\n",
        "    ax.set_title('VQA Accuracy by Answer Type ‚Äî 4 Models')\n",
        "    ax.set_xticks([xi + width * 1.5 for xi in x])\n",
        "    ax.set_xticklabels(q_types)\n",
        "    ax.legend()\n",
        "    ax.grid(axis='y', alpha=0.3)\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('checkpoints/error_analysis_by_type.png', dpi=150, bbox_inches='tight')\n",
        "    plt.show()\n",
        "    print(\"Saved: checkpoints/error_analysis_by_type.png\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e27de8c4",
      "metadata": {
        "id": "e27de8c4"
      },
      "outputs": [],
      "source": [
        "# üíæ L∆∞u error analysis + attention maps l√™n Drive\n",
        "print(\"=== Syncing analysis outputs to Drive ===\")\n",
        "sync_to_drive('checkpoints/error_analysis_by_type.png', 'outputs', 'Error analysis')\n",
        "sync_to_drive('checkpoints/attn_model_*.png', 'outputs', 'Attention maps')\n",
        "print(\"\\n‚úì T·∫•t c·∫£ outputs ƒë√£ l∆∞u an to√†n tr√™n Drive!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ba54caea",
      "metadata": {
        "id": "ba54caea"
      },
      "source": [
        "#### Nh·∫≠n x√©t Error Analysis\n",
        "\n",
        "**D·ª± ki·∫øn xu h∆∞·ªõng theo lo·∫°i c√¢u h·ªèi:**\n",
        "\n",
        "| Answer Type | ƒê·∫∑c ƒëi·ªÉm | Model n√†o t·ªët nh·∫•t? | L√Ω do |\n",
        "|-------------|----------|---------------------|-------|\n",
        "| **yes/no** | Binary, chi·∫øm ~38% VQA | T·∫•t c·∫£ t∆∞∆°ng ƒë·ªëi t·ªët | Ch·ªâ c·∫ßn quy·∫øt ƒë·ªãnh 1 trong 2 ‚Üí decoder d·ªÖ sinh \"yes\"/\"no\" |\n",
        "| **number** | ƒê·∫øm (0-10+), chi·∫øm ~12% | D > B >> C > A | C·∫ßn nh·∫≠n di·ªán + ƒë·∫øm objects ‚Üí pretrained features + attention gi√∫p nhi·ªÅu |\n",
        "| **other** | M·ªü, ƒëa d·∫°ng, chi·∫øm ~50% | D > B > C > A | Y√™u c·∫ßu hi·ªÉu s√¢u ·∫£nh + c√¢u h·ªèi ‚Üí kh√≥ nh·∫•t cho generative model |\n",
        "\n",
        "**Insights:**\n",
        "\n",
        "1. **Yes/No gap nh·ªè:** C√¢u tr·∫£ l·ªùi ch·ªâ 1 token, t·∫•t c·∫£ models ƒë·ªÅu x·ª≠ l√Ω t∆∞∆°ng ƒë·ªëi t·ªët. S·ª± kh√°c bi·ªát ch·ªß y·∫øu t·ª´ visual understanding, kh√¥ng ph·∫£i generation quality.\n",
        "\n",
        "2. **Number gap l·ªõn ·ªü attention:** ƒê·∫øm objects y√™u c·∫ßu focus v√†o t·ª´ng object ‚Üí attention mechanism gi√∫p ƒë√°ng k·ªÉ. Model A (no attn, scratch) g·∫ßn nh∆∞ ƒëo√°n random v√¨ kh√¥ng th·ªÉ focus v√†o v√πng c·∫ßn ƒë·∫øm.\n",
        "\n",
        "3. **Other type kh√≥ nh·∫•t:** C√¢u tr·∫£ l·ªùi d√†i, ƒëa d·∫°ng ‚Üí generative decoder c·∫ßn capacity cao. Pretrained features gi√∫p hi·ªÉu ·∫£nh t·ªët h∆°n, attention gi√∫p focus v√†o chi ti·∫øt relevant.\n",
        "\n",
        "4. **Language bias r√µ nh·∫•t ·ªü \"other\":** Model y·∫øu c√≥ xu h∆∞·ªõng sinh c√¢u tr·∫£ l·ªùi ph·ªï bi·∫øn nh·∫•t (mode collapse) b·∫•t k·ªÉ ·∫£nh, ƒë·∫∑c bi·ªát v·ªõi c√¢u h·ªèi \"what\" ‚Üí lu√¥n tr·∫£ l·ªùi \"white\", \"yes\", \"2\"...\n",
        "\n",
        "> **K·∫øt lu·∫≠n:** Error analysis x√°c nh·∫≠n r·∫±ng **pretrained features + attention** l√† t·ªï h·ª£p m·∫°nh nh·∫•t cho m·ªçi lo·∫°i c√¢u h·ªèi, v·ªõi l·ª£i th·∫ø ƒë·∫∑c bi·ªát r√µ ·ªü c√¢u h·ªèi **number** v√† **other**."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6554207b",
      "metadata": {
        "id": "6554207b"
      },
      "source": [
        "---\n",
        "## Summary\n",
        "\n",
        "### Pipeline Steps\n",
        "\n",
        "| Step | Script / Section | Output |\n",
        "|------|-----------------|--------|\n",
        "| Build Vocab | `src/scripts/1_build_vocab.py` | `data/processed/vocab_*.json` |\n",
        "| L·ª±a ch·ªçn Metrics | Markdown analysis | Gi·∫£i th√≠ch 7 metrics (VQA Acc, EM, BLEU-1/2/3/4, METEOR) |\n",
        "| Phase 1 ‚Äî Baseline (10ep) | `src/train.py --model X` | Checkpoints + Compare + Ph√¢n t√≠ch |\n",
        "| Phase 2 ‚Äî Fine-tune (5ep) | `src/train.py --resume ...` | Checkpoints + Compare + Ph√¢n t√≠ch |\n",
        "| Phase 3 ‚Äî SS (5ep) | `src/train.py --scheduled_sampling` | Checkpoints + Compare + Ph√¢n t√≠ch |\n",
        "| Plot Curves | `src/plot_curves.py` | `checkpoints/training_curves.png` |\n",
        "| Evaluate | `src/evaluate.py --model_type X` | Chi ti·∫øt metrics t·ª´ng model |\n",
        "| Compare | `src/compare.py` | B·∫£ng so s√°nh side-by-side |\n",
        "| Inference | `src/inference.py` | V√≠ d·ª• question + predicted answer |\n",
        "| Attention Viz | `src/visualize.py --model_type C/D` | `checkpoints/attn_model_*.png` |\n",
        "| Qualitative Analysis | Inline code | ·∫¢nh + Q + Predicted vs GT (ƒë√∫ng/sai) |\n",
        "| Error Analysis | Inline code | VQA Accuracy theo answer_type (yes/no, number, other) |\n",
        "\n",
        "### Training Strategy ‚Äî 3 Phases, t·∫•t c·∫£ 4 models\n",
        "\n",
        "```\n",
        "Phase 1: Baseline (10 epochs)          Phase 2: Fine-tune (5 epochs)          Phase 3: Sched. Sampling (5 epochs)\n",
        "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê        ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê        ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
        "‚îÇ ‚Ä¢ Teacher Forcing           ‚îÇ        ‚îÇ ‚Ä¢ B,D: Unfreeze ResNet L3+4 ‚îÇ        ‚îÇ ‚Ä¢ Œµ decays: GT ‚Üí model pred ‚îÇ\n",
        "‚îÇ ‚Ä¢ ResNet FROZEN (B,D)       ‚îÇ   ‚Üí    ‚îÇ ‚Ä¢ A,C: Continue training    ‚îÇ   ‚Üí    ‚îÇ ‚Ä¢ Reduce exposure bias      ‚îÇ\n",
        "‚îÇ ‚Ä¢ All 4 models              ‚îÇ        ‚îÇ ‚Ä¢ All 4 models, LR=5e-4    ‚îÇ        ‚îÇ ‚Ä¢ All 4 models, LR=2e-4    ‚îÇ\n",
        "‚îÇ ‚Ä¢ Evaluate + Compare ‚úì      ‚îÇ        ‚îÇ ‚Ä¢ Evaluate + Compare ‚úì      ‚îÇ        ‚îÇ ‚Ä¢ Evaluate + Compare ‚úì      ‚îÇ\n",
        "‚îÇ ‚Ä¢ Ph√¢n t√≠ch k·∫øt qu·∫£ ‚úì       ‚îÇ        ‚îÇ ‚Ä¢ Ph√¢n t√≠ch k·∫øt qu·∫£ ‚úì       ‚îÇ        ‚îÇ ‚Ä¢ Ph√¢n t√≠ch k·∫øt qu·∫£ ‚úì       ‚îÇ\n",
        "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò        ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò        ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
        "         ‚Üì                                       ‚Üì                                       ‚Üì\n",
        "   B·∫£ng so s√°nh #1                         B·∫£ng so s√°nh #2                         B·∫£ng so s√°nh #3\n",
        " (controlled experiment)            (+ fine-tuning effect)                  (+ SS effect, final result)\n",
        "```\n",
        "\n",
        "### So s√°nh C√¥ng b·∫±ng\n",
        "\n",
        "T·∫•t c·∫£ 4 models nh·∫≠n **c√πng 20 epochs t·ªïng**, c√πng k·ªπ thu·∫≠t training ·ªü m·ªói phase:\n",
        "\n",
        "| Model | Phase 1 | Phase 2 | Phase 3 | Total |\n",
        "|-------|---------|---------|---------|-------|\n",
        "| A | TF, scratch CNN | Continue, lr=5e-4 | +SS | 20 ep |\n",
        "| B | TF, frozen ResNet | Unfreeze CNN, lr=5e-4 | +SS, keep unfreeze | 20 ep |\n",
        "| C | TF, scratch CNN+attn | Continue, lr=5e-4 | +SS | 20 ep |\n",
        "| D | TF, frozen ResNet+attn | Unfreeze CNN, lr=5e-4 | +SS, keep unfreeze | 20 ep |\n",
        "\n",
        "### Ki·∫øn tr√∫c\n",
        "\n",
        "```\n",
        "Image ‚îÄ‚îÄ> CNN Encoder ‚îÄ‚îÄ> img_feature ‚îÄ‚îÄ‚îê\n",
        "                                        ‚îú‚îÄ‚îÄ Hadamard Fusion ‚îÄ‚îÄ> h_0 ‚îÄ‚îÄ> LSTM Decoder ‚îÄ‚îÄ> Answer tokens\n",
        "Question ‚îÄ‚îÄ> LSTM Encoder ‚îÄ‚îÄ> q_feature ‚îÄ‚îò         ‚Üë\n",
        "                                          (Model C,D: Bahdanau Attention\n",
        "                                           attends over 49 spatial regions)\n",
        "```\n",
        "\n",
        "### ƒê√°nh gi√° & So s√°nh\n",
        "\n",
        "- **Metrics:** VQA Accuracy (ch√≠nh), Exact Match, BLEU-1/2/3/4, METEOR\n",
        "- **3 b·∫£ng compare** (Phase 1/2/3) cho th·∫•y progression t·ª´ baseline ‚Üí fine-tune ‚Üí scheduled sampling\n",
        "- **Qualitative analysis:** V√≠ d·ª• tr·ª±c quan ·∫£nh + c√¢u h·ªèi + d·ª± ƒëo√°n vs GT\n",
        "- **Error analysis:** Breakdown accuracy theo answer type (yes/no, number, other)\n",
        "- **Attention visualization:** Heatmap cho th·∫•y v√πng ·∫£nh model C/D focus\n",
        "\n",
        "### K·∫øt lu·∫≠n ch√≠nh\n",
        "\n",
        "1. **Pretrained > Scratch**: Transfer learning t·ª´ ImageNet lu√¥n gi√∫p, gap l·ªõn nh·∫•t\n",
        "2. **Attention > No Attention**: Spatial focus c·∫£i thi·ªán ƒë√°ng k·ªÉ, ƒë·∫∑c bi·ªát cho counting & spatial questions\n",
        "3. **Training strategy matters**: Fine-tune + Scheduled Sampling t√≠ch l≈©y c·∫£i thi·ªán cho t·∫•t c·∫£ models\n",
        "4. **Ranking: D > B > C > A**: Pretrained + Attention l√† t·ªï h·ª£p m·∫°nh nh·∫•t\n",
        "\n",
        "_(Xem output 3 b·∫£ng so s√°nh ·ªü Step 3, Step 6, v√† ph√¢n t√≠ch chi ti·∫øt sau m·ªói b·∫£ng)_"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "G4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
