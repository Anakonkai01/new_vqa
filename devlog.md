# Dev Log â€” VQA Project

## Ngá»¯ cáº£nh cho chat má»›i

**Repo:** https://github.com/Anakonkai01/new_vqa  
**Branch hiá»‡n táº¡i:** `experiment/model-a`  
**Má»¥c tiÃªu:** XÃ¢y dá»±ng 4 kiáº¿n trÃºc VQA (CNN + LSTM-Decoder), xem chi tiáº¿t trong [VQA_PROJECT_PLAN.md](VQA_PROJECT_PLAN.md)

---

## MÃ´i trÆ°á»ng

- **OS:** Linux
- **Python:** 3.9 (conda env `d2l`)
- **GPU:** NVIDIA GeForce MX330 â€” **KHÃ”NG tÆ°Æ¡ng thÃ­ch PyTorch CUDA má»›i** (cuda capability 6.1, PyTorch yÃªu cáº§u 7.0+)
- **â†’ Pháº£i train trÃªn CPU:** `DEVICE = torch.device('cpu')`
- **Cháº¡y script:** `python src/train.py` (tá»« thÆ° má»¥c gá»‘c `/home/anakonkai/Work/Projects/vqa_new`)

---

## Cáº¥u trÃºc thÆ° má»¥c hiá»‡n táº¡i

```
src/
â”œâ”€â”€ models/
â”‚   â”œâ”€â”€ encoder_cnn.py          âœ… SimpleCNN â€” output (batch, 1024)
â”‚   â”œâ”€â”€ encoder_questions.py    âœ… QuestionEncoder â€” output (batch, 1024)
â”‚   â”œâ”€â”€ decoder_lstm.py         âœ… LSTMDecoder â€” teacher forcing mode
â”‚   â””â”€â”€ vqa_models.py           âœ… VQAModelA â€” wrapper gá»™p 3 thÃ nh pháº§n
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ 1_build_vocab.py        âœ… build vocab_questions.json + vocab_answers.json
â”‚   â””â”€â”€ 2_extract_features.py   âœ… extract ResNet101 features â†’ h5 (chá»‰ dÃ¹ng cho Model B, D)
â”œâ”€â”€ dataset.py                  âœ… VQADatasetA â€” load raw image, answer dáº¡ng sequence
â”œâ”€â”€ vocab.py                    âœ… Vocabulary class vá»›i <pad>=0, <start>=1, <end>=2, <unk>=3
â””â”€â”€ train.py                    ğŸ”§ Gáº§n xong â€” xem bug bÃªn dÆ°á»›i

create_dummy_data.py            âœ… Táº¡o dummy data Ä‘á»ƒ test pipeline (100 samples)
VQA_PROJECT_PLAN.md             âœ… Full roadmap 4 models
```

---

## Kiáº¿n trÃºc Model A (Ä‘Ã£ implement)

```
Input: áº£nh (batch, 3, 224, 224) + cÃ¢u há»i (batch, max_q_len)

SimpleCNN:
  5x conv_block (Convâ†’BNâ†’ReLUâ†’MaxPool)
  3â†’64â†’128â†’256â†’512â†’1024 channels
  AdaptiveAvgPool2d(1) â†’ flatten â†’ Linear(1024, hidden=1024)
  Output: (batch, 1024)

QuestionEncoder:
  Embedding(vocab_q_size, 512) + LSTM(512â†’1024, layers=2)
  Output: hidden[-1] â†’ (batch, 1024)

Fusion: img_feature * q_feature (Hadamard) â†’ (batch, 1024)

LSTMDecoder (Teacher Forcing):
  h_0 = fusion.unsqueeze(0).repeat(2, 1, 1)  # (2, batch, 1024)
  c_0 = zeros_like(h_0)
  Input: answer[:, :-1] = [<start>, w1, w2]
  Target: answer[:, 1:]  = [w1, w2, <end>]
  Output: logits (batch, seq_len, vocab_a_size)

Loss: CrossEntropyLoss(ignore_index=0)
      logits.view(-1, vocab_size) vs decoder_target.contiguous().view(-1)

Optimizer: Adam lr=1e-3
Gradient clipping: max_norm=5.0
```

---

## Bug cáº§n fix ngay khi má»Ÿ chat má»›i

### Bug 1 â€” DEVICE sai (QUAN TRá»ŒNG NHáº¤T)
```python
# train.py dÃ²ng ~40
# Hiá»‡n táº¡i: âŒ
DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

# Sá»­a thÃ nh: âœ…
DEVICE = torch.device('cpu')
```
**LÃ½ do:** GPU MX330 detect Ä‘Æ°á»£c nhÆ°ng khÃ´ng cháº¡y Ä‘Æ°á»£c â†’ CUDA out of memory crash.

### Bug 2 â€” vocab_a load sai path
```python
# train.py dÃ²ng ~61
# Hiá»‡n táº¡i: âŒ (load vocab_q cho cáº£ vocab_a)
vocab_a.load(VOCAB_Q_PATH)

# Sá»­a thÃ nh: âœ…
vocab_a.load(VOCAB_A_PATH)
```

### Bug 3 â€” encoder_questions.py tÃªn file cÃ³ 's' thá»«a
```python
# vqa_models.py import:
from models.encoder_questions import QuestionEncoder  # 's' á»Ÿ cuá»‘i
# File thá»±c táº¿ tÃªn lÃ : encoder_questions.py â€” OK, khá»›p rá»“i
```

---

## Viá»‡c cáº§n lÃ m tiáº¿p theo (theo thá»© tá»±)

### Ngay láº­p tá»©c
1. Fix Bug 1 + Bug 2 trong `train.py`
2. Cháº¡y `python create_dummy_data.py` Ä‘á»ƒ táº¡o dummy data
3. Cháº¡y `python src/scripts/1_build_vocab.py` Ä‘á»ƒ build vocab (náº¿u chÆ°a cÃ³ `data/processed/vocab_*.json`)
4. Cháº¡y `python src/train.py` â†’ verify pipeline cháº¡y Ä‘Æ°á»£c trÃªn dummy data

### Sau khi train cháº¡y Ä‘Æ°á»£c
5. Viáº¿t `src/evaluate.py` â€” tÃ­nh BLEU, VQA Accuracy
6. Viáº¿t `src/inference.py` â€” greedy decode Ä‘á»ƒ sinh answer tá»« áº£nh + cÃ¢u há»i
7. Implement Model B (Pretrained ResNet, No Attention) â€” thÃªm class vÃ o `vqa_models.py`
8. Implement Model C (Scratch CNN + Attention)
9. Implement Model D (Pretrained + Attention)
10. Viáº¿t `src/compare.py` â€” so sÃ¡nh 4 model

---

## Dá»¯ liá»‡u

**Dummy data** (Ä‘á»ƒ test pipeline):
```bash
python create_dummy_data.py
# Táº¡o: data/raw/images/train2014/ (100 áº£nh 224x224 random)
#       data/raw/vqa_json/v2_OpenEnded_mscoco_train2014_questions.json
#       data/raw/vqa_json/v2_mscoco_train2014_annotations.json
#       data/processed/train_features.h5
```

**Real data** (cáº§n download, ~13GB):
```bash
wget http://images.cocodataset.org/zips/train2014.zip
unzip train2014.zip -d data/raw/images/
```

---

## Nhá»¯ng Ä‘iá»u Ä‘Ã£ há»c (Ä‘á»ƒ giáº£i thÃ­ch láº¡i náº¿u cáº§n)

- **padding=1 + kernel=3:** Giá»¯ nguyÃªn spatial size sau Conv
- **AdaptiveAvgPool2d(1):** Squeeze spatial 7Ã—7 â†’ 1Ã—1 Ä‘á»ƒ flatten thÃ nh vector
- **Teacher Forcing:** DÃ¹ng ground truth token lÃ m input bÆ°á»›c tiáº¿p thay vÃ¬ predict cá»§a bÆ°á»›c trÆ°á»›c
- **`target[:, :-1]` vs `target[:, 1:]`:** Shift 1 bÆ°á»›c â€” input vÃ  label offset nhau 1 token
- **`contiguous().view(-1)`:** Cáº§n thiáº¿t sau slice Ä‘á»ƒ reshape an toÃ n
- **`c_0 = zeros`:** Cell state khá»Ÿi táº¡o tráº¯ng, chá»‰ h_0 mang context áº£nh + cÃ¢u há»i
- **2 vocab riÃªng (vocab_q, vocab_a):** vocab_a nhá» hÆ¡n, chá»‰ chá»©a tá»« trong answers, decoder hiá»‡u quáº£ hÆ¡n
- **`ignore_index=0`:** KhÃ´ng tÃ­nh loss trÃªn `<pad>` token
- **gradient clipping `max_norm=5.0`:** LSTM hay bá»‹ exploding gradient

---

## Ghi chÃº ká»¹ thuáº­t

- File `encoder_questions.py` (cÃ³ chá»¯ 's') â€” Ä‘áº·t tÃªn hÆ¡i khÃ¡c convention nhÆ°ng váº«n cháº¡y Ä‘Æ°á»£c
- `VQAModelA` trong `vqa_models.py` â€” tÃªn class cÃ³ chá»¯ 'A' Ä‘á»ƒ phÃ¢n biá»‡t vá»›i Model B, C, D sau nÃ y
- Vocab Ä‘Ã£ cÃ³ `<start>=1`, `<end>=2` â†’ `numericalize()` tá»± thÃªm vÃ o cáº£ question láº«n answer
- CNN scratch chá»‰ cÃ³ 2GB VRAM trÃªn MX330 â†’ **báº¯t buá»™c dÃ¹ng CPU**
