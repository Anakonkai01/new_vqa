{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ceb359b8",
   "metadata": {},
   "source": [
    "# VQA End-to-End Pipeline\n",
    "\n",
    "This notebook covers the full pipeline for the Visual Question Answering (VQA) project:\n",
    "\n",
    "| Step | Description |\n",
    "|------|-------------|\n",
    "| 0 | Environment & GPU check |\n",
    "| 1 | Build vocabulary from training data |\n",
    "| 2 | Explore the dataset |\n",
    "| 3 | Train a model (A / B / C / D) |\n",
    "| 4 | Plot training curves |\n",
    "| 5 | Evaluate a single model (all metrics) |\n",
    "| 6 | Compare all 4 models side-by-side |\n",
    "| 7 | Single-sample inference |\n",
    "| 8 | Attention visualization (Model C / D) |\n",
    "\n",
    "**Run from repository root:** `vqa_new/`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "196e18ca",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 0 — Environment & GPU Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57ffaf14",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys, json, warnings\n",
    "import torch\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# ── Ensure the notebook runs from the project root ──────────────────\n",
    "ROOT = os.path.abspath('')          # notebook cwd\n",
    "if ROOT.endswith('src'):            # handle case where notebook is ran from src/\n",
    "    ROOT = os.path.dirname(ROOT)\n",
    "os.chdir(ROOT)\n",
    "sys.path.insert(0, os.path.join(ROOT, 'src'))\n",
    "\n",
    "print(f\"Working directory : {os.getcwd()}\")\n",
    "print(f\"Python            : {sys.version.split()[0]}\")\n",
    "print(f\"PyTorch           : {torch.__version__}\")\n",
    "print(f\"CUDA available    : {torch.cuda.is_available()}\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    for i in range(torch.cuda.device_count()):\n",
    "        props = torch.cuda.get_device_properties(i)\n",
    "        mem_gb = props.total_memory / 1e9\n",
    "        print(f\"GPU {i}             : {props.name}  ({mem_gb:.1f} GB)\")\n",
    "\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"\\nUsing device      : {DEVICE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "995d1770",
   "metadata": {},
   "source": [
    "### Global Path & Hyperparameter Configuration\n",
    "Edit this cell to change model type, epochs, batch size, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12bc3372",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Paths ───────────────────────────────────────────────────────────\n",
    "TRAIN_IMAGE_DIR       = 'data/raw/images/train2014'\n",
    "VAL_IMAGE_DIR         = 'data/raw/images/val2014'\n",
    "TRAIN_QUESTION_JSON   = 'data/raw/vqa_json/v2_OpenEnded_mscoco_train2014_questions.json'\n",
    "VAL_QUESTION_JSON     = 'data/raw/vqa_json/v2_OpenEnded_mscoco_val2014_questions.json'\n",
    "TRAIN_ANNOTATION_JSON = 'data/raw/vqa_json/v2_mscoco_train2014_annotations.json'\n",
    "VAL_ANNOTATION_JSON   = 'data/raw/vqa_json/v2_mscoco_val2014_annotations.json'\n",
    "VOCAB_Q_PATH          = 'data/processed/vocab_questions.json'\n",
    "VOCAB_A_PATH          = 'data/processed/vocab_answers.json'\n",
    "CHECKPOINT_DIR        = 'checkpoints'\n",
    "\n",
    "# ── Training hyperparameters ─────────────────────────────────────────\n",
    "MODEL_TYPE    = 'A'     # 'A' | 'B' | 'C' | 'D'\n",
    "EPOCHS        = 10\n",
    "LR            = 1e-3\n",
    "BATCH_SIZE    = 128\n",
    "RESUME        = None    # set e.g. 'checkpoints/model_a_resume.pth' to continue training\n",
    "\n",
    "# ── Scheduled Sampling (reduces exposure bias) ───────────────────────\n",
    "# epsilon = SS_K / (SS_K + exp(epoch / SS_K))  — starts ~1, decays toward 0\n",
    "SCHEDULED_SAMPLING = False\n",
    "SS_K               = 5.0\n",
    "\n",
    "# ── CNN Fine-tuning (models B and D only) ────────────────────────────\n",
    "# Unfreezes ResNet layer3 + layer4 with a smaller LR to fine-tune high-level\n",
    "# features for VQA without causing catastrophic forgetting of ImageNet knowledge.\n",
    "# Recommended: train ~5 epochs frozen first, then resume with FINETUNE_CNN=True.\n",
    "FINETUNE_CNN    = False   # Set True to unfreeze layer3+layer4 of ResNet backbone\n",
    "CNN_LR_FACTOR   = 0.1     # backbone LR = LR × CNN_LR_FACTOR  (e.g. 1e-4 with default 1e-3)\n",
    "\n",
    "# ── Dataset caps (None = use full dataset) ────────────────────────────\n",
    "MAX_TRAIN_SAMPLES = None\n",
    "MAX_VAL_SAMPLES   = None\n",
    "\n",
    "# ── Evaluation / comparison settings ─────────────────────────────────\n",
    "EVAL_EPOCH        = 10\n",
    "COMPARE_EPOCH     = 10\n",
    "COMPARE_MODELS    = ['A', 'B', 'C', 'D']\n",
    "NUM_EVAL_SAMPLES  = None\n",
    "\n",
    "# ── Decoding strategy ─────────────────────────────────────────────────\n",
    "BEAM_WIDTH = 1    # 1 = greedy (fast); 3–5 = beam search (better quality, slower)\n",
    "\n",
    "print(\"Configuration loaded.\")\n",
    "print(f\"  Model             : {MODEL_TYPE}\")\n",
    "print(f\"  Epochs            : {EPOCHS}  |  LR: {LR}\")\n",
    "print(f\"  Batch size        : {BATCH_SIZE}\")\n",
    "print(f\"  Scheduled Sampling: {SCHEDULED_SAMPLING}  (k={SS_K})\")\n",
    "print(f\"  CNN Fine-tuning   : {FINETUNE_CNN}  (backbone LR factor={CNN_LR_FACTOR})\")\n",
    "print(f\"  Beam Width        : {BEAM_WIDTH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d2f9ab3",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 1 — Build Vocabulary\n",
    "\n",
    "Builds:\n",
    "- `data/processed/vocab_questions.json` — question vocabulary (words appearing ≥ 3 times)\n",
    "- `data/processed/vocab_answers.json` — answer vocabulary (most common answers, threshold ≥ 5)\n",
    "\n",
    "> **Skip this step if the vocab files already exist.** The cell checks for existing files automatically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c111b52",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from vocab import Vocabulary\n",
    "\n",
    "os.makedirs('data/processed', exist_ok=True)\n",
    "\n",
    "def build_vocab_if_missing():\n",
    "    q_exists = os.path.exists(VOCAB_Q_PATH)\n",
    "    a_exists = os.path.exists(VOCAB_A_PATH)\n",
    "\n",
    "    if q_exists and a_exists:\n",
    "        print(\"Vocab files already exist — skipping build.\")\n",
    "        print(f\"  {VOCAB_Q_PATH}\")\n",
    "        print(f\"  {VOCAB_A_PATH}\")\n",
    "        return\n",
    "\n",
    "    # ── Question vocabulary ──────────────────────────────────────────\n",
    "    if not q_exists:\n",
    "        print(f\"Building question vocab from: {TRAIN_QUESTION_JSON}\")\n",
    "        with open(TRAIN_QUESTION_JSON, 'r') as f:\n",
    "            questions = json.load(f)['questions']\n",
    "        q_texts = [q['question'] for q in questions]\n",
    "        q_vocab = Vocabulary()\n",
    "        q_vocab.build(q_texts, threshold=3)\n",
    "        q_vocab.save(VOCAB_Q_PATH)\n",
    "        print(f\"  Saved  : {VOCAB_Q_PATH}  ({len(q_vocab)} tokens)\")\n",
    "\n",
    "    # ── Answer vocabulary ────────────────────────────────────────────\n",
    "    if not a_exists:\n",
    "        print(f\"Building answer vocab from: {TRAIN_ANNOTATION_JSON}\")\n",
    "        with open(TRAIN_ANNOTATION_JSON, 'r') as f:\n",
    "            annotations = json.load(f)['annotations']\n",
    "        a_texts = [ann['multiple_choice_answer'] for ann in annotations]\n",
    "        a_vocab = Vocabulary()\n",
    "        a_vocab.build(a_texts, threshold=5)\n",
    "        a_vocab.save(VOCAB_A_PATH)\n",
    "        print(f\"  Saved  : {VOCAB_A_PATH}  ({len(a_vocab)} tokens)\")\n",
    "\n",
    "build_vocab_if_missing()\n",
    "\n",
    "# Load for use in later cells\n",
    "vocab_q = Vocabulary(); vocab_q.load(VOCAB_Q_PATH)\n",
    "vocab_a = Vocabulary(); vocab_a.load(VOCAB_A_PATH)\n",
    "print(f\"\\nQuestion vocab size : {len(vocab_q)}\")\n",
    "print(f\"Answer vocab size   : {len(vocab_a)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8bb4c66",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 2 — Explore the Dataset\n",
    "\n",
    "Load a few samples from the training set and visualize them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bae40bfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataset import VQADataset, vqa_collate_fn\n",
    "\n",
    "# Quick preview dataset (first 200 samples)\n",
    "preview_dataset = VQADataset(\n",
    "    image_dir=TRAIN_IMAGE_DIR,\n",
    "    question_json_path=TRAIN_QUESTION_JSON,\n",
    "    annotations_json_path=TRAIN_ANNOTATION_JSON,\n",
    "    vocab_q=vocab_q,\n",
    "    vocab_a=vocab_a,\n",
    "    split='train2014',\n",
    "    max_samples=200\n",
    ")\n",
    "\n",
    "print(f\"Dataset size (preview): {len(preview_dataset)} samples\")\n",
    "\n",
    "img_tensor, q_tensor, a_tensor = preview_dataset[0]\n",
    "print(f\"Image tensor shape    : {img_tensor.shape}\")\n",
    "print(f\"Question tensor shape : {q_tensor.shape}\")\n",
    "print(f\"Answer tensor shape   : {a_tensor.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7275c53e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "matplotlib.use('Agg')     # headless-safe backend\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from PIL import Image as PILImage\n",
    "\n",
    "def denormalize(t):\n",
    "    \"\"\"Convert ImageNet-normalized tensor -> (H, W, 3) numpy array in [0, 1].\"\"\"\n",
    "    mean = np.array([0.485, 0.456, 0.406])\n",
    "    std  = np.array([0.229, 0.224, 0.225])\n",
    "    img  = t.permute(1, 2, 0).numpy()\n",
    "    return np.clip(img * std + mean, 0, 1)\n",
    "\n",
    "def decode_tensor_str(tensor, vocab):\n",
    "    special = {vocab.word2idx.get(t, -1) for t in ('<pad>', '<start>', '<end>')}\n",
    "    return ' '.join(vocab.idx2word[int(i)] for i in tensor if int(i) not in special)\n",
    "\n",
    "# Show 6 random samples\n",
    "n_show  = 6\n",
    "indices = np.random.choice(len(preview_dataset), n_show, replace=False)\n",
    "\n",
    "fig, axes = plt.subplots(2, 3, figsize=(14, 8))\n",
    "for ax, idx in zip(axes.flat, indices):\n",
    "    img, q, a = preview_dataset[idx]\n",
    "    q_str = decode_tensor_str(q, vocab_q)\n",
    "    a_str = decode_tensor_str(a, vocab_a)\n",
    "    ax.imshow(denormalize(img))\n",
    "    ax.set_title(f'Q: {q_str}\\nA: {a_str}', fontsize=8, wrap=True)\n",
    "    ax.axis('off')\n",
    "\n",
    "plt.suptitle('Training Samples', fontsize=13, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.savefig('checkpoints/dataset_preview.png', dpi=120, bbox_inches='tight')\n",
    "plt.show()\n",
    "print(\"Saved: checkpoints/dataset_preview.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8551fab7",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 3 — Train\n",
    "\n",
    "Trains the model specified by `MODEL_TYPE` in the configuration cell.\n",
    "\n",
    "**Features enabled:**\n",
    "- Mixed precision (AMP) on CUDA\n",
    "- `ReduceLROnPlateau` learning rate scheduler\n",
    "- Best checkpoint saved to `checkpoints/model_X_best.pth`\n",
    "- Full resume checkpoint saved to `checkpoints/model_X_resume.pth`\n",
    "- Loss history saved to `checkpoints/history_model_X.json`\n",
    "\n",
    "**To resume training:** set `RESUME = 'checkpoints/model_a_resume.pth'` in the config cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f151ef9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import random\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "import tqdm.notebook as tqdm_nb\n",
    "\n",
    "from models.vqa_models import VQAModelA, VQAModelB, VQAModelC, VQAModelD, hadamard_fusion\n",
    "\n",
    "def get_model(model_type, vocab_q_size, vocab_a_size):\n",
    "    models_map = {\n",
    "        'A': VQAModelA, 'B': VQAModelB, 'C': VQAModelC, 'D': VQAModelD\n",
    "    }\n",
    "    if model_type not in models_map:\n",
    "        raise ValueError(f\"Unknown model type: {model_type}. Choose A/B/C/D.\")\n",
    "    return models_map[model_type](vocab_size=vocab_q_size, answer_vocab_size=vocab_a_size)\n",
    "\n",
    "\n",
    "def ss_forward(model, model_type, imgs, questions, decoder_input, epsilon):\n",
    "    \"\"\"\n",
    "    Scheduled Sampling forward pass (Bengio et al. 2015).\n",
    "    epsilon=1.0 -> pure teacher forcing; epsilon=0.0 -> fully autoregressive.\n",
    "    \"\"\"\n",
    "    max_len = decoder_input.size(1)\n",
    "\n",
    "    if model_type in ('C', 'D'):\n",
    "        img_features = F.normalize(model.i_encoder(imgs), p=2, dim=-1)\n",
    "        q_feat   = model.q_encoder(questions)\n",
    "        fusion   = hadamard_fusion(img_features.mean(dim=1), q_feat)\n",
    "    else:\n",
    "        img_feat = F.normalize(model.i_encoder(imgs), p=2, dim=1)\n",
    "        q_feat   = model.q_encoder(questions)\n",
    "        fusion   = hadamard_fusion(img_feat, q_feat)\n",
    "\n",
    "    h = fusion.unsqueeze(0).repeat(model.num_layers, 1, 1)\n",
    "    c = torch.zeros_like(h)\n",
    "    hidden = (h, c)\n",
    "\n",
    "    current_token = decoder_input[:, 0]\n",
    "    logits_list   = []\n",
    "\n",
    "    for t in range(max_len):\n",
    "        tok = current_token.unsqueeze(1)\n",
    "        if model_type in ('C', 'D'):\n",
    "            logit, hidden, _ = model.decoder.decode_step(tok, hidden, img_features)\n",
    "        else:\n",
    "            emb         = model.decoder.dropout(model.decoder.embedding(tok))\n",
    "            out, hidden = model.decoder.lstm(emb, hidden)\n",
    "            logit       = model.decoder.fc(out.squeeze(1))\n",
    "        logits_list.append(logit)\n",
    "        if t < max_len - 1:\n",
    "            current_token = (decoder_input[:, t + 1] if random.random() < epsilon\n",
    "                             else logit.detach().argmax(dim=-1))\n",
    "\n",
    "    return torch.stack(logits_list, dim=1)\n",
    "\n",
    "\n",
    "def train_model(model_type=MODEL_TYPE, epochs=EPOCHS, lr=LR,\n",
    "                batch_size=BATCH_SIZE, resume=RESUME,\n",
    "                scheduled_sampling=SCHEDULED_SAMPLING, ss_k=SS_K,\n",
    "                finetune_cnn=FINETUNE_CNN, cnn_lr_factor=CNN_LR_FACTOR):\n",
    "\n",
    "    os.makedirs(CHECKPOINT_DIR, exist_ok=True)\n",
    "\n",
    "    train_dataset = VQADataset(\n",
    "        image_dir=TRAIN_IMAGE_DIR, question_json_path=TRAIN_QUESTION_JSON,\n",
    "        annotations_json_path=TRAIN_ANNOTATION_JSON,\n",
    "        vocab_q=vocab_q, vocab_a=vocab_a,\n",
    "        split='train2014', max_samples=MAX_TRAIN_SAMPLES\n",
    "    )\n",
    "    val_dataset = VQADataset(\n",
    "        image_dir=VAL_IMAGE_DIR, question_json_path=VAL_QUESTION_JSON,\n",
    "        annotations_json_path=VAL_ANNOTATION_JSON,\n",
    "        vocab_q=vocab_q, vocab_a=vocab_a,\n",
    "        split='val2014', max_samples=MAX_VAL_SAMPLES\n",
    "    )\n",
    "    print(f\"Train: {len(train_dataset):,} | Val: {len(val_dataset):,}\")\n",
    "\n",
    "    pin = torch.cuda.is_available()\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True,\n",
    "                              collate_fn=vqa_collate_fn, num_workers=4, pin_memory=pin)\n",
    "    val_loader   = DataLoader(val_dataset,   batch_size=batch_size, shuffle=False,\n",
    "                              collate_fn=vqa_collate_fn, num_workers=4, pin_memory=pin)\n",
    "\n",
    "    model     = get_model(model_type, len(vocab_q), len(vocab_a)).to(DEVICE)\n",
    "    criterion = nn.CrossEntropyLoss(ignore_index=0)\n",
    "\n",
    "    # ── Differential LR when fine-tuning pretrained CNN backbone ─────────────\n",
    "    # Models B/D: unfreeze ResNet layer3+layer4 at low LR to avoid forgetting.\n",
    "    # Models A/C: scratch CNN — finetune_cnn has no effect.\n",
    "    if finetune_cnn and model_type in ('B', 'D'):\n",
    "        model.i_encoder.unfreeze_top_layers()\n",
    "        backbone_ids    = {id(p) for p in model.i_encoder.backbone_params()}\n",
    "        backbone_params = [p for p in model.parameters()\n",
    "                           if id(p) in backbone_ids and p.requires_grad]\n",
    "        other_params    = [p for p in model.parameters()\n",
    "                           if id(p) not in backbone_ids and p.requires_grad]\n",
    "        optimizer = optim.Adam([\n",
    "            {'params': other_params,    'lr': lr},\n",
    "            {'params': backbone_params, 'lr': lr * cnn_lr_factor},\n",
    "        ])\n",
    "        print(f\"CNN fine-tuning  : ON | backbone LR={lr*cnn_lr_factor:.2e}  other LR={lr:.2e}\")\n",
    "        print(f\"  backbone trainable params: {sum(p.numel() for p in backbone_params):,}\")\n",
    "    else:\n",
    "        optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "        optimizer, mode='min', factor=0.5, patience=2\n",
    "    )\n",
    "    use_amp = torch.cuda.is_available()\n",
    "    scaler  = GradScaler(enabled=use_amp)\n",
    "\n",
    "    history       = {'train_loss': [], 'val_loss': []}\n",
    "    history_path  = f\"{CHECKPOINT_DIR}/history_model_{model_type.lower()}.json\"\n",
    "    best_val_loss = float('inf')\n",
    "    start_epoch   = 0\n",
    "\n",
    "    if resume and os.path.exists(resume):\n",
    "        print(f\"Resuming from: {resume}\")\n",
    "        ckpt = torch.load(resume, map_location=lambda s, l: s)\n",
    "        model.load_state_dict(ckpt['model_state_dict'])\n",
    "        optimizer.load_state_dict(ckpt['optimizer_state_dict'])\n",
    "        scheduler.load_state_dict(ckpt['scheduler_state_dict'])\n",
    "        scaler.load_state_dict(ckpt['scaler_state_dict'])\n",
    "        start_epoch   = ckpt['epoch']\n",
    "        best_val_loss = ckpt['best_val_loss']\n",
    "        history       = ckpt.get('history', history)\n",
    "        print(f\"  Resumed at epoch {start_epoch} | best_val_loss: {best_val_loss:.4f}\")\n",
    "\n",
    "    print(f\"\\nModel: {model_type} | Device: {DEVICE}\")\n",
    "    total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    print(f\"Trainable params: {total_params:,}\")\n",
    "    if scheduled_sampling:\n",
    "        eps0 = ss_k / (ss_k + math.exp(start_epoch / ss_k))\n",
    "        epsN = ss_k / (ss_k + math.exp((start_epoch + epochs - 1) / ss_k))\n",
    "        print(f\"Scheduled Sampling: ON | k={ss_k} | epsilon {eps0:.2f} -> {epsN:.2f}\")\n",
    "\n",
    "    for epoch in tqdm_nb.trange(start_epoch, start_epoch + epochs, desc=f'Model {model_type}'):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        for imgs, questions, answer in train_loader:\n",
    "            imgs, questions, answer = imgs.to(DEVICE), questions.to(DEVICE), answer.to(DEVICE)\n",
    "            dec_in  = answer[:, :-1]\n",
    "            dec_tgt = answer[:, 1:]\n",
    "            optimizer.zero_grad()\n",
    "            with autocast(enabled=use_amp):\n",
    "                if scheduled_sampling:\n",
    "                    epsilon = ss_k / (ss_k + math.exp(epoch / ss_k))\n",
    "                    logits  = ss_forward(model, model_type, imgs, questions, dec_in, epsilon)\n",
    "                else:\n",
    "                    logits  = model(imgs, questions, dec_in)\n",
    "                loss = criterion(logits.view(-1, logits.size(-1)), dec_tgt.contiguous().view(-1))\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.unscale_(optimizer)\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 5.0)\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            total_loss += loss.item()\n",
    "        avg_train = total_loss / len(train_loader)\n",
    "\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for imgs, questions, answer in val_loader:\n",
    "                imgs, questions, answer = imgs.to(DEVICE), questions.to(DEVICE), answer.to(DEVICE)\n",
    "                with autocast(enabled=use_amp):\n",
    "                    logits = model(imgs, questions, answer[:, :-1])\n",
    "                    loss   = criterion(logits.view(-1, logits.size(-1)),\n",
    "                                       answer[:, 1:].contiguous().view(-1))\n",
    "                val_loss += loss.item()\n",
    "        avg_val    = val_loss / len(val_loader)\n",
    "        current_lr = optimizer.param_groups[0]['lr']\n",
    "\n",
    "        log = (f\"Epoch {epoch+1}/{start_epoch+epochs} | Train: {avg_train:.4f} \"\n",
    "               f\"| Val: {avg_val:.4f} | LR: {current_lr:.2e}\")\n",
    "        if scheduled_sampling:\n",
    "            log += f\" | SS ε: {ss_k / (ss_k + math.exp(epoch / ss_k)):.3f}\"\n",
    "        print(log)\n",
    "\n",
    "        scheduler.step(avg_val)\n",
    "        history['train_loss'].append(avg_train)\n",
    "        history['val_loss'].append(avg_val)\n",
    "        with open(history_path, 'w') as f:\n",
    "            json.dump(history, f, indent=2)\n",
    "\n",
    "        torch.save(model.state_dict(),\n",
    "                   f\"{CHECKPOINT_DIR}/model_{model_type.lower()}_epoch{epoch+1}.pth\")\n",
    "        torch.save({\n",
    "            'epoch': epoch + 1,\n",
    "            'model_state_dict':     model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'scheduler_state_dict': scheduler.state_dict(),\n",
    "            'scaler_state_dict':    scaler.state_dict(),\n",
    "            'best_val_loss':        best_val_loss,\n",
    "            'history':              history,\n",
    "        }, f\"{CHECKPOINT_DIR}/model_{model_type.lower()}_resume.pth\")\n",
    "\n",
    "        if avg_val < best_val_loss:\n",
    "            best_val_loss = avg_val\n",
    "            torch.save(model.state_dict(),\n",
    "                       f\"{CHECKPOINT_DIR}/model_{model_type.lower()}_best.pth\")\n",
    "            print(f\"  -> New best val loss: {best_val_loss:.4f}. Saved best checkpoint.\")\n",
    "\n",
    "    print(f\"\\nTraining complete. Best val loss: {best_val_loss:.4f}\")\n",
    "    return history\n",
    "\n",
    "\n",
    "history = train_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b741a68c",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 4 — Plot Training Curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3da8dc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_COLORS = {'A': '#1f77b4', 'B': '#ff7f0e', 'C': '#2ca02c', 'D': '#d62728'}\n",
    "MODEL_LABELS = {\n",
    "    'A': 'A — Scratch CNN, No Attn',\n",
    "    'B': 'B — ResNet101, No Attn',\n",
    "    'C': 'C — Scratch CNN, Attention',\n",
    "    'D': 'D — ResNet101, Attention',\n",
    "}\n",
    "\n",
    "def plot_all_curves(model_types=None, save_path='checkpoints/training_curves.png'):\n",
    "    if model_types is None:\n",
    "        model_types = ['A', 'B', 'C', 'D']\n",
    "\n",
    "    fig, (ax_train, ax_val) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "    plotted = False\n",
    "\n",
    "    for mt in model_types:\n",
    "        path = f\"{CHECKPOINT_DIR}/history_model_{mt.lower()}.json\"\n",
    "        if not os.path.exists(path):\n",
    "            print(f\"  [SKIP] {path} not found\")\n",
    "            continue\n",
    "        with open(path) as f:\n",
    "            h = json.load(f)\n",
    "        eps   = range(1, len(h['train_loss']) + 1)\n",
    "        color = MODEL_COLORS[mt]\n",
    "        label = MODEL_LABELS[mt]\n",
    "        ax_train.plot(eps, h['train_loss'], 'o-', ms=3, color=color, label=label)\n",
    "        ax_val.plot(eps,   h['val_loss'],   's--', ms=3, color=color, label=label)\n",
    "        plotted = True\n",
    "\n",
    "    if not plotted:\n",
    "        print(\"No history files found. Train at least one model first.\")\n",
    "        return\n",
    "\n",
    "    for ax, title in [(ax_train, 'Training Loss'), (ax_val, 'Validation Loss')]:\n",
    "        ax.set_title(title, fontsize=12)\n",
    "        ax.set_xlabel('Epoch')\n",
    "        ax.set_ylabel('Loss')\n",
    "        ax.legend(fontsize=9)\n",
    "        ax.grid(True, alpha=0.3)\n",
    "\n",
    "    fig.suptitle('VQA Training Curves', fontsize=14, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    os.makedirs(os.path.dirname(save_path), exist_ok=True)\n",
    "    plt.savefig(save_path, dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    print(f\"Saved: {save_path}\")\n",
    "\n",
    "plot_all_curves()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7af146e3",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 5 — Evaluate a Single Model\n",
    "\n",
    "Computes all metrics on the full validation split:\n",
    "- **VQA Accuracy** — `min(matching_annotations / 3, 1.0)` per sample\n",
    "- **Exact Match**\n",
    "- **BLEU-1 / 2 / 3 / 4**\n",
    "- **METEOR**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8b2f426",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "from nltk.translate.meteor_score import meteor_score\n",
    "import torch.nn.functional as F\n",
    "\n",
    "nltk.download('wordnet', quiet=True)\n",
    "nltk.download('omw-1.4', quiet=True)\n",
    "\n",
    "from inference import (\n",
    "    get_model as inf_get_model,\n",
    "    batch_greedy_decode,\n",
    "    batch_greedy_decode_with_attention,\n",
    "    batch_beam_search_decode,\n",
    "    batch_beam_search_decode_with_attention,\n",
    ")\n",
    "\n",
    "def decode_tensor_str(a_tensor, vocab):\n",
    "    special = {vocab.word2idx.get(t, -1) for t in ('<pad>', '<start>', '<end>')}\n",
    "    return ' '.join(vocab.idx2word[int(i)] for i in a_tensor if int(i) not in special)\n",
    "\n",
    "\n",
    "def run_evaluate(model_type=MODEL_TYPE, epoch=EVAL_EPOCH,\n",
    "                 num_samples=NUM_EVAL_SAMPLES, beam_width=BEAM_WIDTH):\n",
    "    checkpoint = f\"{CHECKPOINT_DIR}/model_{model_type.lower()}_epoch{epoch}.pth\"\n",
    "    if not os.path.exists(checkpoint):\n",
    "        print(f\"Checkpoint not found: {checkpoint}\")\n",
    "        return None\n",
    "\n",
    "    val_dataset = VQADataset(\n",
    "        image_dir=VAL_IMAGE_DIR,\n",
    "        question_json_path=VAL_QUESTION_JSON,\n",
    "        annotations_json_path=VAL_ANNOTATION_JSON,\n",
    "        vocab_q=vocab_q, vocab_a=vocab_a,\n",
    "        split='val2014', max_samples=num_samples\n",
    "    )\n",
    "\n",
    "    with open(VAL_ANNOTATION_JSON) as f:\n",
    "        raw_anns = json.load(f)['annotations']\n",
    "    qid_to_all = {\n",
    "        ann['question_id']: [a['answer'].lower().strip() for a in ann['answers']]\n",
    "        for ann in raw_anns\n",
    "    }\n",
    "    question_ids = [q['question_id'] for q in val_dataset.questions]\n",
    "\n",
    "    model = inf_get_model(model_type, len(vocab_q), len(vocab_a))\n",
    "    model.load_state_dict(torch.load(checkpoint, map_location=lambda s, l: s))\n",
    "    model.to(DEVICE).eval()\n",
    "\n",
    "    use_attention = model_type in ('C', 'D')\n",
    "    if beam_width > 1:\n",
    "        decode_fn     = batch_beam_search_decode_with_attention if use_attention \\\n",
    "                        else batch_beam_search_decode\n",
    "        decode_kwargs = dict(beam_width=beam_width)\n",
    "        decode_label  = f'beam (width={beam_width})'\n",
    "    else:\n",
    "        decode_fn     = batch_greedy_decode_with_attention if use_attention \\\n",
    "                        else batch_greedy_decode\n",
    "        decode_kwargs = {}\n",
    "        decode_label  = 'greedy'\n",
    "\n",
    "    val_loader = DataLoader(val_dataset, batch_size=64, shuffle=False,\n",
    "                            collate_fn=vqa_collate_fn, num_workers=2)\n",
    "\n",
    "    smoothie  = SmoothingFunction().method1\n",
    "    all_preds = []\n",
    "    all_gts   = []\n",
    "\n",
    "    print(f\"Evaluating Model {model_type} | {len(val_dataset):,} samples | decode: {decode_label}\")\n",
    "    with torch.no_grad():\n",
    "        for imgs, qs, ans in tqdm_nb.tqdm(val_loader, desc='Eval'):\n",
    "            preds = decode_fn(model, imgs, qs, vocab_a, device=DEVICE, **decode_kwargs)\n",
    "            all_preds.extend(preds)\n",
    "            for a_t in ans:\n",
    "                all_gts.append(decode_tensor_str(a_t, vocab_a))\n",
    "\n",
    "    n = len(all_preds)\n",
    "    em = vqa_acc = b1 = b2 = b3 = b4 = met = 0.0\n",
    "\n",
    "    for idx, (pred, gt) in enumerate(zip(all_preds, all_gts)):\n",
    "        p = pred.strip().lower()\n",
    "        g = gt.strip().lower()\n",
    "        if p == g:\n",
    "            em += 1\n",
    "        qid     = question_ids[idx]\n",
    "        answers = qid_to_all.get(qid, [g])\n",
    "        vqa_acc += min(sum(1 for a in answers if a == p) / 3.0, 1.0)\n",
    "        gw = gt.split() or ['<unk>']\n",
    "        pw = pred.split() or ['<unk>']\n",
    "        b1  += sentence_bleu([gw], pw, weights=(1,0,0,0),             smoothing_function=smoothie)\n",
    "        b2  += sentence_bleu([gw], pw, weights=(0.5,0.5,0,0),         smoothing_function=smoothie)\n",
    "        b3  += sentence_bleu([gw], pw, weights=(1/3,1/3,1/3,0),       smoothing_function=smoothie)\n",
    "        b4  += sentence_bleu([gw], pw, weights=(0.25,0.25,0.25,0.25), smoothing_function=smoothie)\n",
    "        met += meteor_score([gw], pw)\n",
    "\n",
    "    results = {\n",
    "        'model_type':   model_type,\n",
    "        'checkpoint':   checkpoint,\n",
    "        'n':            n,\n",
    "        'decode':       decode_label,\n",
    "        'vqa_accuracy': vqa_acc / n * 100,\n",
    "        'exact_match':  em / n * 100,\n",
    "        'bleu1':        b1 / n,\n",
    "        'bleu2':        b2 / n,\n",
    "        'bleu3':        b3 / n,\n",
    "        'bleu4':        b4 / n,\n",
    "        'meteor':       met / n,\n",
    "    }\n",
    "\n",
    "    print(f\"\\n{'='*52}\")\n",
    "    print(f\"  Model        : {results['model_type']}\")\n",
    "    print(f\"  Checkpoint   : {results['checkpoint']}\")\n",
    "    print(f\"  Decode       : {results['decode']}\")\n",
    "    print(f\"  Samples      : {results['n']:,}\")\n",
    "    print(f\"  {'-'*48}\")\n",
    "    print(f\"  VQA Accuracy : {results['vqa_accuracy']:>7.2f}%\")\n",
    "    print(f\"  Exact Match  : {results['exact_match']:>7.2f}%\")\n",
    "    print(f\"  BLEU-1       : {results['bleu1']:>8.4f}\")\n",
    "    print(f\"  BLEU-2       : {results['bleu2']:>8.4f}\")\n",
    "    print(f\"  BLEU-3       : {results['bleu3']:>8.4f}\")\n",
    "    print(f\"  BLEU-4       : {results['bleu4']:>8.4f}\")\n",
    "    print(f\"  METEOR       : {results['meteor']:>8.4f}\")\n",
    "    print(f\"{'='*52}\")\n",
    "    return results\n",
    "\n",
    "\n",
    "eval_results = run_evaluate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46d87c62",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 6 — Compare All 4 Models\n",
    "\n",
    "Evaluates each model (A / B / C / D) and prints a side-by-side comparison table.\n",
    "Models without a checkpoint at the specified epoch are skipped automatically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08c1dd01",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_models(model_types=COMPARE_MODELS, epoch=COMPARE_EPOCH,\n",
    "                   num_samples=NUM_EVAL_SAMPLES, beam_width=BEAM_WIDTH):\n",
    "    val_dataset = VQADataset(\n",
    "        image_dir=VAL_IMAGE_DIR,\n",
    "        question_json_path=VAL_QUESTION_JSON,\n",
    "        annotations_json_path=VAL_ANNOTATION_JSON,\n",
    "        vocab_q=vocab_q, vocab_a=vocab_a,\n",
    "        split='val2014', max_samples=num_samples\n",
    "    )\n",
    "\n",
    "    with open(VAL_ANNOTATION_JSON) as f:\n",
    "        raw_anns = json.load(f)['annotations']\n",
    "    qid_to_all = {\n",
    "        ann['question_id']: [a['answer'].lower().strip() for a in ann['answers']]\n",
    "        for ann in raw_anns\n",
    "    }\n",
    "    question_ids = [q['question_id'] for q in val_dataset.questions]\n",
    "\n",
    "    decode_label = f'beam (width={beam_width})' if beam_width > 1 else 'greedy'\n",
    "    print(f\"Comparing: {model_types} | epoch={epoch} | samples={len(val_dataset):,} | decode={decode_label}\")\n",
    "\n",
    "    table_rows = {}\n",
    "    smoothie   = SmoothingFunction().method1\n",
    "\n",
    "    for mt in model_types:\n",
    "        ckpt = f\"{CHECKPOINT_DIR}/model_{mt.lower()}_epoch{epoch}.pth\"\n",
    "        if not os.path.exists(ckpt):\n",
    "            print(f\"  [SKIP] {ckpt} not found.\")\n",
    "            table_rows[mt] = None\n",
    "            continue\n",
    "\n",
    "        model = inf_get_model(mt, len(vocab_q), len(vocab_a))\n",
    "        model.load_state_dict(torch.load(ckpt, map_location=lambda s, l: s))\n",
    "        model.to(DEVICE).eval()\n",
    "\n",
    "        use_attention = mt in ('C', 'D')\n",
    "        if beam_width > 1:\n",
    "            decode_fn     = batch_beam_search_decode_with_attention if use_attention \\\n",
    "                            else batch_beam_search_decode\n",
    "            decode_kwargs = dict(beam_width=beam_width)\n",
    "        else:\n",
    "            decode_fn     = batch_greedy_decode_with_attention if use_attention \\\n",
    "                            else batch_greedy_decode\n",
    "            decode_kwargs = {}\n",
    "\n",
    "        val_loader = DataLoader(val_dataset, batch_size=64, shuffle=False,\n",
    "                                collate_fn=vqa_collate_fn, num_workers=2)\n",
    "\n",
    "        all_preds, all_gts = [], []\n",
    "        with torch.no_grad():\n",
    "            for imgs, qs, ans in tqdm_nb.tqdm(val_loader, desc=f'Model {mt}', leave=False):\n",
    "                preds = decode_fn(model, imgs, qs, vocab_a, device=DEVICE, **decode_kwargs)\n",
    "                all_preds.extend(preds)\n",
    "                for a_t in ans:\n",
    "                    all_gts.append(decode_tensor_str(a_t, vocab_a))\n",
    "\n",
    "        n = len(all_preds)\n",
    "        em = vqa_acc = b1 = b2 = b3 = b4 = met = 0.0\n",
    "\n",
    "        for idx, (pred, gt) in enumerate(zip(all_preds, all_gts)):\n",
    "            p = pred.strip().lower()\n",
    "            g = gt.strip().lower()\n",
    "            if p == g:\n",
    "                em += 1\n",
    "            qid     = question_ids[idx]\n",
    "            answers = qid_to_all.get(qid, [g])\n",
    "            vqa_acc += min(sum(1 for a in answers if a == p) / 3.0, 1.0)\n",
    "            gw = gt.split() or ['<unk>']\n",
    "            pw = pred.split() or ['<unk>']\n",
    "            b1  += sentence_bleu([gw], pw, weights=(1,0,0,0),             smoothing_function=smoothie)\n",
    "            b2  += sentence_bleu([gw], pw, weights=(0.5,0.5,0,0),         smoothing_function=smoothie)\n",
    "            b3  += sentence_bleu([gw], pw, weights=(1/3,1/3,1/3,0),       smoothing_function=smoothie)\n",
    "            b4  += sentence_bleu([gw], pw, weights=(0.25,0.25,0.25,0.25), smoothing_function=smoothie)\n",
    "            met += meteor_score([gw], pw)\n",
    "\n",
    "        table_rows[mt] = {\n",
    "            'vqa_accuracy': vqa_acc / n * 100,\n",
    "            'exact_match':  em / n * 100,\n",
    "            'bleu1': b1/n, 'bleu2': b2/n, 'bleu3': b3/n, 'bleu4': b4/n,\n",
    "            'meteor': met/n, 'n': n\n",
    "        }\n",
    "\n",
    "    hdr = f\"{'Model':<7} {'VQA Acc':>9} {'Exact':>8} {'BLEU-1':>8} {'BLEU-2':>8} {'BLEU-3':>8} {'BLEU-4':>8} {'METEOR':>8}\"\n",
    "    print()\n",
    "    print(hdr)\n",
    "    print('-' * len(hdr))\n",
    "    for mt in sorted(table_rows):\n",
    "        r = table_rows[mt]\n",
    "        if r is None:\n",
    "            print(f\"{mt:<7} {'N/A':>9} {'N/A':>8} {'N/A':>8} {'N/A':>8} {'N/A':>8} {'N/A':>8} {'N/A':>8}\")\n",
    "        else:\n",
    "            print(f\"{mt:<7}\"\n",
    "                  f\" {r['vqa_accuracy']:>8.2f}%\"\n",
    "                  f\" {r['exact_match']:>7.2f}%\"\n",
    "                  f\" {r['bleu1']:>8.4f}\"\n",
    "                  f\" {r['bleu2']:>8.4f}\"\n",
    "                  f\" {r['bleu3']:>8.4f}\"\n",
    "                  f\" {r['bleu4']:>8.4f}\"\n",
    "                  f\" {r['meteor']:>8.4f}\")\n",
    "    print()\n",
    "    return table_rows\n",
    "\n",
    "\n",
    "compare_results = compare_models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "046ad90f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bar chart: VQA Accuracy & BLEU-1 across all models\n",
    "valid = {mt: r for mt, r in compare_results.items() if r is not None}\n",
    "\n",
    "if valid:\n",
    "    labels   = sorted(valid.keys())\n",
    "    vqa_vals = [valid[m]['vqa_accuracy'] for m in labels]\n",
    "    b1_vals  = [valid[m]['bleu1'] * 100 for m in labels]  # scale to %\n",
    "    met_vals = [valid[m]['meteor'] * 100 for m in labels]\n",
    "\n",
    "    x  = np.arange(len(labels))\n",
    "    w  = 0.26\n",
    "    fig, ax = plt.subplots(figsize=(10, 5))\n",
    "    ax.bar(x - w, vqa_vals, w, label='VQA Acc (%)',   color='#1f77b4')\n",
    "    ax.bar(x,     b1_vals,  w, label='BLEU-1 ×100',   color='#ff7f0e')\n",
    "    ax.bar(x + w, met_vals, w, label='METEOR ×100',   color='#2ca02c')\n",
    "\n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels([f'Model {m}' for m in labels], fontsize=11)\n",
    "    ax.set_ylabel('Score')\n",
    "    ax.set_title('Model Comparison — VQA Acc / BLEU-1 / METEOR', fontsize=13)\n",
    "    ax.legend()\n",
    "    ax.grid(axis='y', alpha=0.3)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('checkpoints/comparison_bar.png', dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    print(\"Saved: checkpoints/comparison_bar.png\")\n",
    "else:\n",
    "    print(\"No valid results to plot.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ad9c070",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 7 — Single-Sample Inference\n",
    "\n",
    "Run all 4 models on the **same image + question** and compare their predicted answers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18aebee0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms as T\n",
    "from inference import greedy_decode, greedy_decode_with_attention\n",
    "\n",
    "# Choose a sample from the val set\n",
    "SAMPLE_IDX = 0   # change to any index in the val question JSON\n",
    "\n",
    "transform = T.Compose([\n",
    "    T.Resize((224, 224)),\n",
    "    T.ToTensor(),\n",
    "    T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "with open(VAL_QUESTION_JSON) as f:\n",
    "    val_questions = json.load(f)['questions']\n",
    "with open(VAL_ANNOTATION_JSON) as f:\n",
    "    val_annotations = json.load(f)['annotations']\n",
    "\n",
    "qid2gt = {\n",
    "    ann['question_id']: ann['multiple_choice_answer']\n",
    "    for ann in val_annotations\n",
    "}\n",
    "\n",
    "sample    = val_questions[SAMPLE_IDX]\n",
    "q_text    = sample['question']\n",
    "img_id    = sample['image_id']\n",
    "qid       = sample['question_id']\n",
    "gt_answer = qid2gt.get(qid, '?')\n",
    "img_path  = os.path.join(VAL_IMAGE_DIR, f'COCO_val2014_{img_id:012d}.jpg')\n",
    "\n",
    "original_img = PILImage.open(img_path).convert('RGB')\n",
    "img_tensor   = transform(original_img)\n",
    "q_tensor     = torch.tensor(vocab_q.numericalize(q_text), dtype=torch.long)\n",
    "\n",
    "print(f\"Question  : {q_text}\")\n",
    "print(f\"GT Answer : {gt_answer}\")\n",
    "print(f\"Image     : {img_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "247f3551",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = {}\n",
    "\n",
    "for mt in ['A', 'B', 'C', 'D']:\n",
    "    ckpt = f\"{CHECKPOINT_DIR}/model_{mt.lower()}_epoch{EVAL_EPOCH}.pth\"\n",
    "    if not os.path.exists(ckpt):\n",
    "        predictions[mt] = '(checkpoint missing)'\n",
    "        continue\n",
    "    model = inf_get_model(mt, len(vocab_q), len(vocab_a))\n",
    "    model.load_state_dict(torch.load(ckpt, map_location=lambda s, l: s))\n",
    "    model.to(DEVICE).eval()\n",
    "\n",
    "    if mt in ('C', 'D'):\n",
    "        ans = greedy_decode_with_attention(model, img_tensor, q_tensor, vocab_a, device=DEVICE)\n",
    "    else:\n",
    "        ans = greedy_decode(model, img_tensor, q_tensor, vocab_a, device=DEVICE)\n",
    "\n",
    "    predictions[mt] = ans\n",
    "\n",
    "# Show image + results\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 5), gridspec_kw={'width_ratios': [1.2, 1]})\n",
    "\n",
    "axes[0].imshow(original_img)\n",
    "axes[0].set_title(f'Q: {q_text}', fontsize=10, wrap=True)\n",
    "axes[0].axis('off')\n",
    "\n",
    "table_text  = [[mt, pred] for mt, pred in sorted(predictions.items())]\n",
    "table_text += [['GT', gt_answer]]\n",
    "col_labels  = ['Model', 'Predicted Answer']\n",
    "tbl = axes[1].table(cellText=table_text, colLabels=col_labels,\n",
    "                    loc='center', cellLoc='left')\n",
    "tbl.scale(1, 2)\n",
    "tbl.auto_set_font_size(False)\n",
    "tbl.set_fontsize(11)\n",
    "axes[1].axis('off')\n",
    "axes[1].set_title('Predictions vs Ground Truth', fontsize=11)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('checkpoints/inference_sample.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nResults:\")\n",
    "for mt, pred in sorted(predictions.items()):\n",
    "    match = '✓' if pred.strip().lower() == gt_answer.strip().lower() else ' '\n",
    "    print(f\"  [{match}] Model {mt}: {pred}\")\n",
    "print(f\"       GT:       {gt_answer}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b84a0480",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 8 — Attention Visualization (Model C / D)\n",
    "\n",
    "For each generated answer token, show which image regions the decoder attended to.\n",
    "The heatmap uses the `alpha` weights from Bahdanau attention (`alpha` shape: `(49,)` → reshaped to `7×7`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9ef810e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.vqa_models import hadamard_fusion\n",
    "\n",
    "ATTN_MODEL_TYPE = 'C'   # change to 'D' for ResNet + attention\n",
    "ATTN_SAMPLE_IDX = SAMPLE_IDX\n",
    "\n",
    "\n",
    "def visualize_attention_notebook(model_type, sample_idx, epoch=EVAL_EPOCH,\n",
    "                                 save_path=None, max_tokens=12):\n",
    "    ckpt = f\"{CHECKPOINT_DIR}/model_{model_type.lower()}_epoch{epoch}.pth\"\n",
    "    if not os.path.exists(ckpt):\n",
    "        print(f\"Checkpoint not found: {ckpt}\")\n",
    "        return\n",
    "\n",
    "    model = inf_get_model(model_type, len(vocab_q), len(vocab_a))\n",
    "    model.load_state_dict(torch.load(ckpt, map_location=lambda s, l: s))\n",
    "    model.to(DEVICE).eval()\n",
    "\n",
    "    # Load sample\n",
    "    sample   = val_questions[sample_idx]\n",
    "    q_text   = sample['question']\n",
    "    img_id   = sample['image_id']\n",
    "    img_path = os.path.join(VAL_IMAGE_DIR, f'COCO_val2014_{img_id:012d}.jpg')\n",
    "\n",
    "    raw_img  = PILImage.open(img_path).convert('RGB')\n",
    "    img_t    = transform(raw_img)\n",
    "    q_t      = torch.tensor(vocab_q.numericalize(q_text), dtype=torch.long)\n",
    "\n",
    "    # ── Decode step-by-step, collect alphas ──────────────────────────\n",
    "    tokens, alphas = [], []\n",
    "    with torch.no_grad():\n",
    "        img  = img_t.unsqueeze(0).to(DEVICE)\n",
    "        q    = q_t.unsqueeze(0).to(DEVICE)\n",
    "\n",
    "        img_features = model.i_encoder(img)\n",
    "        img_features = F.normalize(img_features, p=2, dim=-1)  # (1, 49, hidden)\n",
    "        q_feat       = model.q_encoder(q)                      # (1, hidden)\n",
    "        img_mean     = img_features.mean(dim=1)                # (1, hidden)\n",
    "        fusion       = hadamard_fusion(img_mean, q_feat)\n",
    "\n",
    "        h = fusion.unsqueeze(0).repeat(model.num_layers, 1, 1)\n",
    "        c = torch.zeros_like(h)\n",
    "        hidden = (h, c)\n",
    "\n",
    "        start_idx = vocab_a.word2idx['<start>']\n",
    "        end_idx   = vocab_a.word2idx['<end>']\n",
    "        tok = torch.tensor([[start_idx]], dtype=torch.long, device=DEVICE)\n",
    "\n",
    "        for _ in range(max_tokens):\n",
    "            logit, hidden, alpha = model.decoder.decode_step(tok, hidden, img_features)\n",
    "            pred = logit.argmax(dim=-1).item()\n",
    "            if pred == end_idx:\n",
    "                break\n",
    "            word = vocab_a.idx2word.get(pred, '<unk>')\n",
    "            tokens.append(word)\n",
    "            alphas.append(alpha.squeeze(0).cpu().numpy())  # (49,)\n",
    "            tok = torch.tensor([[pred]], dtype=torch.long, device=DEVICE)\n",
    "\n",
    "    if not tokens:\n",
    "        print(\"No tokens decoded.\")\n",
    "        return\n",
    "\n",
    "    # ── Plot ─────────────────────────────────────────────────────────\n",
    "    n_cols = len(tokens) + 1\n",
    "    fig, axes = plt.subplots(1, n_cols, figsize=(3 * n_cols, 4))\n",
    "\n",
    "    img_np = np.array(raw_img.resize((224, 224))) / 255.0\n",
    "\n",
    "    axes[0].imshow(img_np)\n",
    "    axes[0].set_title('Original', fontsize=9)\n",
    "    axes[0].axis('off')\n",
    "\n",
    "    for i, (word, alpha) in enumerate(zip(tokens, alphas)):\n",
    "        attn_map = alpha.reshape(7, 7)\n",
    "        attn_map = (attn_map - attn_map.min()) / (attn_map.max() - attn_map.min() + 1e-8)\n",
    "        attn_up  = np.array(\n",
    "            PILImage.fromarray((attn_map * 255).astype(np.uint8)).resize((224, 224), PILImage.BILINEAR)\n",
    "        ) / 255.0\n",
    "        axes[i + 1].imshow(img_np)\n",
    "        axes[i + 1].imshow(attn_up, alpha=0.55, cmap='jet')\n",
    "        axes[i + 1].set_title(f'\"{word}\"', fontsize=9)\n",
    "        axes[i + 1].axis('off')\n",
    "\n",
    "    answer = ' '.join(tokens)\n",
    "    gt_ans = qid2gt.get(sample['question_id'], '?')\n",
    "    fig.suptitle(f'Model {model_type} | Q: {q_text}\\nPred: {answer}  |  GT: {gt_ans}',\n",
    "                 fontsize=10, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "\n",
    "    if save_path is None:\n",
    "        save_path = f\"{CHECKPOINT_DIR}/attn_model_{model_type.lower()}.png\"\n",
    "    plt.savefig(save_path, dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    print(f\"Question : {q_text}\")\n",
    "    print(f\"Predicted: {answer}\")\n",
    "    print(f\"GT answer: {gt_ans}\")\n",
    "    print(f\"Saved    : {save_path}\")\n",
    "\n",
    "\n",
    "visualize_attention_notebook(ATTN_MODEL_TYPE, ATTN_SAMPLE_IDX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47d9baa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize attention on multiple samples side-by-side\n",
    "N_SAMPLES_ATTN = 4   # how many samples to show\n",
    "\n",
    "ckpt = f\"{CHECKPOINT_DIR}/model_{ATTN_MODEL_TYPE.lower()}_epoch{EVAL_EPOCH}.pth\"\n",
    "if os.path.exists(ckpt):\n",
    "    model_attn = inf_get_model(ATTN_MODEL_TYPE, len(vocab_q), len(vocab_a))\n",
    "    model_attn.load_state_dict(torch.load(ckpt, map_location=lambda s, l: s))\n",
    "    model_attn.to(DEVICE).eval()\n",
    "\n",
    "    sample_indices = np.random.choice(min(500, len(val_questions)), N_SAMPLES_ATTN, replace=False)\n",
    "\n",
    "    fig, axes = plt.subplots(N_SAMPLES_ATTN, 1, figsize=(20, 5 * N_SAMPLES_ATTN))\n",
    "    if N_SAMPLES_ATTN == 1:\n",
    "        axes = [axes]\n",
    "\n",
    "    for row_ax, s_idx in zip(axes, sample_indices):\n",
    "        s        = val_questions[s_idx]\n",
    "        q_txt    = s['question']\n",
    "        i_id     = s['image_id']\n",
    "        i_path   = os.path.join(VAL_IMAGE_DIR, f'COCO_val2014_{i_id:012d}.jpg')\n",
    "        raw      = PILImage.open(i_path).convert('RGB')\n",
    "        img_t    = transform(raw)\n",
    "        q_t      = torch.tensor(vocab_q.numericalize(q_txt), dtype=torch.long)\n",
    "\n",
    "        toks, alps = [], []\n",
    "        with torch.no_grad():\n",
    "            im  = img_t.unsqueeze(0).to(DEVICE)\n",
    "            qq  = q_t.unsqueeze(0).to(DEVICE)\n",
    "            imf = F.normalize(model_attn.i_encoder(im), p=2, dim=-1)\n",
    "            qf  = model_attn.q_encoder(qq)\n",
    "            fus = hadamard_fusion(imf.mean(dim=1), qf)\n",
    "            h   = fus.unsqueeze(0).repeat(model_attn.num_layers, 1, 1)\n",
    "            hid = (h, torch.zeros_like(h))\n",
    "            tok = torch.tensor([[vocab_a.word2idx['<start>']]], dtype=torch.long, device=DEVICE)\n",
    "            for _ in range(10):\n",
    "                logit, hid, alpha = model_attn.decoder.decode_step(tok, hid, imf)\n",
    "                pred = logit.argmax(dim=-1).item()\n",
    "                if pred == vocab_a.word2idx['<end>']:\n",
    "                    break\n",
    "                toks.append(vocab_a.idx2word.get(pred, '<unk>'))\n",
    "                alps.append(alpha.squeeze(0).cpu().numpy())\n",
    "                tok = torch.tensor([[pred]], dtype=torch.long, device=DEVICE)\n",
    "\n",
    "        if not toks:\n",
    "            continue\n",
    "\n",
    "        n_cols = min(len(toks), 8) + 1\n",
    "        inner_fig, inner_axes = plt.subplots(1, n_cols, figsize=(3 * n_cols, 3.5))\n",
    "        img_np = np.array(raw.resize((224, 224))) / 255.0\n",
    "        inner_axes[0].imshow(img_np)\n",
    "        inner_axes[0].set_title('Orig', fontsize=8); inner_axes[0].axis('off')\n",
    "        for j, (w, a) in enumerate(zip(toks[:n_cols-1], alps[:n_cols-1])):\n",
    "            am = (a.reshape(7,7)); am = (am - am.min()) / (am.max() - am.min() + 1e-8)\n",
    "            au = np.array(PILImage.fromarray((am*255).astype(np.uint8)).resize((224,224), PILImage.BILINEAR)) / 255.\n",
    "            inner_axes[j+1].imshow(img_np)\n",
    "            inner_axes[j+1].imshow(au, alpha=0.5, cmap='jet')\n",
    "            inner_axes[j+1].set_title(f'\"{w}\"', fontsize=8)\n",
    "            inner_axes[j+1].axis('off')\n",
    "        ans = ' '.join(toks)\n",
    "        gt  = qid2gt.get(s['question_id'], '?')\n",
    "        inner_fig.suptitle(f'Q: {q_txt}  |  Pred: {ans}  |  GT: {gt}', fontsize=9, fontweight='bold')\n",
    "        inner_fig.tight_layout()\n",
    "        inner_fig.savefig(f\"{CHECKPOINT_DIR}/attn_multi_{s_idx}.png\", dpi=120, bbox_inches='tight')\n",
    "        plt.show()\n",
    "        plt.close(inner_fig)\n",
    "else:\n",
    "    print(f\"Model {ATTN_MODEL_TYPE} checkpoint not found at: {ckpt}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63486cba",
   "metadata": {},
   "source": [
    "---\n",
    "## Summary\n",
    "\n",
    "| Step | Output |\n",
    "|------|--------|\n",
    "| Vocab | `data/processed/vocab_questions.json`, `vocab_answers.json` |\n",
    "| Train | `checkpoints/model_X_epoch*.pth`, `model_X_best.pth`, `model_X_resume.pth`, `history_model_X.json` |\n",
    "| Curves | `checkpoints/training_curves.png` |\n",
    "| Evaluation | Printed table: VQA Acc / Exact Match / BLEU-1~4 / METEOR |\n",
    "| Comparison | `checkpoints/comparison_bar.png` |\n",
    "| Inference | `checkpoints/inference_sample.png` |\n",
    "| Attention | `checkpoints/attn_model_C.png`, `attn_multi_*.png` |\n",
    "\n",
    "**To re-run a specific step**, simply re-execute that cell — all variables from earlier cells are available throughout the notebook session."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
