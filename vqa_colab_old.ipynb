{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e5c79e5d",
   "metadata": {},
   "source": [
    "# Visual Question Answering â€” End-to-End Pipeline\n",
    "\n",
    "**BÃ i toÃ¡n:** Cho áº£nh + cÃ¢u há»i â†’ sinh cÃ¢u tráº£ lá»i báº±ng LSTM-Decoder.\n",
    "\n",
    "**4 kiáº¿n trÃºc:**\n",
    "\n",
    "| Model | CNN Encoder | Attention |\n",
    "|-------|-------------|----------|\n",
    "| A | Scratch CNN | No |\n",
    "| B | Pretrained ResNet101 | No |\n",
    "| C | Scratch CNN | Bahdanau |\n",
    "| D | Pretrained ResNet101 | Bahdanau |\n",
    "\n",
    "**Pipeline:**\n",
    "1. Clone repo + cÃ i Ä‘áº·t dependencies\n",
    "2. Táº£i dá»¯ liá»‡u VQA 2.0 tá»« Kaggle\n",
    "3. Build vocab (questions + answers)\n",
    "4. Train 4 models (A, B, C, D)\n",
    "5. Plot training curves\n",
    "6. Evaluate tá»«ng model (VQA Accuracy, Exact Match, BLEU-1/2/3/4, METEOR)\n",
    "7. So sÃ¡nh 4 models side-by-side\n",
    "8. Inference trÃªn sample\n",
    "9. Attention Visualization (Model C, D)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f5b83d0",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 0 â€” Environment Setup\n",
    "\n",
    "- Kiá»ƒm tra GPU\n",
    "- Clone repository tá»« GitHub\n",
    "- CÃ i Ä‘áº·t dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20779874",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "print(f\"PyTorch: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48ce8e6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone repository\n",
    "!git clone https://github.com/Anakonkai01/new_vqa.git\n",
    "%cd new_vqa\n",
    "\n",
    "# Checkout branch (thay Ä‘á»•i náº¿u cáº§n)\n",
    "# !git checkout experiment/new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "192b27f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "!git pull"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffbf78fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CÃ i Ä‘áº·t dependencies\n",
    "!pip install -q nltk tqdm matplotlib Pillow\n",
    "\n",
    "import nltk\n",
    "nltk.download('wordnet', quiet=True)\n",
    "nltk.download('omw-1.4', quiet=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "655fe7d4",
   "metadata": {},
   "source": [
    "---\n",
    "## Google Drive â€” Mount & Setup\n",
    "\n",
    "Mount Google Drive Ä‘á»ƒ lÆ°u trá»¯:\n",
    "- **Checkpoints** (model weights) â€” quan trá»ng nháº¥t, máº¥t nhiá»u giá» Ä‘á»ƒ train láº¡i\n",
    "- **Vocab files** â€” nhá» nhÆ°ng cáº§n thiáº¿t Ä‘á»ƒ resume\n",
    "- **Output files** â€” training curves, attention maps, analysis plots\n",
    "\n",
    "> Khi runtime Colab bá»‹ disconnect, má»i dá»¯ liá»‡u local sáº½ **máº¥t**. Drive giÃºp báº¡n resume training tá»« checkpoint Ä‘Ã£ lÆ°u."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd1d5dc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# ThÆ° má»¥c lÆ°u trá»¯ trÃªn Drive\n",
    "DRIVE_DIR = '/content/drive/MyDrive/VQA_Project'\n",
    "\n",
    "import os\n",
    "os.makedirs(f'{DRIVE_DIR}/checkpoints', exist_ok=True)\n",
    "os.makedirs(f'{DRIVE_DIR}/vocab', exist_ok=True)\n",
    "os.makedirs(f'{DRIVE_DIR}/outputs', exist_ok=True)\n",
    "\n",
    "print(f\"Drive project dir: {DRIVE_DIR}\")\n",
    "print(f\"  checkpoints/  â€” model weights (resume, best, milestones)\")\n",
    "print(f\"  vocab/         â€” vocab_questions.json, vocab_answers.json\")\n",
    "print(f\"  outputs/       â€” training curves, attention maps, analysis plots\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbb18152",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil, os, glob\n",
    "\n",
    "DRIVE_DIR = '/content/drive/MyDrive/VQA_Project'\n",
    "\n",
    "def sync_to_drive(src_pattern, drive_subdir, label=\"\"):\n",
    "    \"\"\"Copy files matching src_pattern to Drive subfolder.\"\"\"\n",
    "    dst_dir = f'{DRIVE_DIR}/{drive_subdir}'\n",
    "    os.makedirs(dst_dir, exist_ok=True)\n",
    "    files = glob.glob(src_pattern)\n",
    "    if not files:\n",
    "        print(f\"  [SKIP] No files matching: {src_pattern}\")\n",
    "        return\n",
    "    for f in files:\n",
    "        dst = os.path.join(dst_dir, os.path.basename(f))\n",
    "        shutil.copy2(f, dst)\n",
    "    names = [os.path.basename(f) for f in files]\n",
    "    total_mb = sum(os.path.getsize(f) for f in files) / 1e6\n",
    "    print(f\"  âœ“ {label or drive_subdir}: {len(files)} files ({total_mb:.1f} MB) â†’ Drive/{drive_subdir}/\")\n",
    "\n",
    "def restore_from_drive(drive_subdir, local_dir, label=\"\"):\n",
    "    \"\"\"Restore files from Drive subfolder to local directory.\"\"\"\n",
    "    src_dir = f'{DRIVE_DIR}/{drive_subdir}'\n",
    "    if not os.path.exists(src_dir):\n",
    "        print(f\"  [SKIP] Drive/{drive_subdir}/ not found\")\n",
    "        return 0\n",
    "    os.makedirs(local_dir, exist_ok=True)\n",
    "    files = [f for f in os.listdir(src_dir) if os.path.isfile(os.path.join(src_dir, f))]\n",
    "    for f in files:\n",
    "        shutil.copy2(os.path.join(src_dir, f), os.path.join(local_dir, f))\n",
    "    total_mb = sum(os.path.getsize(os.path.join(local_dir, f)) for f in files) / 1e6\n",
    "    print(f\"  âœ“ {label or drive_subdir}: restored {len(files)} files ({total_mb:.1f} MB)\")\n",
    "    return len(files)\n",
    "\n",
    "print(\"Helper functions defined: sync_to_drive(), restore_from_drive()\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03dbb8fb",
   "metadata": {},
   "source": [
    "### âš¡ Restore tá»« Drive (cháº¡y cell dÆ°á»›i khi runtime restart)\n",
    "\n",
    "Náº¿u Colab **bá»‹ disconnect** giá»¯a chá»«ng, cháº¡y láº¡i cÃ¡c cell:\n",
    "1. Mount Drive (cell trÃªn)\n",
    "2. Cell helper functions (cell trÃªn)\n",
    "3. **Cell restore dÆ°á»›i Ä‘Ã¢y** â€” khÃ´i phá»¥c checkpoints + vocab tá»« Drive vá» local\n",
    "\n",
    "> Sau Ä‘Ã³ **bá» qua** cÃ¡c step Ä‘Ã£ hoÃ n thÃ nh vÃ  cháº¡y tiáº¿p tá»« phase tiáº¿p theo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56b5bc25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# âš¡ RESTORE â€” Chá»‰ cháº¡y khi runtime restart vÃ  cáº§n resume training\n",
    "# Náº¿u Ä‘Ã¢y lÃ  láº§n cháº¡y Ä‘áº§u tiÃªn, Bá» QUA cell nÃ y\n",
    "\n",
    "print(\"=== Restoring from Google Drive ===\")\n",
    "n1 = restore_from_drive('checkpoints', 'checkpoints', 'Checkpoints')\n",
    "n2 = restore_from_drive('vocab', 'data/processed', 'Vocab')\n",
    "n3 = restore_from_drive('outputs', 'checkpoints', 'Outputs (curves, plots)')\n",
    "\n",
    "if n1 + n2 + n3 == 0:\n",
    "    print(\"\\n  KhÃ´ng cÃ³ gÃ¬ Ä‘á»ƒ restore â€” cÃ³ thá»ƒ Ä‘Ã¢y lÃ  láº§n cháº¡y Ä‘áº§u tiÃªn.\")\n",
    "else:\n",
    "    print(f\"\\n  âœ“ Restore hoÃ n táº¥t! Tá»•ng: {n1+n2+n3} files\")\n",
    "    print(\"  â†’ Tiáº¿p tá»¥c training tá»« phase tiáº¿p theo (dÃ¹ng --resume)\")\n",
    "    # Hiá»‡n checkpoints Ä‘Ã£ restore\n",
    "    if os.path.exists('checkpoints'):\n",
    "        print(\"\\n  Checkpoints hiá»‡n cÃ³:\")\n",
    "        for f in sorted(os.listdir('checkpoints')):\n",
    "            if f.endswith('.pth'):\n",
    "                sz = os.path.getsize(f'checkpoints/{f}') / 1e6\n",
    "                print(f\"    {f:40s} {sz:8.1f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b48ad651",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 1 â€” Download VQA 2.0 Data tá»« Kaggle\n",
    "\n",
    "Táº£i 3 datasets:\n",
    "- **vqa-20-images**: COCO train2014 images\n",
    "- **vqa-2-0-val2014**: COCO val2014 images\n",
    "- **vqa2-0-data-json**: VQA 2.0 question + annotation JSON files\n",
    "\n",
    "> **Note:** Cáº§n cáº¥u hÃ¬nh Kaggle API key trÆ°á»›c (upload `kaggle.json` hoáº·c set biáº¿n mÃ´i trÆ°á»ng)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bdcf6ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Náº¿u chÆ°a cÃ³ kaggle.json, upload nÃ³:\n",
    "from google.colab import files\n",
    "files.upload()  # upload kaggle.json\n",
    "!mkdir -p ~/.kaggle && mv kaggle.json ~/.kaggle/ && chmod 600 ~/.kaggle/kaggle.json\n",
    "\n",
    "!pip install -q kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c22c71bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Táº£i dá»¯ liá»‡u tá»« Kaggle\n",
    "!kaggle datasets download -d bishoyabdelmassieh/vqa-20-images -p datasets --unzip\n",
    "!kaggle datasets download -d hongnhnnguyntrn/vqa-2-0-val2014 -p datasets --unzip\n",
    "!kaggle datasets download -d hongnhnnguyntrn/vqa2-0-data-json -p datasets --unzip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ca3f08c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kiá»ƒm tra cáº¥u trÃºc dataset Ä‘Ã£ táº£i\n",
    "import os\n",
    "print(\"Downloaded files:\")\n",
    "for root, dirs, files in os.walk('datasets'):\n",
    "    level = root.replace('datasets', '').count(os.sep)\n",
    "    indent = ' ' * 2 * level\n",
    "    print(f\"{indent}{os.path.basename(root)}/\")\n",
    "    if level < 2:  # chá»‰ hiá»‡n 2 levels Ä‘áº§u\n",
    "        subindent = ' ' * 2 * (level + 1)\n",
    "        for f in files[:5]:\n",
    "            print(f\"{subindent}{f}\")\n",
    "        if len(files) > 5:\n",
    "            print(f\"{subindent}... ({len(files)} files total)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5b09220",
   "metadata": {},
   "source": [
    "### Sáº¯p xáº¿p dá»¯ liá»‡u vÃ o Ä‘Ãºng cáº¥u trÃºc thÆ° má»¥c project\n",
    "\n",
    "Project yÃªu cáº§u cáº¥u trÃºc:\n",
    "```\n",
    "data/raw/images/train2014/   â† COCO train images\n",
    "data/raw/images/val2014/     â† COCO val images  \n",
    "data/raw/vqa_json/           â† VQA 2.0 JSON files\n",
    "data/processed/              â† vocab files (sáº½ Ä‘Æ°á»£c táº¡o á»Ÿ step sau)\n",
    "```\n",
    "\n",
    "> **Quan trá»ng:** Cell dÆ°á»›i sáº½ táº¡o symlinks/move dá»¯ liá»‡u vÃ o Ä‘Ãºng vá»‹ trÃ­. HÃ£y kiá»ƒm tra output cá»§a cell trÃªn Ä‘á»ƒ xÃ¡c nháº­n Ä‘Æ°á»ng dáº«n chÃ­nh xÃ¡c, náº¿u cáº¥u trÃºc Kaggle khÃ¡c thÃ¬ sá»­a láº¡i cell dÆ°á»›i."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbcf1ee5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, glob, shutil\n",
    "\n",
    "# Táº¡o thÆ° má»¥c Ä‘Ã­ch\n",
    "os.makedirs('data/raw/images', exist_ok=True)\n",
    "os.makedirs('data/raw/vqa_json', exist_ok=True)\n",
    "os.makedirs('data/processed', exist_ok=True)\n",
    "os.makedirs('checkpoints', exist_ok=True)\n",
    "\n",
    "# â”€â”€ Helper: tÃ¬m thÆ° má»¥c chá»©a COCO images â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "def find_coco_dir(base, split):\n",
    "    \"\"\"TÃ¬m thÆ° má»¥c chá»©a áº£nh COCO_<split>_*.jpg trong base.\"\"\"\n",
    "    for root, dirs, files in os.walk(base):\n",
    "        for f in files:\n",
    "            if f.startswith(f'COCO_{split}_') and f.endswith('.jpg'):\n",
    "                return root\n",
    "    return None\n",
    "\n",
    "# â”€â”€ Symlink train2014 images â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "train_dir = find_coco_dir('datasets', 'train2014')\n",
    "if train_dir and not os.path.exists('data/raw/images/train2014'):\n",
    "    os.symlink(os.path.abspath(train_dir), 'data/raw/images/train2014')\n",
    "    print(f\"Linked train2014: {train_dir} -> data/raw/images/train2014\")\n",
    "elif os.path.exists('data/raw/images/train2014'):\n",
    "    print(\"train2014 already exists.\")\n",
    "else:\n",
    "    print(\"WARNING: Could not find train2014 images in datasets/\")\n",
    "\n",
    "# â”€â”€ Symlink val2014 images â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "val_dir = find_coco_dir('datasets', 'val2014')\n",
    "if val_dir and not os.path.exists('data/raw/images/val2014'):\n",
    "    os.symlink(os.path.abspath(val_dir), 'data/raw/images/val2014')\n",
    "    print(f\"Linked val2014: {val_dir} -> data/raw/images/val2014\")\n",
    "elif os.path.exists('data/raw/images/val2014'):\n",
    "    print(\"val2014 already exists.\")\n",
    "else:\n",
    "    print(\"WARNING: Could not find val2014 images in datasets/\")\n",
    "\n",
    "# â”€â”€ Copy VQA JSON files â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "json_patterns = [\n",
    "    'v2_OpenEnded_mscoco_train2014_questions.json',\n",
    "    'v2_OpenEnded_mscoco_val2014_questions.json',\n",
    "    'v2_mscoco_train2014_annotations.json',\n",
    "    'v2_mscoco_val2014_annotations.json',\n",
    "]\n",
    "for jname in json_patterns:\n",
    "    dst = f'data/raw/vqa_json/{jname}'\n",
    "    if os.path.exists(dst):\n",
    "        print(f\"  Already exists: {dst}\")\n",
    "        continue\n",
    "    # TÃ¬m file trong datasets/\n",
    "    matches = glob.glob(f'datasets/**/{jname}', recursive=True)\n",
    "    if matches:\n",
    "        shutil.copy2(matches[0], dst)\n",
    "        print(f\"  Copied: {matches[0]} -> {dst}\")\n",
    "    else:\n",
    "        print(f\"  WARNING: {jname} not found in datasets/\")\n",
    "\n",
    "# â”€â”€ Verify â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "print(\"\\n--- Verification ---\")\n",
    "for p in ['data/raw/images/train2014', 'data/raw/images/val2014']:\n",
    "    if os.path.exists(p):\n",
    "        n = len(os.listdir(p))\n",
    "        print(f\"  {p}: {n:,} files\")\n",
    "    else:\n",
    "        print(f\"  MISSING: {p}\")\n",
    "for p in json_patterns:\n",
    "    full = f'data/raw/vqa_json/{p}'\n",
    "    sz = os.path.getsize(full) / 1e6 if os.path.exists(full) else 0\n",
    "    print(f\"  {full}: {sz:.1f} MB\" if sz > 0 else f\"  MISSING: {full}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "442f1d33",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 2 â€” Build Vocabulary\n",
    "\n",
    "XÃ¢y dá»±ng:\n",
    "- **Question vocabulary**: cÃ¡c tá»« xuáº¥t hiá»‡n >= 3 láº§n trong training questions\n",
    "- **Answer vocabulary**: cÃ¡c cÃ¢u tráº£ lá»i xuáº¥t hiá»‡n >= 5 láº§n\n",
    "\n",
    "Output:\n",
    "- `data/processed/vocab_questions.json`\n",
    "- `data/processed/vocab_answers.json`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3145d1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python src/scripts/1_build_vocab.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59448812",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kiá»ƒm tra vocab Ä‘Ã£ táº¡o\n",
    "import json\n",
    "\n",
    "with open('data/processed/vocab_questions.json') as f:\n",
    "    vq = json.load(f)\n",
    "with open('data/processed/vocab_answers.json') as f:\n",
    "    va = json.load(f)\n",
    "\n",
    "print(f\"Question vocab size: {len(vq['word2idx'])}\")\n",
    "print(f\"Answer vocab size  : {len(va['word2idx'])}\")\n",
    "print(f\"\\nSample question words: {list(vq['word2idx'].keys())[:15]}\")\n",
    "print(f\"Sample answer words  : {list(va['word2idx'].keys())[:15]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da31e69f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ğŸ’¾ LÆ°u vocab lÃªn Drive (nhá», nhÆ°ng quan trá»ng Ä‘á»ƒ resume)\n",
    "print(\"=== Syncing vocab to Drive ===\")\n",
    "sync_to_drive('data/processed/vocab_*.json', 'vocab', 'Vocab files')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa32bb7f",
   "metadata": {},
   "source": [
    "---\n",
    "## ÄÃ¡nh giÃ¡ dá»±a vÃ o Ä‘á»™ Ä‘o nÃ o? Táº¡i sao?\n",
    "\n",
    "BÃ i toÃ¡n VQA vá»›i output dáº¡ng **generative** (LSTM-Decoder sinh cÃ¢u tráº£ lá»i token-by-token) cáº§n nhiá»u gÃ³c Ä‘Ã¡nh giÃ¡ khÃ¡c nhau. ChÃºng tÃ´i sá»­ dá»¥ng **7 metrics** sau:\n",
    "\n",
    "### 1. VQA Accuracy (Metric chÃ­nh)\n",
    "$$\\text{VQA Acc}(a) = \\min\\left(\\frac{\\text{sá»‘ annotators tráº£ lá»i giá»‘ng prediction}}{3},\\; 1.0\\right)$$\n",
    "\n",
    "- ÄÃ¢y lÃ  **official metric** cá»§a VQA Challenge (Antol et al., 2015).\n",
    "- Má»—i cÃ¢u há»i cÃ³ **10 annotators** tráº£ lá»i â†’ náº¿u â‰¥3 ngÆ°á»i Ä‘á»“ng Ã½ vá»›i prediction â†’ Ä‘iá»ƒm tá»‘i Ä‘a.\n",
    "- **Táº¡i sao chá»n**: Metric nÃ y pháº£n Ã¡nh thá»±c táº¿ ráº±ng nhiá»u cÃ¢u há»i cÃ³ nhiá»u Ä‘Ã¡p Ã¡n há»£p lá»‡ (vÃ­ dá»¥: \"red\" vÃ  \"dark red\" Ä‘á»u Ä‘Ãºng).\n",
    "\n",
    "### 2. Exact Match\n",
    "- So khá»›p chÃ­nh xÃ¡c giá»¯a prediction vÃ  ground truth (majority answer).\n",
    "- **Táº¡i sao chá»n**: Metric Ä‘Æ¡n giáº£n nháº¥t, dá»… hiá»ƒu, nhÆ°ng **quÃ¡ nghiÃªm** â€” khÃ´ng cho phÃ©p cÃ¡c biáº¿n thá»ƒ há»£p lá»‡.\n",
    "\n",
    "### 3. BLEU-1, BLEU-2, BLEU-3, BLEU-4 (Papineni et al., 2002)\n",
    "$$\\text{BLEU-N} = \\text{BP} \\times \\exp\\left(\\sum_{n=1}^{N} w_n \\log p_n\\right)$$\n",
    "\n",
    "- Äo **n-gram precision** giá»¯a predicted answer vÃ  ground truth.\n",
    "- BLEU-1: unigram (tá»« Ä‘Æ¡n), BLEU-4: 4-gram (cá»¥m 4 tá»«).\n",
    "- **Táº¡i sao chá»n**: Metric chuáº©n cho **text generation** (machine translation, image captioning). BLEU-4 Ä‘áº·c biá»‡t quan trá»ng vÃ¬ Ä‘o kháº£ nÄƒng sinh cá»¥m tá»« Ä‘Ãºng, khÃ´ng chá»‰ tá»« Ä‘Æ¡n.\n",
    "\n",
    "### 4. METEOR (Banerjee & Lavie, 2005)\n",
    "- XÃ©t **synonyms + stemming + alignment** giá»¯a prediction vÃ  ground truth.\n",
    "- **Táº¡i sao chá»n**: BÃ¹ Ä‘áº¯p nhÆ°á»£c Ä‘iá»ƒm cá»§a BLEU â€” BLEU chá»‰ so khá»›p exact n-gram, cÃ²n METEOR hiá»ƒu ráº±ng \"car\" vÃ  \"automobile\" lÃ  cÃ¹ng nghÄ©a. TÆ°Æ¡ng quan vá»›i Ä‘Ã¡nh giÃ¡ con ngÆ°á»i tá»‘t hÆ¡n BLEU.\n",
    "\n",
    "### Tá»•ng káº¿t lá»±a chá»n metrics\n",
    "\n",
    "| Metric | Äáº·c Ä‘iá»ƒm | Vai trÃ² |\n",
    "|--------|----------|---------|\n",
    "| **VQA Accuracy** | Multi-annotator, official | Metric **chÃ­nh** Ä‘á»ƒ xáº¿p háº¡ng |\n",
    "| **Exact Match** | Strict matching | Baseline Ä‘Æ¡n giáº£n |\n",
    "| **BLEU-1â†’4** | N-gram precision | ÄÃ¡nh giÃ¡ cháº¥t lÆ°á»£ng text generation |\n",
    "| **METEOR** | Synonym-aware | Bá»• sung cho BLEU, xÃ©t ngá»¯ nghÄ©a |\n",
    "\n",
    "> **VQA Accuracy** lÃ  metric quyáº¿t Ä‘á»‹nh khi so sÃ¡nh cÃ¡c model, cÃ¡c metric cÃ²n láº¡i cung cáº¥p gÃ³c nhÃ¬n bá»• sung vá» cháº¥t lÆ°á»£ng sinh cÃ¢u tráº£ lá»i."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48d13083",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 3 â€” Training Strategy\n",
    "\n",
    "### Táº¡i sao cáº§n chia thÃ nh nhiá»u Phase?\n",
    "\n",
    "Training má»™t VQA model hiá»‡u quáº£ **khÃ´ng nÃªn lÃ m táº¥t cáº£ cÃ¹ng lÃºc**. CÃ³ 3 ká»¹ thuáº­t cáº§n Ã¡p dá»¥ng **tuáº§n tá»±**, má»—i ká»¹ thuáº­t chá»‰ hiá»‡u quáº£ khi ká»¹ thuáº­t trÆ°á»›c Ä‘Ã£ hoÃ n thÃ nh:\n",
    "\n",
    "| Phase | Ká»¹ thuáº­t | Ãp dá»¥ng cho | LÃ½ do pháº£i lÃ m tuáº§n tá»± |\n",
    "|-------|---------|------------|----------------------|\n",
    "| **1 â€” Baseline** | Teacher Forcing, ResNet frozen | Cáº£ 4 models | Decoder + Q-Encoder cáº§n há»c cÃ¡ch sá»­ dá»¥ng features trÆ°á»›c |\n",
    "| **2 â€” Fine-tune** | Unfreeze ResNet (B,D) / Continue training (A,C) | Cáº£ 4 models | ResNet chá»‰ nÃªn adapt khi decoder á»•n Ä‘á»‹nh; A/C train thÃªm Ä‘á»ƒ cÃ´ng báº±ng |\n",
    "| **3 â€” Scheduled Sampling** | Dáº§n thay GT báº±ng model prediction | Cáº£ 4 models | Model pháº£i predict tÆ°Æ¡ng Ä‘á»‘i Ä‘Ãºng trÆ°á»›c, náº¿u khÃ´ng SS sáº½ feed garbage |\n",
    "\n",
    "> **NguyÃªn táº¯c cÃ´ng báº±ng:** Má»—i phase Ã¡p dá»¥ng cho **táº¥t cáº£ 4 models** vá»›i cÃ¹ng sá»‘ epochs **vÃ  cÃ¹ng batch size (`bs=256`)**. Evaluate + Compare sau **má»—i phase** Ä‘á»ƒ tháº¥y progression. ÄÃ¢y lÃ  controlled experiment â€” thay Ä‘á»•i duy nháº¥t giá»¯a cÃ¡c models lÃ  **kiáº¿n trÃºc** (CNNEncoder + cÃ³/khÃ´ng Attention).\n",
    "\n",
    "### VÃ¬ sao KHÃ”NG unfreeze ResNet ngay tá»« Ä‘áº§u?\n",
    "\n",
    "> ResNet101 pretrained Ä‘Ã£ há»c features ráº¥t tá»‘t tá»« ImageNet. Náº¿u unfreeze ngay vá»›i `lr=1e-3`, **gradient tá»« random decoder** sáº½ lÃ  noise, phÃ¡ há»§y pretrained weights (catastrophic forgetting) trÆ°á»›c khi decoder ká»‹p há»c. **Chuáº©n practice** (Show Attend & Tell, Bottom-Up Top-Down): freeze trÆ°á»›c â†’ unfreeze sau.\n",
    "\n",
    "### VÃ¬ sao KHÃ”NG dÃ¹ng Scheduled Sampling ngay tá»« Ä‘áº§u?\n",
    "\n",
    "> á» epoch Ä‘áº§u, model predict gáº§n nhÆ° random. Scheduled Sampling sáº½ feed **garbage tokens** lÃ m input â†’ training cháº­m 2-3Ã—, loss khÃ³ giáº£m, gradient noisy. SS chá»‰ cÃ³ Ã½ nghÄ©a khi model Ä‘Ã£ Ä‘áº¡t prediction tÆ°Æ¡ng Ä‘á»‘i Ä‘Ãºng â†’ \"há»c cÃ¡ch recover tá»« lá»—i nhá»\" thay vÃ¬ \"bá»‹ Ä‘áº§u Ä‘á»™c bá»Ÿi noise\".\n",
    "\n",
    "### Tham sá»‘ tá»‘i Æ°u cho RTX PRO 6000 Blackwell (~102GB VRAM)\n",
    "\n",
    "| Parameter | Value | Ghi chÃº |\n",
    "|-----------|-------|---------|\n",
    "| `embed_size` | 512 | Chuáº©n cho VQA |\n",
    "| `hidden_size` | 1024 | Chuáº©n cho VQA |\n",
    "| `num_layers` | 2 | Äá»§ cho LSTM decoder |\n",
    "| `batch_size` | 256 | Thá»‘ng nháº¥t cho cáº£ 3 phases, 4 models â€” 102GB VRAM cho phÃ©p |\n",
    "| AMP | BFloat16 | Tá»± detect Blackwell Ampere+ â†’ BF16, ~2Ã— faster |\n",
    "| TF32 | Auto-enabled | Near-FP32 accuracy cho matmul + conv |\n",
    "| `cudnn.benchmark` | True | Auto-tune conv algorithms |\n",
    "| `grad_clip` | 5.0 | Stabilize training |\n",
    "| `num_workers` | 8 | Táº­n dá»¥ng bandwidth |\n",
    "| Scheduler | ReduceLROnPlateau | factor=0.5, patience=2 |\n",
    "\n",
    "### Chá»‘ng Overfitting â€” Regularization Strategy\n",
    "\n",
    "| Ká»¹ thuáº­t | GiÃ¡ trá»‹ | TÃ¡c dá»¥ng |\n",
    "|----------|---------|----------|\n",
    "| **Weight Decay** (L2) | `1e-5` | Penalize large weights â†’ ngÄƒn model memorize training data |\n",
    "| **Embedding Dropout** | `0.5` | Dropout trÃªn embedding layer (cáº£ LSTMDecoder vÃ  LSTMDecoderWithAttention) |\n",
    "| **LSTM Dropout** | `0.5` | Dropout giá»¯a LSTM layers (khi `num_layers > 1`) |\n",
    "| **Data Augmentation** | `--augment` | `RandomHorizontalFlip(0.5)` + `ColorJitter(0.2, 0.2, 0.2, 0.05)` â€” chá»‰ cho train set |\n",
    "| **Early Stopping** | `patience=3` | Dá»«ng training náº¿u val loss khÃ´ng cáº£i thiá»‡n sau 3 epochs liÃªn tiáº¿p |\n",
    "| **ReduceLROnPlateau** | `patience=2` | Giáº£m LR Ã— 0.5 khi val loss plateau |\n",
    "\n",
    "> **Táº¡i sao cáº§n regularization?** Vá»›i ~443K training samples nhÆ°ng model cÃ³ hÃ ng triá»‡u parameters (Ä‘áº·c biá»‡t khi unfreeze ResNet ~41M params á»Ÿ Phase 2), model ráº¥t dá»… overfit â€” train loss giáº£m nhÆ°ng val loss tÄƒng. Regularization Ä‘iá»u hÃ²a giá»¯a **model capacity** vÃ  **generalization**.\n",
    "\n",
    "### Batch Size â€” Controlled Experiment\n",
    "\n",
    "> **NguyÃªn táº¯c:** Táº¥t cáº£ 4 models dÃ¹ng **cÃ¹ng `batch_size=256`** trong **táº¥t cáº£ 3 phases** Ä‘á»ƒ Ä‘áº£m báº£o so sÃ¡nh cÃ´ng báº±ng khoa há»c. Batch size khÃ¡c nhau dáº«n Ä‘áº¿n:\n",
    "> - **Sá»‘ gradient updates/epoch khÃ¡c nhau** (inversely proportional)\n",
    "> - **Implicit regularization khÃ¡c nhau** (smaller batch â†’ more noise â†’ more regularization)\n",
    "> - **Effective learning rate khÃ¡c nhau** (theo linear scaling rule)\n",
    ">\n",
    "> Vá»›i **RTX PRO 6000 Blackwell 102GB VRAM**, `batch_size=256` thoáº£i mÃ¡i cho cáº£ Model D (ResNet Spatial + Attention + Unfreeze â€” model tá»‘n VRAM nháº¥t). Äiá»u nÃ y cho phÃ©p giá»¯ **cÃ¹ng batch size xuyÃªn suá»‘t 20 epochs** â†’ controlled experiment hoÃ n háº£o.\n",
    "\n",
    "### Training plan tá»•ng quan â€” Cáº£ 3 Phases\n",
    "\n",
    "| Model | Phase 1 (10ep) | Phase 2 (5ep) | Phase 3 (5ep) | Total |\n",
    "|-------|---------------|--------------|--------------|-------|\n",
    "| **A** | TF, bs=256, lr=1e-3 | Continue, bs=256, lr=5e-4 | +SS, bs=256, lr=2e-4 | 20 ep |\n",
    "| **B** | TF frozen, bs=256, lr=1e-3 | Unfreeze CNN, bs=256, lr=5e-4 | +SS+unfreeze, bs=256, lr=2e-4 | 20 ep |\n",
    "| **C** | TF, bs=256, lr=1e-3 | Continue, bs=256, lr=5e-4 | +SS, bs=256, lr=2e-4 | 20 ep |\n",
    "| **D** | TF frozen, bs=256, lr=1e-3 | Unfreeze CNN, bs=256, lr=5e-4 | +SS+unfreeze, bs=256, lr=2e-4 | 20 ep |\n",
    "\n",
    "> Táº¥t cáº£ models: **augment + weight_decay=1e-5 + early_stopping=3** xuyÃªn suá»‘t. `batch_size=256` cá»‘ Ä‘á»‹nh. Biáº¿n duy nháº¥t: **kiáº¿n trÃºc model**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ff255f1",
   "metadata": {},
   "source": [
    "### Phase 1 â€” Baseline Training (Teacher Forcing, ResNet Frozen)\n",
    "\n",
    "Train 4 kiáº¿n trÃºc vá»›i **pure teacher forcing** vÃ  ResNet **frozen** (Model B, D).\n",
    "\n",
    "**Má»¥c tiÃªu:** Decoder + Question Encoder há»™i tá»¥ trÆ°á»›c, há»c cÃ¡ch sá»­ dá»¥ng image features.\n",
    "\n",
    "| Model | Encoder | Attention | batch_size | Æ¯á»›c tÃ­nh thá»i gian/epoch |\n",
    "|-------|---------|-----------|------------|------------------------|\n",
    "| A | Scratch CNN (5 conv blocks) | No | 256 | ~15 min |\n",
    "| B | ResNet101 (frozen) | No | 256 | ~10 min |\n",
    "| C | Scratch CNN Spatial (49 regions) | Bahdanau | 256 | ~20 min |\n",
    "| D | ResNet101 Spatial (frozen) | Bahdanau | 256 | ~15 min |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0622b048",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Phase 1 â€” Train Model A: Scratch CNN, No Attention\n",
    "!python src/train.py --model A --epochs 10 --lr 1e-3 --batch_size 256 --num_workers 8 \\\n",
    "    --augment --weight_decay 1e-5 --early_stopping 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "462a35d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Phase 1 â€” Train Model B: ResNet101 (pretrained, frozen), No Attention\n",
    "!python src/train.py --model B --epochs 10 --lr 1e-3 --batch_size 256 --num_workers 8 \\\n",
    "    --augment --weight_decay 1e-5 --early_stopping 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf4e8460",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Phase 1 â€” Train Model C: Scratch CNN Spatial, Bahdanau Attention\n",
    "!python src/train.py --model C --epochs 10 --lr 1e-3 --batch_size 256 --num_workers 8 \\\n",
    "    --augment --weight_decay 1e-5 --early_stopping 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6a2d160",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Phase 1 â€” Train Model D: ResNet101 Spatial (pretrained, frozen), Bahdanau Attention\n",
    "!python src/train.py --model D --epochs 10 --lr 1e-3 --batch_size 256 --num_workers 8 \\\n",
    "    --augment --weight_decay 1e-5 --early_stopping 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04679148",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kiá»ƒm tra checkpoints Phase 1\n",
    "import os\n",
    "print(\"Saved checkpoints after Phase 1:\")\n",
    "for f in sorted(os.listdir('checkpoints')):\n",
    "    sz = os.path.getsize(f'checkpoints/{f}') / 1e6\n",
    "    print(f\"  {f:45s} {sz:8.1f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f24af4c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ğŸ’¾ Sync Phase 1 checkpoints â†’ Drive\n",
    "print(\"=== Syncing Phase 1 checkpoints to Drive ===\")\n",
    "sync_to_drive('checkpoints/model_*_resume.pth', 'checkpoints', 'Resume checkpoints')\n",
    "sync_to_drive('checkpoints/model_*_best.pth', 'checkpoints', 'Best checkpoints')\n",
    "sync_to_drive('checkpoints/model_*_epoch10.pth', 'checkpoints', 'Epoch 10 (milestone)')\n",
    "sync_to_drive('checkpoints/model_*_history.json', 'checkpoints', 'Training history')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61c73f4d",
   "metadata": {},
   "source": [
    "#### Evaluate & Compare â€” Sau Phase 1 (Baseline)\n",
    "\n",
    "So sÃ¡nh cÃ´ng báº±ng láº§n 1: Táº¥t cáº£ 4 models cÃ¹ng Ä‘iá»u kiá»‡n (10 epochs, teacher forcing, ResNet frozen).\n",
    "\n",
    "ÄÃ¢y lÃ  **controlled experiment** â€” chá»‰ khÃ¡c nhau vá» kiáº¿n trÃºc (scratch vs pretrained, no attn vs attn)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58ee1465",
   "metadata": {},
   "outputs": [],
   "source": [
    "# So sÃ¡nh 4 models sau Phase 1 (epoch 10)\n",
    "!python src/compare.py --models A,B,C,D --epoch 10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09d503f5",
   "metadata": {},
   "source": [
    "#### PhÃ¢n tÃ­ch káº¿t quáº£ Phase 1 â€” Baseline\n",
    "\n",
    "**Káº¿t quáº£ thá»±c táº¿:** D (48.84%) > B (48.66%) > C (45.33%) > A (45.25%) â€” **Ä‘Ãºng dá»± Ä‘oÃ¡n D > B > C > A**\n",
    "\n",
    "| So sÃ¡nh | Î” VQA Acc | Î” Exact | Î” BLEU-1 | Î” METEOR |\n",
    "|---------|-----------|---------|----------|----------|\n",
    "| Pretrained vs Scratch (B âˆ’ A) | **+3.41%** | +3.12% | +0.0322 | +0.0190 |\n",
    "| Pretrained vs Scratch (D âˆ’ C) | **+3.51%** | +3.04% | +0.0312 | +0.0181 |\n",
    "| Attention vs No Attn (C âˆ’ A) | **+0.08%** | +0.34% | +0.0037 | +0.0022 |\n",
    "| Attention vs No Attn (D âˆ’ B) | **+0.18%** | +0.26% | +0.0027 | +0.0013 |\n",
    "\n",
    "**PhÃ¢n tÃ­ch chi tiáº¿t:**\n",
    "\n",
    "1. **Pretrained >> Scratch (gap ~3.5%):**\n",
    "   - ResNet101 Ä‘Ã£ há»c **feature extraction cháº¥t lÆ°á»£ng cao** tá»« 1.2 triá»‡u áº£nh ImageNet â†’ edges, textures, objects, scenes.\n",
    "   - Scratch CNN (5 conv blocks, 5 layers) pháº£i há»c táº¥t cáº£ tá»« Ä‘áº§u chá»‰ vá»›i ~443K VQA samples â€” **khÃ´ng Ä‘á»§ data vÃ  capacity** Ä‘á»ƒ match 101-layer pretrained model.\n",
    "   - Gap **nháº¥t quÃ¡n** trÃªn táº¥t cáº£ metrics (VQA Acc, Exact, BLEU, METEOR) â†’ pretrained features thá»±c sá»± tá»‘t hÆ¡n, khÃ´ng pháº£i noise.\n",
    "\n",
    "2. **Attention gáº§n nhÆ° khÃ´ng giÃºp Ã­ch á»Ÿ Phase 1 (gap < 0.2%):**\n",
    "   - **Scratch CNN (C vs A): +0.08%** â€” SimpleCNNSpatial chÆ°a há»c Ä‘Æ°á»£c spatial features cÃ³ Ã½ nghÄ©a â†’ attention trÃªn features kÃ©m â‰ˆ random pooling â†’ khÃ´ng tá»‘t hÆ¡n global average pooling.\n",
    "   - **Frozen ResNet (D vs B): +0.18%** â€” ResNet cÃ³ spatial features tá»‘t (ImageNet), nhÆ°ng **frozen** â†’ chÆ°a adapt cho VQA domain. Attention trÃªn \"generic object features\" giÃºp nháº¹ nhÆ°ng chÆ°a significant.\n",
    "   - **ÄÃ¢y lÃ  káº¿t quáº£ hoÃ n toÃ n há»£p lÃ½** â€” attention chá»‰ hiá»‡u quáº£ khi spatial features cháº¥t lÆ°á»£ng cao VÃ€ relevant cho task. Phase 2 (unfreeze ResNet) sáº½ cáº£i thiá»‡n features â†’ attention gap sáº½ má»Ÿ rá»™ng.\n",
    "\n",
    "3. **Model D máº¡nh nháº¥t (48.84%):** Káº¿t há»£p pretrained features + attention â†’ nhÆ°ng chÃªnh lá»‡ch vá»›i B chá»‰ 0.18% cho tháº¥y á»Ÿ Phase 1 **pretrained features lÃ  yáº¿u tá»‘ quyáº¿t Ä‘á»‹nh**, attention chÆ°a phÃ¡t huy.\n",
    "\n",
    "> **Key insight Phase 1:** Pretrained features dominate (~3.5% gap) while attention provides negligible benefit (<0.2%). Äiá»u nÃ y confirm ráº±ng:\n",
    "> - Phase 2 (unfreeze CNN) sáº½ lÃ  **bÆ°á»›c nháº£y quan trá»ng** â€” adapt features cho VQA domain.\n",
    "> - Phase 3 (Scheduled Sampling) sáº½ giÃºp **giáº£m exposure bias** â†’ cáº£i thiá»‡n sequence generation quality.\n",
    "> - Attention gap dá»± kiáº¿n **má»Ÿ rá»™ng** sau Phase 2 khi spatial features adapt cho VQA."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4680ea0",
   "metadata": {},
   "source": [
    "### Phase 2 â€” Fine-tune / Continue Training (5 epochs, táº¥t cáº£ 4 models)\n",
    "\n",
    "Sau Phase 1, decoder + question encoder Ä‘Ã£ há»™i tá»¥. Phase 2 Ã¡p dá»¥ng cho **cáº£ 4 models** Ä‘á»ƒ Ä‘áº£m báº£o so sÃ¡nh cÃ´ng báº±ng:\n",
    "\n",
    "| Model | Ká»¹ thuáº­t Phase 2 | LÃ½ do |\n",
    "|-------|-----------------|-------|\n",
    "| **A** | Continue training (lr giáº£m) | Scratch CNN Ä‘Ã£ train end-to-end, tiáº¿p tá»¥c tá»‘i Æ°u |\n",
    "| **B** | **Unfreeze layer3+4** + differential LR | Adapt pretrained features cho VQA domain |\n",
    "| **C** | Continue training (lr giáº£m) | Scratch CNN Ä‘Ã£ train end-to-end, tiáº¿p tá»¥c tá»‘i Æ°u |\n",
    "| **D** | **Unfreeze layer3+4** + differential LR | Adapt pretrained features cho VQA domain |\n",
    "\n",
    "**Differential Learning Rate (Model B, D):**\n",
    "- Backbone (layer3+4): `lr Ã— 0.1 = 5e-5` â€” thay Ä‘á»•i cháº­m, giá»¯ pretrained knowledge\n",
    "- Head (decoder + Q-Encoder): `lr = 5e-4` â€” adapt nhanh hÆ¡n\n",
    "\n",
    "**Model A, C:** CÅ©ng giáº£m LR xuá»‘ng `5e-4` vÃ  train thÃªm 5 epochs ~ cÃ¹ng tá»•ng epochs vá»›i B, D.\n",
    "\n",
    "| Model | batch_size | LR (head) | LR (backbone) | Epochs |\n",
    "|-------|-----------|-----------|---------------|--------|\n",
    "| A | 256 | 5e-4 | â€” | 5 |\n",
    "| B | 256 | 5e-4 | 5e-5 | 5 |\n",
    "| C | 256 | 5e-4 | â€” | 5 |\n",
    "| D | 256 | 5e-4 | 5e-5 | 5 |\n",
    "\n",
    "> **`batch_size=256`** â€” giá»¯ nguyÃªn nhÆ° Phase 1. RTX PRO 6000 Blackwell 102GB VRAM cho phÃ©p dÃ¹ng bs=256 ngay cáº£ khi unfreeze ResNet cho Model D. Äáº£m báº£o cÃ¹ng sá»‘ gradient updates/epoch xuyÃªn suá»‘t."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b94268b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Phase 2 â€” Continue training Model A (resume, lower LR)\n",
    "!python src/train.py --model A --epochs 5 --lr 5e-4 --batch_size 256 \\\n",
    "    --resume checkpoints/model_a_resume.pth --num_workers 8 \\\n",
    "    --augment --weight_decay 1e-5 --early_stopping 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb009614",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Phase 2 â€” Fine-tune Model B: resume tá»« Phase 1 + unfreeze layer3+layer4\n",
    "!python src/train.py --model B --epochs 5 --lr 5e-4 --batch_size 256 \\\n",
    "    --resume checkpoints/model_b_resume.pth --finetune_cnn --cnn_lr_factor 0.1 --num_workers 8 \\\n",
    "    --augment --weight_decay 1e-5 --early_stopping 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79ec032e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Phase 2 â€” Continue training Model C (resume, lower LR)\n",
    "!python src/train.py --model C --epochs 5 --lr 5e-4 --batch_size 256 \\\n",
    "    --resume checkpoints/model_c_resume.pth --num_workers 8 \\\n",
    "    --augment --weight_decay 1e-5 --early_stopping 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5565d677",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Phase 2 â€” Fine-tune Model D: resume tá»« Phase 1 + unfreeze layer3+layer4\n",
    "!python src/train.py --model D --epochs 5 --lr 5e-4 --batch_size 256 \\\n",
    "    --resume checkpoints/model_d_resume.pth --finetune_cnn --cnn_lr_factor 0.1 --num_workers 8 \\\n",
    "    --augment --weight_decay 1e-5 --early_stopping 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32929059",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kiá»ƒm tra checkpoints sau Phase 2\n",
    "import os\n",
    "print(\"Saved checkpoints after Phase 2 (fine-tuning):\")\n",
    "for f in sorted(os.listdir('checkpoints')):\n",
    "    sz = os.path.getsize(f'checkpoints/{f}') / 1e6\n",
    "    print(f\"  {f:45s} {sz:8.1f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7aac375",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ğŸ’¾ Sync Phase 2 checkpoints â†’ Drive\n",
    "print(\"=== Syncing Phase 2 checkpoints to Drive ===\")\n",
    "sync_to_drive('checkpoints/model_*_resume.pth', 'checkpoints', 'Resume checkpoints')\n",
    "sync_to_drive('checkpoints/model_*_best.pth', 'checkpoints', 'Best checkpoints')\n",
    "sync_to_drive('checkpoints/model_*_epoch15.pth', 'checkpoints', 'Epoch 15 (milestone)')\n",
    "sync_to_drive('checkpoints/model_*_history.json', 'checkpoints', 'Training history')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68617b64",
   "metadata": {},
   "source": [
    "#### Evaluate & Compare â€” Sau Phase 2 (Fine-tune / Continue)\n",
    "\n",
    "So sÃ¡nh cÃ´ng báº±ng láº§n 2: Táº¥t cáº£ 4 models cÃ¹ng cÃ³ **15 epochs tá»•ng**.\n",
    "\n",
    "- Model B, D: Ä‘Æ°á»£c hÆ°á»Ÿng lá»£i tá»« unfreeze ResNet â†’ pretrained features adapt cho VQA\n",
    "- Model A, C: tiáº¿p tá»¥c tá»‘i Æ°u vá»›i scratch CNN\n",
    "\n",
    "So sÃ¡nh nÃ y cho tháº¥y **áº£nh hÆ°á»Ÿng thá»±c sá»± cá»§a fine-tuning pretrained backbone**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bc9019d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# So sÃ¡nh 4 models sau Phase 2 (epoch 15)\n",
    "!python src/compare.py --models A,B,C,D --epoch 15"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b22b3cc",
   "metadata": {},
   "source": [
    "#### PhÃ¢n tÃ­ch káº¿t quáº£ Phase 2 â€” Fine-tune / Continue Training\n",
    "\n",
    "**So sÃ¡nh Phase 2 vs Phase 1 â€” áº¢nh hÆ°á»Ÿng cá»§a Fine-tuning:**\n",
    "\n",
    "1. **Model B, D (Unfreeze ResNet layer3+4):**\n",
    "   - Pretrained ResNet Ä‘Æ°á»£c train trÃªn ImageNet (object classification) â†’ features tá»‘t nhÆ°ng **chÆ°a tá»‘i Æ°u cho VQA**.\n",
    "   - Unfreeze top layers cho phÃ©p ResNet **adapt features cho VQA domain** â€” vÃ­ dá»¥: há»c biá»ƒu diá»…n tá»‘t hÆ¡n cho counting, spatial relationships, colors.\n",
    "   - **Differential LR** (backbone: 5e-5, head: 5e-4) ngÄƒn **catastrophic forgetting** â€” giá»¯ pretrained knowledge á»Ÿ early layers, chá»‰ tinh chá»‰nh high-level features.\n",
    "\n",
    "2. **Model A, C (Continue training):**\n",
    "   - Scratch CNN tiáº¿p tá»¥c tá»‘i Æ°u vá»›i LR tháº¥p hÆ¡n (5e-4 vs 1e-3).\n",
    "   - Cáº£i thiá»‡n marginal â€” pháº§n lá»›n learning Ä‘Ã£ xáº£y ra á»Ÿ Phase 1.\n",
    "   - Äáº£m báº£o **so sÃ¡nh cÃ´ng báº±ng**: tá»•ng epochs báº±ng nhau cho táº¥t cáº£ models.\n",
    "\n",
    "3. **Ká»³ vá»ng cáº£i thiá»‡n:**\n",
    "   - B, D cáº£i thiá»‡n **Ä‘Ã¡ng ká»ƒ** nhá» unfreeze CNN â†’ features adapt cho VQA.\n",
    "   - A, C cáº£i thiá»‡n **nháº¹** â€” chá»§ yáº¿u tá»« continued optimization.\n",
    "   - Gap giá»¯a pretrained vs scratch **má»Ÿ rá»™ng** sau phase nÃ y.\n",
    "\n",
    "> **Key insight:** Fine-tuning pretrained backbone lÃ  ká»¹ thuáº­t quan trá»ng â€” nhÆ°ng **chá»‰ hiá»‡u quáº£ khi decoder Ä‘Ã£ á»•n Ä‘á»‹nh** (Phase 1). Náº¿u unfreeze ngay tá»« Ä‘áº§u, gradient noise tá»« random decoder sáº½ phÃ¡ há»§y pretrained weights."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48264d56",
   "metadata": {},
   "source": [
    "### Phase 3 â€” Scheduled Sampling (5 epochs, táº¥t cáº£ 4 models)\n",
    "\n",
    "Ãp dá»¥ng Scheduled Sampling cho **cáº£ 4 models** Ä‘á»ƒ so sÃ¡nh cÃ´ng báº±ng.\n",
    "\n",
    "**CÆ¡ cháº¿:**\n",
    "- Má»—i decode step, vá»›i xÃ¡c suáº¥t `Îµ` dÃ¹ng GT token, `(1-Îµ)` dÃ¹ng model's prediction\n",
    "- `Îµ` giáº£m dáº§n theo inverse-sigmoid decay: `Îµ(epoch) = k / (k + exp(epoch/k))`\n",
    "- `ss_k=5`: tá»‘c Ä‘á»™ decay vá»«a pháº£i\n",
    "\n",
    "**Táº¡i sao chá»‰ Ã¡p dá»¥ng á»Ÿ Phase 3?**\n",
    "> Model Ä‘Ã£ predict tÆ°Æ¡ng Ä‘á»‘i Ä‘Ãºng sau Phase 1+2 â†’ SS giÃºp \"há»c cÃ¡ch recover tá»« lá»—i nhá»\" thay vÃ¬ \"bá»‹ Ä‘áº§u Ä‘á»™c bá»Ÿi garbage tokens\" nhÆ° khi Ã¡p dá»¥ng ngay tá»« Ä‘áº§u.\n",
    "\n",
    "| Model | batch_size | LR (head) | LR (backbone) | ss_k | Epochs |\n",
    "|-------|-----------|-----------|---------------|------|--------|\n",
    "| A | 256 | 2e-4 | â€” | 5 | 5 |\n",
    "| B | 256 | 2e-4 | 2e-5 | 5 | 5 |\n",
    "| C | 256 | 2e-4 | â€” | 5 | 5 |\n",
    "| D | 256 | 2e-4 | 2e-5 | 5 | 5 |\n",
    "\n",
    "> Tá»•ng má»—i model: **20 epochs** (10 + 5 + 5). **`batch_size=256` xuyÃªn suá»‘t cáº£ 3 phases** â€” controlled experiment hoÃ n háº£o. So sÃ¡nh sau Phase 3 = so sÃ¡nh cuá»‘i cÃ¹ng."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d29f328",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Phase 3 â€” Scheduled Sampling cho Model A\n",
    "!python src/train.py --model A --epochs 5 --lr 2e-4 --batch_size 256 \\\n",
    "    --resume checkpoints/model_a_resume.pth \\\n",
    "    --scheduled_sampling --ss_k 5 --num_workers 8 \\\n",
    "    --augment --weight_decay 1e-5 --early_stopping 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "704359ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Phase 3 â€” Scheduled Sampling cho Model B (giá»¯ unfreeze CNN)\n",
    "!python src/train.py --model B --epochs 5 --lr 2e-4 --batch_size 256 \\\n",
    "    --resume checkpoints/model_b_resume.pth --finetune_cnn --cnn_lr_factor 0.1 \\\n",
    "    --scheduled_sampling --ss_k 5 --num_workers 8 \\\n",
    "    --augment --weight_decay 1e-5 --early_stopping 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1975c63b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Phase 3 â€” Scheduled Sampling cho Model C\n",
    "!python src/train.py --model C --epochs 5 --lr 2e-4 --batch_size 256 \\\n",
    "    --resume checkpoints/model_c_resume.pth \\\n",
    "    --scheduled_sampling --ss_k 5 --num_workers 8 \\\n",
    "    --augment --weight_decay 1e-5 --early_stopping 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0753006f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Phase 3 â€” Scheduled Sampling cho Model D (giá»¯ unfreeze CNN)\n",
    "!python src/train.py --model D --epochs 5 --lr 2e-4 --batch_size 256 \\\n",
    "    --resume checkpoints/model_d_resume.pth --finetune_cnn --cnn_lr_factor 0.1 \\\n",
    "    --scheduled_sampling --ss_k 5 --num_workers 8 \\\n",
    "    --augment --weight_decay 1e-5 --early_stopping 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ef01b72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kiá»ƒm tra checkpoints sau Phase 3\n",
    "import os\n",
    "print(\"Saved checkpoints after Phase 3 (scheduled sampling):\")\n",
    "for f in sorted(os.listdir('checkpoints')):\n",
    "    sz = os.path.getsize(f'checkpoints/{f}') / 1e6\n",
    "    print(f\"  {f:45s} {sz:8.1f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "439f8e3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ğŸ’¾ Sync Phase 3 checkpoints â†’ Drive (FINAL)\n",
    "print(\"=== Syncing Phase 3 checkpoints to Drive (FINAL) ===\")\n",
    "sync_to_drive('checkpoints/model_*_resume.pth', 'checkpoints', 'Resume checkpoints')\n",
    "sync_to_drive('checkpoints/model_*_best.pth', 'checkpoints', 'Best checkpoints')\n",
    "sync_to_drive('checkpoints/model_*_epoch20.pth', 'checkpoints', 'Epoch 20 (milestone)')\n",
    "sync_to_drive('checkpoints/model_*_history.json', 'checkpoints', 'Training history')\n",
    "print(\"\\nâœ“ Táº¥t cáº£ checkpoints cuá»‘i cÃ¹ng Ä‘Ã£ lÆ°u an toÃ n trÃªn Drive!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d781f07",
   "metadata": {},
   "source": [
    "#### Evaluate & Compare â€” Sau Phase 3 (Scheduled Sampling) â€” Final\n",
    "\n",
    "So sÃ¡nh cÃ´ng báº±ng láº§n 3 (cuá»‘i cÃ¹ng): Táº¥t cáº£ 4 models cÃ¹ng **20 epochs**, cÃ¹ng Ã¡p dá»¥ng Scheduled Sampling.\n",
    "\n",
    "ÄÃ¢y lÃ  **káº¿t quáº£ chÃ­nh** Ä‘á»ƒ Ä‘Æ°a vÃ o bÃ¡o cÃ¡o â€” controlled experiment vá»›i cáº£ 3 biáº¿n:\n",
    "1. **Scratch vs Pretrained**: A vs B, C vs D\n",
    "2. **No Attention vs Attention**: A vs C, B vs D\n",
    "3. **Progression**: Phase 1 â†’ 2 â†’ 3 cho tháº¥y áº£nh hÆ°á»Ÿng cá»§a fine-tuning vÃ  scheduled sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51788e9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# So sÃ¡nh cuá»‘i cÃ¹ng: 4 models sau Phase 3 (epoch 20)\n",
    "!python src/compare.py --models A,B,C,D --epoch 20"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7b436ce",
   "metadata": {},
   "source": [
    "#### PhÃ¢n tÃ­ch káº¿t quáº£ Phase 3 â€” Scheduled Sampling (Final)\n",
    "\n",
    "**So sÃ¡nh Phase 3 vs Phase 2 â€” áº¢nh hÆ°á»Ÿng cá»§a Scheduled Sampling:**\n",
    "\n",
    "1. **Scheduled Sampling giáº£i quyáº¿t Exposure Bias:**\n",
    "   - Training dÃ¹ng **teacher forcing** (ground truth input) nhÆ°ng inference dÃ¹ng **model's own predictions**.\n",
    "   - Sá»± khÃ¡c biá»‡t nÃ y gá»i lÃ  **exposure bias** â€” model chÆ°a bao giá» tháº¥y input sai cá»§a chÃ­nh mÃ¬nh trong training.\n",
    "   - SS dáº§n thay GT báº±ng model prediction: $\\epsilon(epoch) = \\frac{k}{k + e^{epoch/k}}$ â†’ model há»c **recover tá»« lá»—i nhá»**.\n",
    "\n",
    "2. **Cáº£i thiá»‡n dá»± kiáº¿n:**\n",
    "   - Táº¥t cáº£ 4 models Ä‘á»u hÆ°á»Ÿng lá»£i tá»« SS, nhÆ°ng má»©c Ä‘á»™ khÃ¡c nhau.\n",
    "   - Model Ä‘Ã£ predict tÆ°Æ¡ng Ä‘á»‘i Ä‘Ãºng (B, D) â†’ SS giÃºp polish thÃªm.\n",
    "   - Model yáº¿u hÆ¡n (A) â†’ SS cÅ©ng giÃºp, nhÆ°ng náº¿u prediction quÃ¡ kÃ©m thÃ¬ SS cÃ³ thá»ƒ khÃ´ng giÃºp nhiá»u.\n",
    "\n",
    "3. **Káº¿t quáº£ tá»•ng há»£p â€” Ranking cuá»‘i cÃ¹ng:**\n",
    "\n",
    "   | Rank | Model | Äáº·c Ä‘iá»ƒm | LÃ½ do |\n",
    "   |------|-------|----------|-------|\n",
    "   | 1 | **D** | Pretrained + Attention | Features tá»‘t nháº¥t + attention focus spatial |\n",
    "   | 2 | **B** | Pretrained + No Attn | Features tá»‘t, nhÆ°ng thiáº¿u spatial focus |\n",
    "   | 3 | **C** | Scratch + Attention | Attention giÃºp, nhÆ°ng features yáº¿u |\n",
    "   | 4 | **A** | Scratch + No Attn | Baseline yáº¿u nháº¥t |\n",
    "\n",
    "**PhÃ¢n tÃ­ch 2 trá»¥c chÃ­nh:**\n",
    "\n",
    "- **Trá»¥c 1 â€” Pretrained vs Scratch:** Pretrained features **luÃ´n tá»‘t hÆ¡n** vÃ¬ ResNet101 mang kiáº¿n thá»©c tá»« ImageNet (1.2M áº£nh, 1000 classes). Scratch CNN chá»‰ cÃ³ dá»¯ liá»‡u VQA (~443K) vÃ  kiáº¿n trÃºc Ä‘Æ¡n giáº£n (5 conv blocks vs 101 layers).\n",
    "\n",
    "- **Trá»¥c 2 â€” Attention vs No Attention:** Attention **giÃºp Ä‘Ã¡ng ká»ƒ** cho cÃ¡c cÃ¢u há»i cáº§n spatial reasoning (vá»‹ trÃ­, Ä‘áº¿m, mÃ u sáº¯c váº­t cá»¥ thá»ƒ). Tuy nhiÃªn, attention chá»‰ hiá»‡u quáº£ khi features Ä‘á»§ tá»‘t â€” Ä‘Ã¢y lÃ  lÃ½ do D > C nhÆ°ng gap D-B cÃ³ thá»ƒ khÃ¡c gap C-A.\n",
    "\n",
    "- **Trá»¥c 3 â€” Phase progression:** Fine-tuning (Phase 2) + Scheduled Sampling (Phase 3) **tÃ­ch lÅ©y cáº£i thiá»‡n** cho táº¥t cáº£ models, chá»©ng minh ráº±ng training strategy quan trá»ng khÃ´ng kÃ©m kiáº¿n trÃºc."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9efbb05",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 4 â€” Plot Training Curves (All Phases)\n",
    "\n",
    "So sÃ¡nh train/val loss cá»§a 4 models qua toÃ n bá»™ 20 epochs (3 phases).\n",
    "\n",
    "Output: `checkpoints/training_curves.png`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6812c136",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python src/plot_curves.py --models A,B,C,D --output checkpoints/training_curves.png"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c403aedf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hiá»ƒn thá»‹ training curves\n",
    "from IPython.display import Image, display\n",
    "display(Image(filename='checkpoints/training_curves.png'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f51eac7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ğŸ’¾ LÆ°u training curves lÃªn Drive\n",
    "print(\"=== Syncing training curves to Drive ===\")\n",
    "sync_to_drive('checkpoints/training_curves.png', 'outputs', 'Training curves')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "534a4aa9",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 5 â€” Evaluate tá»«ng Model (Best Checkpoint)\n",
    "\n",
    "ÄÃ¡nh giÃ¡ chi tiáº¿t tá»«ng model sá»­ dá»¥ng **best checkpoint** (lowest val loss qua táº¥t cáº£ phases).\n",
    "\n",
    "Metrics:\n",
    "- **VQA Accuracy**: `min(matching_annotations / 3, 1.0)` â€” official VQA metric\n",
    "- **Exact Match**: prediction == ground truth (strict)\n",
    "- **BLEU-1, BLEU-2, BLEU-3, BLEU-4**: n-gram overlap\n",
    "- **METEOR**: synonym-aware matching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7152d9cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate Model A (best checkpoint)\n",
    "!python src/evaluate.py --model_type A --checkpoint checkpoints/model_a_best.pth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "572915e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate Model B (best checkpoint)\n",
    "!python src/evaluate.py --model_type B --checkpoint checkpoints/model_b_best.pth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bc2abbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate Model C (best checkpoint)\n",
    "!python src/evaluate.py --model_type C --checkpoint checkpoints/model_c_best.pth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ac4794b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate Model D (best checkpoint)\n",
    "!python src/evaluate.py --model_type D --checkpoint checkpoints/model_d_best.pth"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fe0b481",
   "metadata": {},
   "source": [
    "### (Optional) Evaluate vá»›i Beam Search\n",
    "\n",
    "Thay vÃ¬ greedy decode (chá»n token xÃ¡c suáº¥t cao nháº¥t), beam search giá»¯ top-k candidates táº¡i má»—i bÆ°á»›c Ä‘á»ƒ tÃ¬m sequence tá»‘t hÆ¡n."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55761ca1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (Optional) Evaluate vá»›i beam search width=3\n",
    "# !python src/evaluate.py --model_type D --beam_width 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76cde939",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 6 â€” So sÃ¡nh tá»•ng há»£p 4 Models\n",
    "\n",
    "### 3 báº£ng so sÃ¡nh Ä‘Ã£ cháº¡y á»Ÿ Step 3:\n",
    "1. **Phase 1** (epoch 10): Baseline â€” controlled experiment, chá»‰ khÃ¡c kiáº¿n trÃºc\n",
    "2. **Phase 2** (epoch 15): + Fine-tune/Continue â€” áº£nh hÆ°á»Ÿng cá»§a CNN fine-tuning\n",
    "3. **Phase 3** (epoch 20): + Scheduled Sampling â€” áº£nh hÆ°á»Ÿng cá»§a SS\n",
    "\n",
    "### PhÃ¢n tÃ­ch chÃ­nh:\n",
    "- **Scratch vs Pretrained** (A vs B, C vs D): Pretrained features cÃ³ tá»‘t hÆ¡n?\n",
    "- **No Attention vs Attention** (A vs C, B vs D): Attention cÃ³ giÃºp?\n",
    "- **Phase progression**: Fine-tuning vÃ  SS cáº£i thiá»‡n bao nhiÃªu %?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68ca3b63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# So sÃ¡nh cuá»‘i cÃ¹ng â€” best checkpoint cá»§a má»—i model\n",
    "# (DÃ¹ng epoch 20 â€” sau táº¥t cáº£ phases)\n",
    "!python src/compare.py --models A,B,C,D --epoch 20"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ec5e2cf",
   "metadata": {},
   "source": [
    "#### PhÃ¢n tÃ­ch tá»•ng há»£p â€” So sÃ¡nh 4 Models qua 3 Phases\n",
    "\n",
    "**Progression qua 3 Phases:**\n",
    "\n",
    "Má»—i phase Ä‘Ã³ng gÃ³p má»™t yáº¿u tá»‘ khÃ¡c nhau vÃ o performance:\n",
    "\n",
    "| Phase | Ká»¹ thuáº­t Ã¡p dá»¥ng | áº¢nh hÆ°á»Ÿng chÃ­nh |\n",
    "|-------|-----------------|-----------------|\n",
    "| Phase 1 (10 ep) | Teacher Forcing, frozen ResNet | Decoder + Q-Encoder há»™i tá»¥, há»c cÃ¡ch sá»­ dá»¥ng features |\n",
    "| Phase 2 (+5 ep) | Unfreeze CNN (B,D), lower LR | CNN features adapt cho VQA domain â†’ B,D cáº£i thiá»‡n nhiá»u |\n",
    "| Phase 3 (+5 ep) | Scheduled Sampling | Giáº£m exposure bias â†’ cáº£i thiá»‡n inference quality |\n",
    "\n",
    "**Káº¿t luáº­n chÃ­nh:**\n",
    "\n",
    "1. **Pretrained features quan trá»ng nháº¥t:** Gap lá»›n nháº¥t giá»¯a cÃ¡c models Ä‘áº¿n tá»« viá»‡c sá»­ dá»¥ng pretrained ResNet101 vs scratch CNN. Transfer learning tá»« ImageNet cung cáº¥p feature extraction cháº¥t lÆ°á»£ng cao mÃ  scratch CNN khÃ´ng thá»ƒ Ä‘áº¡t Ä‘Æ°á»£c vá»›i lÆ°á»£ng dá»¯ liá»‡u háº¡n cháº¿.\n",
    "\n",
    "2. **Attention cáº£i thiá»‡n Ä‘Ã¡ng ká»ƒ nhÆ°ng phá»¥ thuá»™c feature quality:** Attention mechanism giÃºp model focus vÃ o vÃ¹ng áº£nh relevant, nhÆ°ng chá»‰ thá»±c sá»± hiá»‡u quáº£ khi features Ä‘á»§ tá»‘t (D > C máº¡nh hÆ¡n C > A).\n",
    "\n",
    "3. **Training strategy tÃ­ch lÅ©y:** Má»—i phase Ä‘Ã³ng gÃ³p cáº£i thiá»‡n riÃªng â€” khÃ´ng cÃ³ shortcut. Fine-tuning trÆ°á»›c khi Scheduled Sampling lÃ  thá»© tá»± Ä‘Ãºng.\n",
    "\n",
    "4. **Generative VQA vs Discriminative VQA:** Há»‡ thá»‘ng sinh answer token-by-token khÃ³ hÆ¡n nhiá»u so vá»›i chá»n 1 trong N Ä‘Ã¡p Ã¡n cá»‘ Ä‘á»‹nh, nhÆ°ng linh hoáº¡t hÆ¡n â€” cÃ³ thá»ƒ sinh cÃ¢u tráº£ lá»i chÆ°a tháº¥y trong training."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69b55d5a",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 7 â€” Single-Sample Inference\n",
    "\n",
    "Cháº¡y inference trÃªn 1 sample cá»¥ thá»ƒ Ä‘á»ƒ xem model sinh cÃ¢u tráº£ lá»i nhÆ° tháº¿ nÃ o.\n",
    "\n",
    "Script `inference.py` máº·c Ä‘á»‹nh cháº¡y model A trÃªn sample Ä‘áº§u tiÃªn. CÃ³ thá»ƒ sá»­a trá»±c tiáº¿p trong code náº¿u muá»‘n Ä‘á»•i model/sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95e6c9ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python src/inference.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d396b98",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 8 â€” Attention Visualization (Model C, D)\n",
    "\n",
    "Trá»±c quan hÃ³a cÆ¡ cháº¿ attention:\n",
    "- Vá»›i má»—i token Ä‘Æ°á»£c sinh ra, hiá»ƒn thá»‹ **heatmap** trÃªn áº£nh gá»‘c cho tháº¥y vÃ¹ng nÃ o model Ä‘ang \"nhÃ¬n vÃ o\"\n",
    "- Attention weights `alpha` cÃ³ shape `(49,)` â†’ reshape thÃ nh `7Ã—7` â†’ upsample lÃªn `224Ã—224`\n",
    "\n",
    "Output: `checkpoints/attn_model_c.png`, `checkpoints/attn_model_d.png`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9934c6e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Attention visualization â€” Model C\n",
    "!python src/visualize.py --model_type C --sample_idx 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e56294ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Attention visualization â€” Model D\n",
    "!python src/visualize.py --model_type D --sample_idx 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bf774c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hiá»ƒn thá»‹ attention maps\n",
    "from IPython.display import Image, display\n",
    "import os\n",
    "\n",
    "for mt in ['c', 'd']:\n",
    "    path = f'checkpoints/attn_model_{mt}.png'\n",
    "    if os.path.exists(path):\n",
    "        print(f\"\\n--- Model {mt.upper()} Attention ---\")\n",
    "        display(Image(filename=path))\n",
    "    else:\n",
    "        print(f\"Not found: {path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f9e08b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ğŸ’¾ LÆ°u attention maps lÃªn Drive\n",
    "print(\"=== Syncing attention maps to Drive ===\")\n",
    "sync_to_drive('checkpoints/attn_model_*.png', 'outputs', 'Attention maps')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de16d614",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 9 â€” Qualitative Analysis (VÃ­ dá»¥ Dá»± Ä‘oÃ¡n ÄÃºng & Sai)\n",
    "\n",
    "Hiá»ƒn thá»‹ má»™t sá»‘ vÃ­ dá»¥ cá»¥ thá»ƒ: áº£nh + cÃ¢u há»i + predicted answer vs ground truth.\n",
    "\n",
    "Má»¥c Ä‘Ã­ch:\n",
    "- Xem **model dá»± Ä‘oÃ¡n Ä‘Ãºng** trong trÆ°á»ng há»£p nÃ o\n",
    "- Xem **model sai** á»Ÿ Ä‘Ã¢u vÃ  táº¡i sao\n",
    "- So sÃ¡nh trá»±c quan 4 models trÃªn cÃ¹ng má»™t cÃ¢u há»i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f60c7dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, json, os, sys, random\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "import torch.nn.functional as F\n",
    "\n",
    "sys.path.append('src')\n",
    "from vocab import Vocabulary\n",
    "from inference import get_model, greedy_decode, greedy_decode_with_attention\n",
    "from models.vqa_models import hadamard_fusion\n",
    "\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Load vocab\n",
    "vocab_q = Vocabulary(); vocab_q.load('data/processed/vocab_questions.json')\n",
    "vocab_a = Vocabulary(); vocab_a.load('data/processed/vocab_answers.json')\n",
    "\n",
    "# Load val data\n",
    "VAL_IMAGE_DIR = 'data/raw/images/val2014'\n",
    "VAL_Q_JSON    = 'data/raw/vqa_json/v2_OpenEnded_mscoco_val2014_questions.json'\n",
    "VAL_A_JSON    = 'data/raw/vqa_json/v2_mscoco_val2014_annotations.json'\n",
    "\n",
    "with open(VAL_Q_JSON) as f:\n",
    "    val_questions = json.load(f)['questions']\n",
    "with open(VAL_A_JSON) as f:\n",
    "    val_annotations = json.load(f)['annotations']\n",
    "\n",
    "qid2ann = {ann['question_id']: ann for ann in val_annotations}\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "def denorm(t):\n",
    "    mean = torch.tensor([.485,.456,.406]).view(3,1,1)\n",
    "    std  = torch.tensor([.229,.224,.225]).view(3,1,1)\n",
    "    return (t*std+mean).clamp(0,1).permute(1,2,0).numpy()\n",
    "\n",
    "# Load all 4 models (best checkpoint)\n",
    "models_dict = {}\n",
    "for mt in ['A', 'B', 'C', 'D']:\n",
    "    ckpt = f'checkpoints/model_{mt.lower()}_best.pth'\n",
    "    if not os.path.exists(ckpt):\n",
    "        ckpt = f'checkpoints/model_{mt.lower()}_epoch20.pth'\n",
    "    if not os.path.exists(ckpt):\n",
    "        print(f\"  [SKIP] No checkpoint for Model {mt}\")\n",
    "        continue\n",
    "    m = get_model(mt, len(vocab_q), len(vocab_a))\n",
    "    m.load_state_dict(torch.load(ckpt, map_location='cpu'))\n",
    "    m.to(DEVICE).eval()\n",
    "    models_dict[mt] = m\n",
    "    print(f\"  Loaded Model {mt}: {ckpt}\")\n",
    "\n",
    "# Pick random samples\n",
    "random.seed(42)\n",
    "sample_indices = random.sample(range(len(val_questions)), min(6, len(val_questions)))\n",
    "\n",
    "fig, axes = plt.subplots(len(sample_indices), 1, figsize=(14, 5 * len(sample_indices)))\n",
    "if len(sample_indices) == 1:\n",
    "    axes = [axes]\n",
    "\n",
    "for row, idx in enumerate(sample_indices):\n",
    "    q_info = val_questions[idx]\n",
    "    q_text = q_info['question']\n",
    "    q_id   = q_info['question_id']\n",
    "    img_id = q_info['image_id']\n",
    "    gt_ans = qid2ann[q_id]['multiple_choice_answer']\n",
    "\n",
    "    img_path = os.path.join(VAL_IMAGE_DIR, f'COCO_val2014_{img_id:012d}.jpg')\n",
    "    if not os.path.exists(img_path):\n",
    "        continue\n",
    "\n",
    "    img = Image.open(img_path).convert('RGB')\n",
    "    img_t = transform(img)\n",
    "    q_t   = torch.tensor(vocab_q.numericalize(q_text), dtype=torch.long)\n",
    "\n",
    "    # Get predictions from all models\n",
    "    preds = {}\n",
    "    for mt, model in models_dict.items():\n",
    "        with torch.no_grad():\n",
    "            if mt in ('A', 'B'):\n",
    "                preds[mt] = greedy_decode(model, img_t, q_t, vocab_a, device=DEVICE)\n",
    "            else:\n",
    "                preds[mt] = greedy_decode_with_attention(model, img_t, q_t, vocab_a, device=DEVICE)\n",
    "\n",
    "    # Display\n",
    "    axes[row].imshow(denorm(img_t))\n",
    "    axes[row].axis('off')\n",
    "\n",
    "    pred_text = ' | '.join([f'{mt}: \"{p}\"' for mt, p in preds.items()])\n",
    "    match_markers = ' | '.join([\n",
    "        f'{mt}: {\"âœ“\" if p.strip().lower() == gt_ans.strip().lower() else \"âœ—\"}'\n",
    "        for mt, p in preds.items()\n",
    "    ])\n",
    "\n",
    "    axes[row].set_title(\n",
    "        f'Q: {q_text}\\nGT: \"{gt_ans}\" | {pred_text}\\n{match_markers}',\n",
    "        fontsize=9, loc='left', wrap=True\n",
    "    )\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('checkpoints/qualitative_analysis.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "print(\"Saved: checkpoints/qualitative_analysis.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb13c945",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ğŸ’¾ LÆ°u qualitative analysis lÃªn Drive\n",
    "print(\"=== Syncing qualitative analysis to Drive ===\")\n",
    "sync_to_drive('checkpoints/qualitative_analysis.png', 'outputs', 'Qualitative analysis')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "519f3929",
   "metadata": {},
   "source": [
    "#### Nháº­n xÃ©t Qualitative Analysis\n",
    "\n",
    "Tá»« cÃ¡c vÃ­ dá»¥ trÃªn, cÃ³ thá»ƒ quan sÃ¡t:\n",
    "\n",
    "1. **CÃ¢u há»i Yes/No:** Táº¥t cáº£ models thÆ°á»ng xá»­ lÃ½ tá»‘t â€” cÃ¢u tráº£ lá»i ngáº¯n (1 token), dá»… sinh.\n",
    "\n",
    "2. **CÃ¢u há»i Ä‘áº¿m (How many?):** Models pretrained (B, D) thÆ°á»ng chÃ­nh xÃ¡c hÆ¡n vÃ¬ ResNet features tá»‘t hÆ¡n cho object recognition. Attention (D) giÃºp focus vÃ o vÃ¹ng chá»©a objects cáº§n Ä‘áº¿m.\n",
    "\n",
    "3. **CÃ¢u há»i vá» thuá»™c tÃ­nh (What color? What kind?):** YÃªu cáº§u model hiá»ƒu fine-grained visual features. Scratch CNN (A, C) thÆ°á»ng predict cÃ¢u tráº£ lá»i phá»• biáº¿n nháº¥t thay vÃ¬ cÃ¢u tráº£ lá»i Ä‘Ãºng cho áº£nh cá»¥ thá»ƒ.\n",
    "\n",
    "4. **CÃ¢u há»i spatial (Where? What is on the left?):** Attention models (C, D) cÃ³ lá»£i tháº¿ rÃµ rá»‡t â€” cÃ³ thá»ƒ focus vÃ o vÃ¹ng spatial cá»¥ thá»ƒ trong áº£nh.\n",
    "\n",
    "5. **Failure cases phá»• biáº¿n:**\n",
    "   - Predict cÃ¢u tráº£ lá»i phá»• biáº¿n nháº¥t (\"yes\", \"2\", \"white\") báº¥t ká»ƒ áº£nh â€” **language bias**.\n",
    "   - Sinh tá»« láº·p hoáº·c vÃ´ nghÄ©a â€” **decoder degeneration** (thÆ°á»ng xáº£y ra á»Ÿ scratch models).\n",
    "   - CÃ¢u tráº£ lá»i gáº§n Ä‘Ãºng nhÆ°ng khÃ´ng exactly match (vÃ­ dá»¥ \"dark blue\" vs \"blue\") â€” metric quÃ¡ strict."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0da7f6bc",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 10 â€” Error Analysis theo Loáº¡i CÃ¢u há»i\n",
    "\n",
    "PhÃ¢n tÃ­ch accuracy theo **loáº¡i cÃ¢u há»i** (question type) Ä‘á»ƒ hiá»ƒu model máº¡nh/yáº¿u á»Ÿ Ä‘Ã¢u.\n",
    "\n",
    "VQA 2.0 annotation cung cáº¥p `answer_type` (3 loáº¡i chÃ­nh):\n",
    "- **yes/no**: CÃ¢u há»i Ä‘Ãºng/sai\n",
    "- **number**: CÃ¢u há»i Ä‘áº¿m\n",
    "- **other**: CÃ¢u há»i má»Ÿ (what, where, who, ...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a74ba83",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, json, os, sys\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import DataLoader\n",
    "import tqdm\n",
    "\n",
    "sys.path.append('src')\n",
    "from vocab import Vocabulary\n",
    "from dataset import VQADataset, vqa_collate_fn\n",
    "from inference import (get_model, batch_greedy_decode, batch_greedy_decode_with_attention)\n",
    "\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "vocab_q = Vocabulary(); vocab_q.load('data/processed/vocab_questions.json')\n",
    "vocab_a = Vocabulary(); vocab_a.load('data/processed/vocab_answers.json')\n",
    "\n",
    "# Load annotations with answer_type\n",
    "VAL_A_JSON = 'data/raw/vqa_json/v2_mscoco_val2014_annotations.json'\n",
    "with open(VAL_A_JSON) as f:\n",
    "    raw_anns = json.load(f)['annotations']\n",
    "qid2type = {ann['question_id']: ann['answer_type'] for ann in raw_anns}\n",
    "qid2all  = {ann['question_id']: [a['answer'].lower().strip() for a in ann['answers']] for ann in raw_anns}\n",
    "\n",
    "# Load val dataset\n",
    "val_dataset = VQADataset(\n",
    "    image_dir='data/raw/images/val2014',\n",
    "    question_json_path='data/raw/vqa_json/v2_OpenEnded_mscoco_val2014_questions.json',\n",
    "    annotations_json_path=VAL_A_JSON,\n",
    "    vocab_q=vocab_q, vocab_a=vocab_a, split='val2014',\n",
    "    max_samples=5000  # Limit for speed; remove for full eval\n",
    ")\n",
    "question_ids = [q['question_id'] for q in val_dataset.questions]\n",
    "\n",
    "val_loader = DataLoader(val_dataset, batch_size=64, shuffle=False,\n",
    "                        collate_fn=vqa_collate_fn, num_workers=2)\n",
    "\n",
    "def decode_tensor(a_tensor, vocab_a):\n",
    "    special = {vocab_a.word2idx['<pad>'], vocab_a.word2idx['<start>'], vocab_a.word2idx['<end>']}\n",
    "    return ' '.join([vocab_a.idx2word[int(i)] for i in a_tensor if int(i) not in special])\n",
    "\n",
    "# Evaluate each model by answer_type\n",
    "results_by_type = {}\n",
    "\n",
    "for mt in ['A', 'B', 'C', 'D']:\n",
    "    ckpt = f'checkpoints/model_{mt.lower()}_best.pth'\n",
    "    if not os.path.exists(ckpt):\n",
    "        ckpt = f'checkpoints/model_{mt.lower()}_epoch20.pth'\n",
    "    if not os.path.exists(ckpt):\n",
    "        print(f\"  [SKIP] No checkpoint for Model {mt}\")\n",
    "        continue\n",
    "\n",
    "    model = get_model(mt, len(vocab_q), len(vocab_a))\n",
    "    model.load_state_dict(torch.load(ckpt, map_location='cpu'))\n",
    "    model.to(DEVICE).eval()\n",
    "\n",
    "    decode_fn = batch_greedy_decode_with_attention if mt in ('C','D') else batch_greedy_decode\n",
    "    all_preds = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for imgs, qs, ans in tqdm.tqdm(val_loader, desc=f'Model {mt}', leave=False):\n",
    "            preds = decode_fn(model, imgs, qs, vocab_a, device=DEVICE)\n",
    "            all_preds.extend(preds)\n",
    "\n",
    "    # Compute VQA accuracy per answer_type\n",
    "    type_correct = {'yes/no': 0, 'number': 0, 'other': 0}\n",
    "    type_total   = {'yes/no': 0, 'number': 0, 'other': 0}\n",
    "\n",
    "    for idx, pred_str in enumerate(all_preds):\n",
    "        qid  = question_ids[idx]\n",
    "        atype = qid2type.get(qid, 'other')\n",
    "        pred_clean = pred_str.strip().lower()\n",
    "        all_answers = qid2all.get(qid, [])\n",
    "        match_count = sum(1 for a in all_answers if a == pred_clean)\n",
    "        vqa_acc = min(match_count / 3.0, 1.0)\n",
    "\n",
    "        type_correct[atype] = type_correct.get(atype, 0) + vqa_acc\n",
    "        type_total[atype]   = type_total.get(atype, 0) + 1\n",
    "\n",
    "    results_by_type[mt] = {\n",
    "        t: (type_correct[t] / type_total[t] * 100) if type_total[t] > 0 else 0\n",
    "        for t in ['yes/no', 'number', 'other']\n",
    "    }\n",
    "    print(f\"  Model {mt}: yes/no={results_by_type[mt]['yes/no']:.1f}%  \"\n",
    "          f\"number={results_by_type[mt]['number']:.1f}%  \"\n",
    "          f\"other={results_by_type[mt]['other']:.1f}%\")\n",
    "\n",
    "# Plot grouped bar chart\n",
    "if results_by_type:\n",
    "    fig, ax = plt.subplots(figsize=(10, 5))\n",
    "    q_types = ['yes/no', 'number', 'other']\n",
    "    x       = range(len(q_types))\n",
    "    width   = 0.18\n",
    "    colors  = {'A': '#1f77b4', 'B': '#ff7f0e', 'C': '#2ca02c', 'D': '#d62728'}\n",
    "\n",
    "    for i, (mt, res) in enumerate(sorted(results_by_type.items())):\n",
    "        vals = [res[t] for t in q_types]\n",
    "        bars = ax.bar([xi + i * width for xi in x], vals, width,\n",
    "                      label=f'Model {mt}', color=colors.get(mt, None))\n",
    "        for bar, v in zip(bars, vals):\n",
    "            ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.5,\n",
    "                    f'{v:.1f}', ha='center', va='bottom', fontsize=7)\n",
    "\n",
    "    ax.set_xlabel('Answer Type')\n",
    "    ax.set_ylabel('VQA Accuracy (%)')\n",
    "    ax.set_title('VQA Accuracy by Answer Type â€” 4 Models')\n",
    "    ax.set_xticks([xi + width * 1.5 for xi in x])\n",
    "    ax.set_xticklabels(q_types)\n",
    "    ax.legend()\n",
    "    ax.grid(axis='y', alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('checkpoints/error_analysis_by_type.png', dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    print(\"Saved: checkpoints/error_analysis_by_type.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e27de8c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ğŸ’¾ LÆ°u error analysis + attention maps lÃªn Drive\n",
    "print(\"=== Syncing analysis outputs to Drive ===\")\n",
    "sync_to_drive('checkpoints/error_analysis_by_type.png', 'outputs', 'Error analysis')\n",
    "sync_to_drive('checkpoints/attn_model_*.png', 'outputs', 'Attention maps')\n",
    "print(\"\\nâœ“ Táº¥t cáº£ outputs Ä‘Ã£ lÆ°u an toÃ n trÃªn Drive!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba54caea",
   "metadata": {},
   "source": [
    "#### Nháº­n xÃ©t Error Analysis\n",
    "\n",
    "**Dá»± kiáº¿n xu hÆ°á»›ng theo loáº¡i cÃ¢u há»i:**\n",
    "\n",
    "| Answer Type | Äáº·c Ä‘iá»ƒm | Model nÃ o tá»‘t nháº¥t? | LÃ½ do |\n",
    "|-------------|----------|---------------------|-------|\n",
    "| **yes/no** | Binary, chiáº¿m ~38% VQA | Táº¥t cáº£ tÆ°Æ¡ng Ä‘á»‘i tá»‘t | Chá»‰ cáº§n quyáº¿t Ä‘á»‹nh 1 trong 2 â†’ decoder dá»… sinh \"yes\"/\"no\" |\n",
    "| **number** | Äáº¿m (0-10+), chiáº¿m ~12% | D > B >> C > A | Cáº§n nháº­n diá»‡n + Ä‘áº¿m objects â†’ pretrained features + attention giÃºp nhiá»u |\n",
    "| **other** | Má»Ÿ, Ä‘a dáº¡ng, chiáº¿m ~50% | D > B > C > A | YÃªu cáº§u hiá»ƒu sÃ¢u áº£nh + cÃ¢u há»i â†’ khÃ³ nháº¥t cho generative model |\n",
    "\n",
    "**Insights:**\n",
    "\n",
    "1. **Yes/No gap nhá»:** CÃ¢u tráº£ lá»i chá»‰ 1 token, táº¥t cáº£ models Ä‘á»u xá»­ lÃ½ tÆ°Æ¡ng Ä‘á»‘i tá»‘t. Sá»± khÃ¡c biá»‡t chá»§ yáº¿u tá»« visual understanding, khÃ´ng pháº£i generation quality.\n",
    "\n",
    "2. **Number gap lá»›n á»Ÿ attention:** Äáº¿m objects yÃªu cáº§u focus vÃ o tá»«ng object â†’ attention mechanism giÃºp Ä‘Ã¡ng ká»ƒ. Model A (no attn, scratch) gáº§n nhÆ° Ä‘oÃ¡n random vÃ¬ khÃ´ng thá»ƒ focus vÃ o vÃ¹ng cáº§n Ä‘áº¿m.\n",
    "\n",
    "3. **Other type khÃ³ nháº¥t:** CÃ¢u tráº£ lá»i dÃ i, Ä‘a dáº¡ng â†’ generative decoder cáº§n capacity cao. Pretrained features giÃºp hiá»ƒu áº£nh tá»‘t hÆ¡n, attention giÃºp focus vÃ o chi tiáº¿t relevant.\n",
    "\n",
    "4. **Language bias rÃµ nháº¥t á»Ÿ \"other\":** Model yáº¿u cÃ³ xu hÆ°á»›ng sinh cÃ¢u tráº£ lá»i phá»• biáº¿n nháº¥t (mode collapse) báº¥t ká»ƒ áº£nh, Ä‘áº·c biá»‡t vá»›i cÃ¢u há»i \"what\" â†’ luÃ´n tráº£ lá»i \"white\", \"yes\", \"2\"...\n",
    "\n",
    "> **Káº¿t luáº­n:** Error analysis xÃ¡c nháº­n ráº±ng **pretrained features + attention** lÃ  tá»• há»£p máº¡nh nháº¥t cho má»i loáº¡i cÃ¢u há»i, vá»›i lá»£i tháº¿ Ä‘áº·c biá»‡t rÃµ á»Ÿ cÃ¢u há»i **number** vÃ  **other**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6554207b",
   "metadata": {},
   "source": [
    "---\n",
    "## Summary\n",
    "\n",
    "### Pipeline Steps\n",
    "\n",
    "| Step | Script / Section | Output |\n",
    "|------|-----------------|--------|\n",
    "| Build Vocab | `src/scripts/1_build_vocab.py` | `data/processed/vocab_*.json` |\n",
    "| Lá»±a chá»n Metrics | Markdown analysis | Giáº£i thÃ­ch 7 metrics (VQA Acc, EM, BLEU-1/2/3/4, METEOR) |\n",
    "| Phase 1 â€” Baseline (10ep) | `src/train.py --model X` | Checkpoints + Compare + PhÃ¢n tÃ­ch |\n",
    "| Phase 2 â€” Fine-tune (5ep) | `src/train.py --resume ...` | Checkpoints + Compare + PhÃ¢n tÃ­ch |\n",
    "| Phase 3 â€” SS (5ep) | `src/train.py --scheduled_sampling` | Checkpoints + Compare + PhÃ¢n tÃ­ch |\n",
    "| Plot Curves | `src/plot_curves.py` | `checkpoints/training_curves.png` |\n",
    "| Evaluate | `src/evaluate.py --model_type X` | Chi tiáº¿t metrics tá»«ng model |\n",
    "| Compare | `src/compare.py` | Báº£ng so sÃ¡nh side-by-side |\n",
    "| Inference | `src/inference.py` | VÃ­ dá»¥ question + predicted answer |\n",
    "| Attention Viz | `src/visualize.py --model_type C/D` | `checkpoints/attn_model_*.png` |\n",
    "| Qualitative Analysis | Inline code | áº¢nh + Q + Predicted vs GT (Ä‘Ãºng/sai) |\n",
    "| Error Analysis | Inline code | VQA Accuracy theo answer_type (yes/no, number, other) |\n",
    "\n",
    "### Training Strategy â€” 3 Phases, táº¥t cáº£ 4 models\n",
    "\n",
    "```\n",
    "Phase 1: Baseline (10 epochs)          Phase 2: Fine-tune (5 epochs)          Phase 3: Sched. Sampling (5 epochs)\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚ â€¢ Teacher Forcing           â”‚        â”‚ â€¢ B,D: Unfreeze ResNet L3+4 â”‚        â”‚ â€¢ Îµ decays: GT â†’ model pred â”‚\n",
    "â”‚ â€¢ ResNet FROZEN (B,D)       â”‚   â†’    â”‚ â€¢ A,C: Continue training    â”‚   â†’    â”‚ â€¢ Reduce exposure bias      â”‚\n",
    "â”‚ â€¢ All 4 models              â”‚        â”‚ â€¢ All 4 models, LR=5e-4    â”‚        â”‚ â€¢ All 4 models, LR=2e-4    â”‚\n",
    "â”‚ â€¢ Evaluate + Compare âœ“      â”‚        â”‚ â€¢ Evaluate + Compare âœ“      â”‚        â”‚ â€¢ Evaluate + Compare âœ“      â”‚\n",
    "â”‚ â€¢ PhÃ¢n tÃ­ch káº¿t quáº£ âœ“       â”‚        â”‚ â€¢ PhÃ¢n tÃ­ch káº¿t quáº£ âœ“       â”‚        â”‚ â€¢ PhÃ¢n tÃ­ch káº¿t quáº£ âœ“       â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "         â†“                                       â†“                                       â†“\n",
    "   Báº£ng so sÃ¡nh #1                         Báº£ng so sÃ¡nh #2                         Báº£ng so sÃ¡nh #3\n",
    " (controlled experiment)            (+ fine-tuning effect)                  (+ SS effect, final result)\n",
    "```\n",
    "\n",
    "### So sÃ¡nh CÃ´ng báº±ng\n",
    "\n",
    "Táº¥t cáº£ 4 models nháº­n **cÃ¹ng 20 epochs tá»•ng**, cÃ¹ng ká»¹ thuáº­t training á»Ÿ má»—i phase:\n",
    "\n",
    "| Model | Phase 1 | Phase 2 | Phase 3 | Total |\n",
    "|-------|---------|---------|---------|-------|\n",
    "| A | TF, scratch CNN | Continue, lr=5e-4 | +SS | 20 ep |\n",
    "| B | TF, frozen ResNet | Unfreeze CNN, lr=5e-4 | +SS, keep unfreeze | 20 ep |\n",
    "| C | TF, scratch CNN+attn | Continue, lr=5e-4 | +SS | 20 ep |\n",
    "| D | TF, frozen ResNet+attn | Unfreeze CNN, lr=5e-4 | +SS, keep unfreeze | 20 ep |\n",
    "\n",
    "### Kiáº¿n trÃºc\n",
    "\n",
    "```\n",
    "Image â”€â”€> CNN Encoder â”€â”€> img_feature â”€â”€â”\n",
    "                                        â”œâ”€â”€ Hadamard Fusion â”€â”€> h_0 â”€â”€> LSTM Decoder â”€â”€> Answer tokens\n",
    "Question â”€â”€> LSTM Encoder â”€â”€> q_feature â”€â”˜         â†‘\n",
    "                                          (Model C,D: Bahdanau Attention\n",
    "                                           attends over 49 spatial regions)\n",
    "```\n",
    "\n",
    "### ÄÃ¡nh giÃ¡ & So sÃ¡nh\n",
    "\n",
    "- **Metrics:** VQA Accuracy (chÃ­nh), Exact Match, BLEU-1/2/3/4, METEOR\n",
    "- **3 báº£ng compare** (Phase 1/2/3) cho tháº¥y progression tá»« baseline â†’ fine-tune â†’ scheduled sampling\n",
    "- **Qualitative analysis:** VÃ­ dá»¥ trá»±c quan áº£nh + cÃ¢u há»i + dá»± Ä‘oÃ¡n vs GT\n",
    "- **Error analysis:** Breakdown accuracy theo answer type (yes/no, number, other)\n",
    "- **Attention visualization:** Heatmap cho tháº¥y vÃ¹ng áº£nh model C/D focus\n",
    "\n",
    "### Káº¿t luáº­n chÃ­nh\n",
    "\n",
    "1. **Pretrained > Scratch**: Transfer learning tá»« ImageNet luÃ´n giÃºp, gap lá»›n nháº¥t\n",
    "2. **Attention > No Attention**: Spatial focus cáº£i thiá»‡n Ä‘Ã¡ng ká»ƒ, Ä‘áº·c biá»‡t cho counting & spatial questions\n",
    "3. **Training strategy matters**: Fine-tune + Scheduled Sampling tÃ­ch lÅ©y cáº£i thiá»‡n cho táº¥t cáº£ models\n",
    "4. **Ranking: D > B > C > A**: Pretrained + Attention lÃ  tá»• há»£p máº¡nh nháº¥t\n",
    "\n",
    "_(Xem output 3 báº£ng so sÃ¡nh á»Ÿ Step 3, Step 6, vÃ  phÃ¢n tÃ­ch chi tiáº¿t sau má»—i báº£ng)_"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
