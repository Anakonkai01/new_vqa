# VQA Project Plan â€” LSTM-Decoder Architecture

## Má»¥c tiÃªu

XÃ¢y dá»±ng há»‡ thá»‘ng Visual Question Answering sá»­ dá»¥ng CNN (Image Encoder) + LSTM (Question Encoder) + LSTM-Decoder (Answer Generator).

**Input:** áº¢nh + CÃ¢u há»i  
**Output:** CÃ¢u tráº£ lá»i Ä‘Æ°á»£c **sinh ra** bá»Ÿi LSTM-Decoder (generative)

---

## 4 Model cáº§n xÃ¢y dá»±ng

| Model | CNN Encoder        | Attention | Decoder       |
|-------|--------------------|-----------|---------------|
| **A** | Train from scratch | KhÃ´ng     | LSTM-Decoder  |
| **B** | Pretrained ResNet  | KhÃ´ng     | LSTM-Decoder  |
| **C** | Train from scratch | CÃ³        | LSTM-Decoder  |
| **D** | Pretrained ResNet  | CÃ³        | LSTM-Decoder  |

---

## Kiáº¿n trÃºc tá»•ng quÃ¡t

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    KIáº¾N TRÃšC Tá»”NG QUÃT                       â”‚
â”‚                                                              â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚  IMAGE   â”‚     â”‚   QUESTION   â”‚     â”‚   LSTM-DECODER   â”‚   â”‚
â”‚  â”‚ ENCODER  â”‚     â”‚   ENCODER    â”‚     â”‚   (sinh answer)  â”‚   â”‚
â”‚  â”‚ (CNN)    â”‚     â”‚   (LSTM)     â”‚     â”‚                  â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜     â”‚  Input: fusion   â”‚   â”‚
â”‚       â”‚                  â”‚             â”‚  Output: tokens   â”‚   â”‚
â”‚       â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜             â”‚  one by one       â”‚   â”‚
â”‚               â”‚                        â”‚                  â”‚   â”‚
â”‚          â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”                   â”‚  <start> â†’ "yes" â”‚   â”‚
â”‚          â”‚ FUSION  â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¶â”‚  "yes"  â†’ <end>  â”‚   â”‚
â”‚          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                                                              â”‚
â”‚  âš¡ Attention: CNN giá»¯ spatial (196, 2048)                   â”‚
â”‚     Decoder attend vÃ o tá»«ng vÃ¹ng áº£nh má»—i bÆ°á»›c sinh          â”‚
â”‚                                                              â”‚
â”‚  âŒ No Attention: CNN mean pool â†’ (2048)                     â”‚
â”‚     Fusion = concat/hadamard â†’ init decoder                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Model A & B â€” No Attention

```
CNN â†’ (batch, 2048) â†’ Linear â†’ (batch, hidden)
LSTM-Encoder(question) â†’ (batch, hidden)
Fusion = img * q â†’ (batch, hidden)
LSTM-Decoder:
  - Initial hidden state = fusion
  - Má»—i bÆ°á»›c: input = previous_word_embedding
  - Output = next token
  - Dá»«ng khi sinh <end> hoáº·c max_len
```

### Model C & D â€” With Attention

```
CNN â†’ (batch, 196, 2048) â†’ Linear â†’ (batch, 196, hidden)   # giá»¯ spatial
LSTM-Encoder(question) â†’ (batch, hidden)
LSTM-Decoder má»—i bÆ°á»›c:
  1. TÃ­nh attention weights: Î± = softmax(decoder_hidden @ image_regions)
  2. Context vector: c = Î£(Î± * image_regions)
  3. Decoder input = [context; previous_word_embedding; question_feature]
  4. Sinh token tiáº¿p theo
```

### Scratch vs Pretrained

- **Scratch:** CNN random init, `requires_grad = True`, train cÃ¹ng toÃ n bá»™ model
- **Pretrained:** ResNet101 pretrained ImageNet, freeze hoáº·c fine-tune last layers

---

## Váº¥n Ä‘á» vá»›i code hiá»‡n táº¡i

Model hiá»‡n táº¡i dÃ¹ng **classifier** (discriminative â€” chá»n 1 trong N Ä‘Ã¡p Ã¡n), nhÆ°ng Ä‘á» bÃ i yÃªu cáº§u **LSTM-Decoder** (generative â€” sinh answer token-by-token). Cáº§n **viáº¿t láº¡i**:

1. **Extract features:** cáº§n thÃªm báº£n giá»¯ spatial `(N, 196, 2048)` cho Attention models
2. **Dataset:** answer pháº£i tráº£ vá» dáº¡ng **sequence tokens** `[<start>, token1, token2, ..., <end>]` thay vÃ¬ 1 class index
3. **Model:** thay classifier báº±ng LSTM-Decoder
4. **Train:** thÃªm teacher forcing
5. **Evaluate:** thÃªm cÃ¡c metrics cho generative task

---

## Cáº¥u trÃºc thÆ° má»¥c Ä‘á» xuáº¥t

```
src/
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ 1_build_vocab.py                # ÄÃ£ cÃ³ â€” OK
â”‚   â”œâ”€â”€ 2_extract_features.py           # Sá»¬A: thÃªm mode spatial (196, 2048)
â”‚   â””â”€â”€ 3_preprocess_answers.py         # Má»šI: answer â†’ sequence tokens
â”œâ”€â”€ models/
â”‚   â”œâ”€â”€ encoder_cnn.py                  # Má»šI: CNN scratch + pretrained
â”‚   â”œâ”€â”€ encoder_question.py             # Má»šI: LSTM encoder cho question
â”‚   â”œâ”€â”€ decoder_lstm.py                 # Má»šI: LSTM decoder (no attention)
â”‚   â”œâ”€â”€ decoder_attention.py            # Má»šI: LSTM decoder (with attention)
â”‚   â””â”€â”€ vqa_model.py                    # Má»šI: Wrapper gá»™p encoder + decoder
â”œâ”€â”€ dataset.py                          # Sá»¬A: answer dáº¡ng sequence
â”œâ”€â”€ vocab.py                            # ÄÃ£ cÃ³ â€” OK
â”œâ”€â”€ train.py                            # Sá»¬A: teacher forcing, 4 model configs
â”œâ”€â”€ evaluate.py                         # Má»šI: tÃ­nh BLEU, CIDEr, VQA Accuracy
â”œâ”€â”€ inference.py                        # Má»šI: sinh cÃ¢u tráº£ lá»i tá»« áº£nh + cÃ¢u há»i
â””â”€â”€ compare.py                          # Má»šI: so sÃ¡nh 4 model, visualization
```

---

## CÃ¡c bÆ°á»›c thá»±c hiá»‡n

### Phase 1: Chuáº©n bá»‹ dá»¯ liá»‡u

| BÆ°á»›c | Viá»‡c cáº§n lÃ m | File | Tráº¡ng thÃ¡i |
|------|-------------|------|------------|
| 1 | Sá»­a extract features â€” thÃªm mode spatial `(N, 196, 2048)` | `scripts/2_extract_features.py` | â³ ChÆ°a lÃ m (chá»‰ cáº§n cho Model C, D) |
| 2 | Viáº¿t láº¡i dataset â€” answer dáº¡ng sequence, load raw image | `dataset.py` | âœ… HoÃ n thÃ nh |
| 3 | Táº¡o dummy data Ä‘á»ƒ test pipeline | `create_dummy_data.py` | âœ… HoÃ n thÃ nh |

### Phase 2: XÃ¢y dá»±ng Models

| BÆ°á»›c | Viá»‡c cáº§n lÃ m | File | Tráº¡ng thÃ¡i |
|------|-------------|------|------------|
| 4 | CNN Encoder scratch | `models/encoder_cnn.py` | âœ… HoÃ n thÃ nh â€” output `(batch, 1024)` |
| 5 | Question Encoder | `models/encoder_questions.py` | âœ… HoÃ n thÃ nh â€” output `(batch, 1024)` |
| 6 | LSTM Decoder (No Attention) | `models/decoder_lstm.py` | âœ… HoÃ n thÃ nh â€” teacher forcing mode |
| 7 | VQA Wrapper Model A | `models/vqa_models.py` | âœ… HoÃ n thÃ nh â€” `VQAModelA` |
| 8 | LSTM Decoder (Attention) | `models/decoder_attention.py` | â³ ChÆ°a lÃ m (Model C, D) |

### Phase 3: Training

| BÆ°á»›c | Viá»‡c cáº§n lÃ m | File | Tráº¡ng thÃ¡i |
|------|-------------|------|------------|
| 9 | Training loop Model A | `train.py` | ğŸ”§ Gáº§n xong â€” cÃ²n 1 bug nhá» (xem devlog) |
| 10 | ThÃªm validation loop | `train.py` | â³ ChÆ°a lÃ m |

### Phase 4: Evaluation & So sÃ¡nh

| BÆ°á»›c | Viá»‡c cáº§n lÃ m | File | Tráº¡ng thÃ¡i |
|------|-------------|------|------------|
| 11 | Code evaluation | `evaluate.py` | â³ ChÆ°a lÃ m |
| 12 | Code inference (greedy/beam search) | `inference.py` | â³ ChÆ°a lÃ m |
| 13 | So sÃ¡nh 4 model | `compare.py` | â³ ChÆ°a lÃ m |

---

## Äá»™ Ä‘o Ä‘Ã¡nh giÃ¡

| Äá»™ Ä‘o | MÃ´ táº£ | LÃ½ do sá»­ dá»¥ng |
|-------|-------|----------------|
| **BLEU** (1,2,3,4) | N-gram precision giá»¯a predicted vs ground truth | Metric chuáº©n cho text generation |
| **METEOR** | XÃ©t synonyms + stemming + alignment | Bá»• sung cho BLEU, xÃ©t ngá»¯ nghÄ©a tá»‘t hÆ¡n |
| **CIDEr** | TF-IDF weighted n-gram consensus | Äáº·c trÆ°ng cho image-text tasks |
| **VQA Accuracy** | `min(count(predicted_ans) / 3, 1)` theo VQA Challenge | **Metric chÃ­nh** cá»§a VQA benchmark |
| **ROUGE-L** | Longest Common Subsequence F1 | ÄÃ¡nh giÃ¡ bá»• sung cho sequence |

---

## Báº£ng so sÃ¡nh (template)

| Model | BLEU-1 | BLEU-4 | METEOR | CIDEr | VQA Acc | Training Time |
|-------|--------|--------|--------|-------|---------|---------------|
| A (Scratch, No Attn) | â€” | â€” | â€” | â€” | â€” | â€” |
| B (Pretrained, No Attn) | â€” | â€” | â€” | â€” | â€” | â€” |
| C (Scratch, Attn) | â€” | â€” | â€” | â€” | â€” | â€” |
| D (Pretrained, Attn) | â€” | â€” | â€” | â€” | â€” | â€” |

### Dá»± kiáº¿n káº¿t quáº£

- **B > A**: Pretrained features cháº¥t lÆ°á»£ng cao hÆ¡n scratch
- **D > C**: TÆ°Æ¡ng tá»±, pretrained + attention máº¡nh nháº¥t
- **C > A, D > B**: Attention giÃºp focus vÃ o vÃ¹ng áº£nh liÃªn quan Ä‘áº¿n cÃ¢u há»i
- **D** lÃ  model tá»‘t nháº¥t tá»•ng thá»ƒ

---

## PhÃ¢n tÃ­ch bá»• sung (Ä‘á»ƒ Ä‘áº¡t full Ä‘iá»ƒm)

1. **Qualitative Analysis**: Hiá»ƒn thá»‹ áº£nh + cÃ¢u há»i + predicted answer vs ground truth (Ä‘Ãºng & sai)
2. **Attention Heatmap**: Visualize vÃ¹ng áº£nh mÃ  model C/D táº­p trung khi tráº£ lá»i
3. **Error Analysis**: PhÃ¢n loáº¡i lá»—i theo loáº¡i cÃ¢u há»i (yes/no, counting, color, ...)
4. **Ablation Study**: áº¢nh hÆ°á»Ÿng cá»§a hyperparameters (embed_size, hidden_size, num_layers)
5. **Training Curves**: Plot loss & accuracy theo epoch cho cáº£ 4 model trÃªn cÃ¹ng 1 biá»ƒu Ä‘á»“

---

## LÆ°u Ã½ ká»¹ thuáº­t

- **GPU MX330 khÃ´ng tÆ°Æ¡ng thÃ­ch PyTorch CUDA má»›i** â†’ Train trÃªn CPU hoáº·c Google Colab
- **Teacher Forcing Ratio**: Báº¯t Ä‘áº§u 1.0, giáº£m dáº§n vá» 0.5 theo epoch
- **Beam Search**: DÃ¹ng beam_size = 3-5 khi inference Ä‘á»ƒ cáº£i thiá»‡n cháº¥t lÆ°á»£ng
- **Gradient Clipping**: `clip_grad_norm_(model.parameters(), max_norm=5.0)` trÃ¡nh exploding gradients
- Vocab answer cáº§n `<start>`, `<end>`, `<pad>`, `<unk>` tokens (Ä‘Ã£ cÃ³ trong `Vocabulary`)
